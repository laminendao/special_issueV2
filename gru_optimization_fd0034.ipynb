{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sommaire: <a class=\"anchor\" id=\"sommaire\"></a>\n",
    "* [Sommaire](#sommaire)\n",
    "* [Preambule](#prem)\n",
    "     * [Package Loading](#package)\n",
    "     * [Functions](#function)\n",
    "* [GRU](#lstm)\n",
    "    * [1.FD001](#fd001)\n",
    "        * [1.1 Data loading](#fd001dataload)\n",
    "        * [1.2 Model selection](#fd001modelselect)\n",
    "    * [2.FD002](#fd002)\n",
    "         * [2.1 Data loading](#fd002dataload)\n",
    "         * [2.2 Model selection](#fd002modelselect)\n",
    "    * [3.FD003](#fd003)\n",
    "         * [3.1 Data loading](#fd003dataload)\n",
    "         * [3.2 Model selection](#fd003modelselect)\n",
    "    * [4.FD004](#fd004)\n",
    "         * [4.1 Data loading](#fd004dataload)\n",
    "         * [4.2 Model selection](#fd004modelselect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package loading <a class = \"anchor\" id=\"package\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lime.lime_tabular import RecurrentTabularExplainer\n",
    "from tqdm import tqdm\n",
    "import keras\n",
    "from sp_modif.model_function import *\n",
    "from sp_modif.methods import *\n",
    "from sp_modif.data_prep import *\n",
    "from sp_modif.evaluator import *\n",
    "from sp_modif.SHAP import *\n",
    "from sp_modif.L2X import *\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import random\n",
    "# import keras\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, GRU\n",
    "from scipy import optimize\n",
    "from methods import *\n",
    "import warnings\n",
    "from tensorflow.keras import optimizers\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"okay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions <a class = \"anchor\" id=\"function\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixer le seed pour la reproductibilit√©\n",
    "SEED = 0\n",
    "def set_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "# Appeler la fonction pour fixer le seed\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rul_piecewise_fct(X_train, rul):\n",
    "    \n",
    "    X_train['RUL'].clip(upper=rul, inplace=True)\n",
    "    \n",
    "    return X_train\n",
    "\n",
    "def prep_data(train, test, drop_sensors, remaining_sensors, alpha):\n",
    "    X_train_interim = add_operating_condition(train.drop(drop_sensors, axis=1))\n",
    "    X_test_interim = add_operating_condition(test.drop(drop_sensors, axis=1))\n",
    "\n",
    "    X_train_interim, X_test_interim = condition_scaler(X_train_interim, X_test_interim, remaining_sensors)\n",
    "\n",
    "    X_train_interim = exponential_smoothing(X_train_interim, remaining_sensors, 0, alpha)\n",
    "    X_test_interim = exponential_smoothing(X_test_interim, remaining_sensors, 0, alpha)\n",
    "    \n",
    "    return X_train_interim, X_test_interim\n",
    "\n",
    "# Models functions\n",
    "\n",
    "# 1layers\n",
    "def model_gru_1layer(input_shape, nodes_per_layer, dropout, activation, weights_file):\n",
    "    \n",
    "    cb = keras.callbacks.EarlyStopping(monitor='loss', patience=4)\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))\n",
    "    model.save_weights(weights_file)\n",
    "\n",
    "    return model\n",
    "# 2layers\n",
    "def model_gru_2layer(input_shape, nodes_per_layer, dropout, activation, weights_file):\n",
    "    \n",
    "    cb = keras.callbacks.EarlyStopping(monitor='loss', patience=4)\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(GRU(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))\n",
    "    model.save_weights(weights_file)\n",
    "\n",
    "    return model\n",
    "\n",
    "# 3layers\n",
    "def model_gru_3layer(input_shape, nodes_per_layer, dropout, activation, weights_file):\n",
    "    \n",
    "    cb = keras.callbacks.EarlyStopping(monitor='loss', patience=4)\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(GRU(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(GRU(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))\n",
    "    model.save_weights(weights_file)\n",
    "\n",
    "    return model\n",
    "\n",
    "def model_gru_4layer(input_shape, nodes_per_layer, dropout, activation, weights_file):\n",
    "    \n",
    "    cb = keras.callbacks.EarlyStopping(monitor='loss', patience=4)\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(GRU(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(GRU(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(GRU(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))\n",
    "    model.save_weights(weights_file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. FD003 <a class = \"anchor\" id=\"fd031\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24720, 27) (16596, 26) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "# Data loading\n",
    "\n",
    "train, test, y_test = prepare_data('FD003.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50','P30','Nf','Nc','Ps30','phi',\n",
    "                'NRf','NRc','BPR','htBleed','W31','W32'] # selection based on main_notebook\n",
    "\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_piecewise = 130\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10752"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lower alpha's perform better, so we can ditch a few high ones to reduce the search space\n",
    "alpha_list = [0.01, 0.05] + list(np.arange(10,60+1,10)/100)\n",
    "\n",
    "sequence_list = list(np.arange(10,40+1,5))\n",
    "epoch_list = list(np.arange(5,20+1,5))\n",
    "nodes_list = [[32], [64], [128], [256]]\n",
    "\n",
    "# lowest dropout=0.1, because I know zero dropout will yield better training results but worse generalization\n",
    "dropouts = list(np.arange(1,4)/10)  \n",
    "\n",
    "# again, earlier testing revealed relu performed significantly worse, so I removed it from the options\n",
    "activation_functions = ['tanh']\n",
    "batch_size_list = [32, 64, 128, 256]\n",
    "sensor_list = [sensor_names]\n",
    "\n",
    "tuning_options = np.prod([len(alpha_list),\n",
    "                          len(sequence_list),\n",
    "                          len(epoch_list),\n",
    "                          len(nodes_list),\n",
    "                          len(dropouts),\n",
    "                          len(activation_functions),\n",
    "                          len(batch_size_list),\n",
    "                          len(sensor_list)])\n",
    "tuning_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 25\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "341/341 [==============================] - 8s 19ms/step - loss: 5413.0015 - val_loss: 2089.2949\n",
      "Epoch 2/25\n",
      "341/341 [==============================] - 6s 19ms/step - loss: 2580.4509 - val_loss: 1607.8918\n",
      "Epoch 3/25\n",
      "341/341 [==============================] - 6s 19ms/step - loss: 1978.9729 - val_loss: 1757.5923\n",
      "Epoch 4/25\n",
      "341/341 [==============================] - 6s 18ms/step - loss: 1894.9200 - val_loss: 1842.9081\n",
      "Epoch 5/25\n",
      "341/341 [==============================] - 7s 19ms/step - loss: 1336.4696 - val_loss: 595.5422\n",
      "Epoch 6/25\n",
      "341/341 [==============================] - 6s 19ms/step - loss: 501.4252 - val_loss: 540.9349\n",
      "Epoch 7/25\n",
      "341/341 [==============================] - 6s 19ms/step - loss: 333.1841 - val_loss: 330.1049\n",
      "Epoch 8/25\n",
      "341/341 [==============================] - 6s 19ms/step - loss: 287.5868 - val_loss: 340.4538\n",
      "Epoch 9/25\n",
      "341/341 [==============================] - 7s 19ms/step - loss: 269.4568 - val_loss: 324.0808\n",
      "Epoch 10/25\n",
      "341/341 [==============================] - 7s 20ms/step - loss: 272.4776 - val_loss: 337.7659\n",
      "Epoch 11/25\n",
      "341/341 [==============================] - 7s 19ms/step - loss: 254.0897 - val_loss: 320.1577\n",
      "Epoch 12/25\n",
      "341/341 [==============================] - 6s 19ms/step - loss: 239.3018 - val_loss: 324.1551\n",
      "Epoch 13/25\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 245.1377 - val_loss: 355.9459\n",
      "Epoch 14/25\n",
      "341/341 [==============================] - 31s 92ms/step - loss: 241.9442 - val_loss: 319.9184\n",
      "Epoch 15/25\n",
      "341/341 [==============================] - 8s 24ms/step - loss: 233.9029 - val_loss: 435.4840\n",
      "Epoch 16/25\n",
      "341/341 [==============================] - 165s 485ms/step - loss: 232.1772 - val_loss: 273.2066\n",
      "Epoch 17/25\n",
      "341/341 [==============================] - 5s 15ms/step - loss: 229.4582 - val_loss: 323.5182\n",
      "Epoch 18/25\n",
      "341/341 [==============================] - 6s 16ms/step - loss: 220.6276 - val_loss: 310.5872\n",
      "Epoch 19/25\n",
      "341/341 [==============================] - 60s 177ms/step - loss: 218.6319 - val_loss: 346.5442\n",
      "Epoch 20/25\n",
      "341/341 [==============================] - 5s 16ms/step - loss: 216.3213 - val_loss: 247.8349\n",
      "Epoch 21/25\n",
      "341/341 [==============================] - 5s 15ms/step - loss: 202.4053 - val_loss: 230.4481\n",
      "Epoch 22/25\n",
      "341/341 [==============================] - 5s 16ms/step - loss: 202.2004 - val_loss: 237.7340\n",
      "Epoch 23/25\n",
      "341/341 [==============================] - 5s 15ms/step - loss: 200.3741 - val_loss: 279.3575\n",
      "Epoch 24/25\n",
      "341/341 [==============================] - 125s 368ms/step - loss: 199.8082 - val_loss: 257.9547\n",
      "Epoch 25/25\n",
      "341/341 [==============================] - 5s 15ms/step - loss: 198.6108 - val_loss: 243.0693\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "341/341 [==============================] - 4s 8ms/step - loss: 7123.3291 - val_loss: 3510.6484\n",
      "Epoch 2/25\n",
      "341/341 [==============================] - 1052s 3s/step - loss: 4494.5654 - val_loss: 2260.0505\n",
      "Epoch 3/25\n",
      "341/341 [==============================] - 918s 3s/step - loss: 3095.1187 - val_loss: 1729.7494\n",
      "Epoch 4/25\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 2376.8379 - val_loss: 1605.4067\n",
      "Epoch 5/25\n",
      "341/341 [==============================] - 1021s 3s/step - loss: 2050.6572 - val_loss: 1659.5454\n",
      "Epoch 6/25\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 1923.2566 - val_loss: 1563.0597\n",
      "Epoch 7/25\n",
      "341/341 [==============================] - 885s 3s/step - loss: 1224.4427 - val_loss: 625.2505\n",
      "Epoch 8/25\n",
      "341/341 [==============================] - 5s 15ms/step - loss: 641.5928 - val_loss: 494.1991\n",
      "Epoch 9/25\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 467.3332 - val_loss: 363.2439\n",
      "Epoch 10/25\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 377.8585 - val_loss: 382.8527\n",
      "Epoch 11/25\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 319.5290 - val_loss: 409.6248\n",
      "Epoch 12/25\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 288.2033 - val_loss: 356.8397\n",
      "Epoch 13/25\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 273.5357 - val_loss: 455.8073\n",
      "Epoch 14/25\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 269.1926 - val_loss: 326.0431\n",
      "Epoch 15/25\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 250.2886 - val_loss: 436.0558\n",
      "Epoch 16/25\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 246.9275 - val_loss: 316.3122\n",
      "Epoch 17/25\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 244.0422 - val_loss: 309.2063\n",
      "Epoch 18/25\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 229.7874 - val_loss: 387.4572\n",
      "Epoch 19/25\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 234.4518 - val_loss: 334.0566\n",
      "Epoch 20/25\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 234.3279 - val_loss: 336.9728\n",
      "Epoch 21/25\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 225.4994 - val_loss: 283.5336\n",
      "Epoch 22/25\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 222.1299 - val_loss: 281.3304\n",
      "Epoch 23/25\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 220.1915 - val_loss: 353.6640\n",
      "Epoch 24/25\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 221.3741 - val_loss: 322.6534\n",
      "Epoch 25/25\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 218.1692 - val_loss: 286.9145\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "86/86 [==============================] - 2s 14ms/step - loss: 9278.2842 - val_loss: 5772.5986\n",
      "Epoch 2/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 8170.3003 - val_loss: 5333.0488\n",
      "Epoch 3/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 7670.3877 - val_loss: 4970.6870\n",
      "Epoch 4/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 7226.7588 - val_loss: 4644.6392\n",
      "Epoch 5/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 6822.1406 - val_loss: 4346.8164\n",
      "Epoch 6/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 6452.6182 - val_loss: 4072.6851\n",
      "Epoch 7/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 6103.0962 - val_loss: 3819.7693\n",
      "Epoch 8/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 5775.9126 - val_loss: 3585.9395\n",
      "Epoch 9/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 5465.4487 - val_loss: 3370.0356\n",
      "Epoch 10/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 5179.9463 - val_loss: 3171.3467\n",
      "Epoch 11/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 4910.7041 - val_loss: 2988.5188\n",
      "Epoch 12/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 4660.5312 - val_loss: 2820.5249\n",
      "Epoch 13/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 4426.7603 - val_loss: 2667.1785\n",
      "Epoch 14/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 4206.3687 - val_loss: 2527.0779\n",
      "Epoch 15/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 4002.5532 - val_loss: 2399.9624\n",
      "Epoch 16/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 3812.2476 - val_loss: 2284.8721\n",
      "Epoch 17/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 3637.9988 - val_loss: 2181.4465\n",
      "Epoch 18/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 3475.7996 - val_loss: 2088.4346\n",
      "Epoch 19/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 3317.5205 - val_loss: 2005.4213\n",
      "Epoch 20/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 3182.9126 - val_loss: 1932.3392\n",
      "Epoch 21/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 3049.7751 - val_loss: 1868.4041\n",
      "Epoch 22/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 2940.3467 - val_loss: 1812.8962\n",
      "Epoch 23/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 2829.7766 - val_loss: 1765.3018\n",
      "Epoch 24/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 2732.8022 - val_loss: 1724.6897\n",
      "Epoch 25/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 2649.1045 - val_loss: 1691.3660\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "86/86 [==============================] - 8s 87ms/step - loss: 6366.4185 - val_loss: 2913.7407\n",
      "Epoch 2/25\n",
      "86/86 [==============================] - 7s 87ms/step - loss: 3764.8384 - val_loss: 1908.4106\n",
      "Epoch 3/25\n",
      "86/86 [==============================] - 7s 86ms/step - loss: 2613.1108 - val_loss: 1619.5367\n",
      "Epoch 4/25\n",
      "86/86 [==============================] - 7s 81ms/step - loss: 2128.2446 - val_loss: 1632.8934\n",
      "Epoch 5/25\n",
      "86/86 [==============================] - 8s 89ms/step - loss: 1946.5192 - val_loss: 1644.0084\n",
      "Epoch 6/25\n",
      "86/86 [==============================] - 8s 93ms/step - loss: 1612.4592 - val_loss: 1138.0184\n",
      "Epoch 7/25\n",
      "86/86 [==============================] - 7s 86ms/step - loss: 1193.6229 - val_loss: 903.8345\n",
      "Epoch 8/25\n",
      "86/86 [==============================] - 7s 86ms/step - loss: 890.7731 - val_loss: 859.0070\n",
      "Epoch 9/25\n",
      "86/86 [==============================] - 8s 87ms/step - loss: 755.8021 - val_loss: 741.4676\n",
      "Epoch 10/25\n",
      "86/86 [==============================] - 10s 114ms/step - loss: 654.5876 - val_loss: 658.5786\n",
      "Epoch 11/25\n",
      "86/86 [==============================] - 9s 99ms/step - loss: 599.5367 - val_loss: 714.8111\n",
      "Epoch 12/25\n",
      "86/86 [==============================] - 9s 99ms/step - loss: 547.2377 - val_loss: 748.7083\n",
      "Epoch 13/25\n",
      "86/86 [==============================] - 9s 99ms/step - loss: 511.0911 - val_loss: 670.9528\n",
      "Epoch 14/25\n",
      "86/86 [==============================] - 8s 95ms/step - loss: 481.8381 - val_loss: 551.7487\n",
      "Epoch 15/25\n",
      "86/86 [==============================] - 8s 94ms/step - loss: 460.8968 - val_loss: 676.2681\n",
      "Epoch 16/25\n",
      "86/86 [==============================] - 8s 92ms/step - loss: 437.6646 - val_loss: 568.3447\n",
      "Epoch 17/25\n",
      "86/86 [==============================] - 8s 88ms/step - loss: 431.3627 - val_loss: 460.8642\n",
      "Epoch 18/25\n",
      "86/86 [==============================] - 8s 94ms/step - loss: 416.9499 - val_loss: 465.3149\n",
      "Epoch 19/25\n",
      "86/86 [==============================] - 9s 101ms/step - loss: 378.9102 - val_loss: 449.7257\n",
      "Epoch 20/25\n",
      "86/86 [==============================] - 8s 94ms/step - loss: 345.3725 - val_loss: 607.7355\n",
      "Epoch 21/25\n",
      "86/86 [==============================] - 8s 94ms/step - loss: 386.1718 - val_loss: 520.2017\n",
      "Epoch 22/25\n",
      "86/86 [==============================] - 8s 93ms/step - loss: 339.0974 - val_loss: 457.9904\n",
      "Epoch 23/25\n",
      "86/86 [==============================] - 8s 88ms/step - loss: 315.1486 - val_loss: 465.5582\n",
      "Epoch 24/25\n",
      "86/86 [==============================] - 8s 89ms/step - loss: 306.6927 - val_loss: 456.5527\n",
      "Epoch 25/25\n",
      "86/86 [==============================] - 8s 90ms/step - loss: 284.9406 - val_loss: 342.0586\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "682/682 [==============================] - 10s 14ms/step - loss: 4057.3796 - val_loss: 1606.8805\n",
      "Epoch 2/25\n",
      "682/682 [==============================] - 10s 15ms/step - loss: 1616.1038 - val_loss: 631.0889\n",
      "Epoch 3/25\n",
      "682/682 [==============================] - 10s 14ms/step - loss: 548.0004 - val_loss: 671.4364\n",
      "Epoch 4/25\n",
      "682/682 [==============================] - 10s 15ms/step - loss: 385.5128 - val_loss: 963.2460\n",
      "Epoch 5/25\n",
      "682/682 [==============================] - 10s 14ms/step - loss: 312.4513 - val_loss: 308.6931\n",
      "Epoch 6/25\n",
      "682/682 [==============================] - 10s 14ms/step - loss: 259.1480 - val_loss: 292.5043\n",
      "Epoch 7/25\n",
      "682/682 [==============================] - 10s 14ms/step - loss: 237.0266 - val_loss: 277.1904\n",
      "Epoch 8/25\n",
      "682/682 [==============================] - 10s 14ms/step - loss: 231.1839 - val_loss: 283.7207\n",
      "Epoch 9/25\n",
      "682/682 [==============================] - 10s 15ms/step - loss: 222.4268 - val_loss: 275.4079\n",
      "Epoch 10/25\n",
      "682/682 [==============================] - 10s 14ms/step - loss: 215.4541 - val_loss: 306.4682\n",
      "Epoch 11/25\n",
      "682/682 [==============================] - 10s 15ms/step - loss: 208.3738 - val_loss: 229.4259\n",
      "Epoch 12/25\n",
      "682/682 [==============================] - 10s 15ms/step - loss: 211.0762 - val_loss: 274.9149\n",
      "Epoch 13/25\n",
      "682/682 [==============================] - 10s 14ms/step - loss: 206.3725 - val_loss: 287.6190\n",
      "Epoch 14/25\n",
      "682/682 [==============================] - 10s 14ms/step - loss: 201.1672 - val_loss: 227.1511\n",
      "Epoch 15/25\n",
      "682/682 [==============================] - 10s 15ms/step - loss: 196.0549 - val_loss: 323.0409\n",
      "Epoch 16/25\n",
      "682/682 [==============================] - 10s 14ms/step - loss: 191.8573 - val_loss: 209.5956\n",
      "Epoch 17/25\n",
      "682/682 [==============================] - 10s 14ms/step - loss: 188.5698 - val_loss: 219.6884\n",
      "Epoch 18/25\n",
      "682/682 [==============================] - 10s 14ms/step - loss: 184.8374 - val_loss: 211.8724\n",
      "Epoch 19/25\n",
      "682/682 [==============================] - 10s 14ms/step - loss: 179.3894 - val_loss: 216.7781\n",
      "Epoch 20/25\n",
      "682/682 [==============================] - 10s 14ms/step - loss: 174.9678 - val_loss: 214.4371\n",
      "Epoch 21/25\n",
      "682/682 [==============================] - 11s 16ms/step - loss: 172.3272 - val_loss: 160.7866\n",
      "Epoch 22/25\n",
      "682/682 [==============================] - 10s 15ms/step - loss: 168.4312 - val_loss: 200.6892\n",
      "Epoch 23/25\n",
      "682/682 [==============================] - 10s 15ms/step - loss: 162.4371 - val_loss: 205.2069\n",
      "Epoch 24/25\n",
      "682/682 [==============================] - 10s 15ms/step - loss: 157.8595 - val_loss: 184.8862\n",
      "Epoch 25/25\n",
      "682/682 [==============================] - 10s 15ms/step - loss: 158.1035 - val_loss: 205.7499\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "171/171 [==============================] - 3s 9ms/step - loss: 8833.8779 - val_loss: 5435.6812\n",
      "Epoch 2/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 7579.1362 - val_loss: 4743.0273\n",
      "Epoch 3/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 6764.5171 - val_loss: 4164.6274\n",
      "Epoch 4/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 6055.6860 - val_loss: 3670.3525\n",
      "Epoch 5/25\n",
      "171/171 [==============================] - 1s 8ms/step - loss: 5435.2856 - val_loss: 3248.2905\n",
      "Epoch 6/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 4888.5820 - val_loss: 2889.7588\n",
      "Epoch 7/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 4415.5298 - val_loss: 2588.1086\n",
      "Epoch 8/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 3995.9011 - val_loss: 2337.0500\n",
      "Epoch 9/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 3633.1204 - val_loss: 2132.2087\n",
      "Epoch 10/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 3318.8447 - val_loss: 1968.6857\n",
      "Epoch 11/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 3047.5676 - val_loss: 1841.4861\n",
      "Epoch 12/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 2839.6807 - val_loss: 1746.3855\n",
      "Epoch 13/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 2645.4797 - val_loss: 1679.1731\n",
      "Epoch 14/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 2484.0312 - val_loss: 1635.7812\n",
      "Epoch 15/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 2363.7300 - val_loss: 1610.5288\n",
      "Epoch 16/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 2172.0769 - val_loss: 1256.4916\n",
      "Epoch 17/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 1825.2550 - val_loss: 1107.3456\n",
      "Epoch 18/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 1529.3549 - val_loss: 824.2120\n",
      "Epoch 19/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 1286.9376 - val_loss: 749.3698\n",
      "Epoch 20/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 1140.5684 - val_loss: 678.9828\n",
      "Epoch 21/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 1022.2551 - val_loss: 630.4904\n",
      "Epoch 22/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 911.0550 - val_loss: 545.8815\n",
      "Epoch 23/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 800.7996 - val_loss: 473.7963\n",
      "Epoch 24/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 699.6927 - val_loss: 388.9255\n",
      "Epoch 25/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 619.9767 - val_loss: 365.2323\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "341/341 [==============================] - 3s 6ms/step - loss: 8226.6982 - val_loss: 4769.4575\n",
      "Epoch 2/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 6435.7720 - val_loss: 3693.6194\n",
      "Epoch 3/25\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 5179.1343 - val_loss: 2907.6555\n",
      "Epoch 4/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 4213.4658 - val_loss: 2349.9319\n",
      "Epoch 5/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 3466.4521 - val_loss: 1976.2947\n",
      "Epoch 6/25\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 2918.2390 - val_loss: 1750.4041\n",
      "Epoch 7/25\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 2529.5122 - val_loss: 1636.9252\n",
      "Epoch 8/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 2264.2485 - val_loss: 1605.3441\n",
      "Epoch 9/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 2093.3823 - val_loss: 1627.0607\n",
      "Epoch 10/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 1992.4753 - val_loss: 1675.9786\n",
      "Epoch 11/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 1946.0900 - val_loss: 1732.7867\n",
      "Epoch 12/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 1702.8905 - val_loss: 774.1905\n",
      "Epoch 13/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 911.9260 - val_loss: 672.8839\n",
      "Epoch 14/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 681.2753 - val_loss: 552.3515\n",
      "Epoch 15/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 530.4734 - val_loss: 428.8732\n",
      "Epoch 16/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 432.1942 - val_loss: 401.8839\n",
      "Epoch 17/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 369.1555 - val_loss: 297.7620\n",
      "Epoch 18/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 327.3094 - val_loss: 338.1377\n",
      "Epoch 19/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 306.7432 - val_loss: 322.2633\n",
      "Epoch 20/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 291.2249 - val_loss: 361.9881\n",
      "Epoch 21/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 271.8293 - val_loss: 304.7184\n",
      "Epoch 22/25\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 273.4585 - val_loss: 322.7572\n",
      "Epoch 23/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 259.0183 - val_loss: 375.6066\n",
      "Epoch 24/25\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 260.8938 - val_loss: 289.1082\n",
      "Epoch 25/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 254.5235 - val_loss: 313.4441\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "171/171 [==============================] - 10s 56ms/step - loss: 5166.6523 - val_loss: 1964.1414\n",
      "Epoch 2/25\n",
      "171/171 [==============================] - 9s 55ms/step - loss: 2428.0979 - val_loss: 1622.7653\n",
      "Epoch 3/25\n",
      "171/171 [==============================] - 9s 55ms/step - loss: 1932.6412 - val_loss: 1787.6663\n",
      "Epoch 4/25\n",
      "171/171 [==============================] - 10s 57ms/step - loss: 1807.8871 - val_loss: 1546.6040\n",
      "Epoch 5/25\n",
      "171/171 [==============================] - 10s 58ms/step - loss: 991.1414 - val_loss: 684.5662\n",
      "Epoch 6/25\n",
      "171/171 [==============================] - 10s 58ms/step - loss: 641.5154 - val_loss: 634.5566\n",
      "Epoch 7/25\n",
      "171/171 [==============================] - 10s 56ms/step - loss: 511.3628 - val_loss: 623.1236\n",
      "Epoch 8/25\n",
      "171/171 [==============================] - 10s 56ms/step - loss: 445.8635 - val_loss: 671.7533\n",
      "Epoch 9/25\n",
      "171/171 [==============================] - 10s 56ms/step - loss: 411.2390 - val_loss: 517.0950\n",
      "Epoch 10/25\n",
      "171/171 [==============================] - 9s 54ms/step - loss: 374.0604 - val_loss: 382.9214\n",
      "Epoch 11/25\n",
      "171/171 [==============================] - 10s 56ms/step - loss: 341.8627 - val_loss: 362.3888\n",
      "Epoch 12/25\n",
      "171/171 [==============================] - 11s 62ms/step - loss: 301.2793 - val_loss: 344.6158\n",
      "Epoch 13/25\n",
      "171/171 [==============================] - 11s 64ms/step - loss: 298.2895 - val_loss: 446.2392\n",
      "Epoch 14/25\n",
      "171/171 [==============================] - 10s 59ms/step - loss: 281.1488 - val_loss: 317.3282\n",
      "Epoch 15/25\n",
      "171/171 [==============================] - 10s 60ms/step - loss: 257.6151 - val_loss: 513.0154\n",
      "Epoch 16/25\n",
      "171/171 [==============================] - 11s 62ms/step - loss: 251.9111 - val_loss: 301.0171\n",
      "Epoch 17/25\n",
      "171/171 [==============================] - 10s 61ms/step - loss: 252.0031 - val_loss: 304.1817\n",
      "Epoch 18/25\n",
      "171/171 [==============================] - 10s 57ms/step - loss: 248.9303 - val_loss: 445.3041\n",
      "Epoch 19/25\n",
      "171/171 [==============================] - 10s 57ms/step - loss: 240.9979 - val_loss: 333.9348\n",
      "Epoch 20/25\n",
      "171/171 [==============================] - 10s 58ms/step - loss: 236.2128 - val_loss: 353.4523\n",
      "Epoch 21/25\n",
      "171/171 [==============================] - 10s 58ms/step - loss: 227.2558 - val_loss: 301.5349\n",
      "Epoch 22/25\n",
      "171/171 [==============================] - 10s 57ms/step - loss: 234.6299 - val_loss: 279.3431\n",
      "Epoch 23/25\n",
      "171/171 [==============================] - 9s 54ms/step - loss: 219.5399 - val_loss: 366.4003\n",
      "Epoch 24/25\n",
      "171/171 [==============================] - 10s 56ms/step - loss: 227.6195 - val_loss: 397.6898\n",
      "Epoch 25/25\n",
      "171/171 [==============================] - 10s 57ms/step - loss: 220.1444 - val_loss: 278.5939\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "682/682 [==============================] - 4s 5ms/step - loss: 7273.6382 - val_loss: 3647.6487\n",
      "Epoch 2/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 4656.7305 - val_loss: 2331.3633\n",
      "Epoch 3/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 3191.4409 - val_loss: 1746.0291\n",
      "Epoch 4/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 2419.7324 - val_loss: 1605.3488\n",
      "Epoch 5/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 2000.2740 - val_loss: 1333.3022\n",
      "Epoch 6/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 1096.8990 - val_loss: 580.6918\n",
      "Epoch 7/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 672.9065 - val_loss: 504.7343\n",
      "Epoch 8/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 476.1377 - val_loss: 343.9882\n",
      "Epoch 9/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 389.8038 - val_loss: 407.0108\n",
      "Epoch 10/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 353.2018 - val_loss: 377.7922\n",
      "Epoch 11/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 335.9433 - val_loss: 346.2313\n",
      "Epoch 12/25\n",
      "682/682 [==============================] - 4s 5ms/step - loss: 322.7536 - val_loss: 336.0198\n",
      "Epoch 13/25\n",
      "682/682 [==============================] - 4s 5ms/step - loss: 324.8326 - val_loss: 331.0583\n",
      "Epoch 14/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 312.0969 - val_loss: 299.4547\n",
      "Epoch 15/25\n",
      "682/682 [==============================] - 4s 5ms/step - loss: 304.3900 - val_loss: 356.0553\n",
      "Epoch 16/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 310.1183 - val_loss: 303.4942\n",
      "Epoch 17/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 303.7850 - val_loss: 310.5356\n",
      "Epoch 18/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 299.1868 - val_loss: 362.8107\n",
      "Epoch 19/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 294.1944 - val_loss: 292.6416\n",
      "Epoch 20/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 297.0347 - val_loss: 304.6440\n",
      "Epoch 21/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 295.8135 - val_loss: 288.7430\n",
      "Epoch 22/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 292.0132 - val_loss: 288.2601\n",
      "Epoch 23/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 287.4362 - val_loss: 311.1232\n",
      "Epoch 24/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 287.4280 - val_loss: 321.2339\n",
      "Epoch 25/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 282.8980 - val_loss: 318.5120\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "iteration  10\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "86/86 [==============================] - 3s 29ms/step - loss: 8615.6699 - val_loss: 5159.0063\n",
      "Epoch 2/25\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 7244.6367 - val_loss: 4489.6587\n",
      "Epoch 3/25\n",
      "86/86 [==============================] - 2s 24ms/step - loss: 6437.9756 - val_loss: 3924.4951\n",
      "Epoch 4/25\n",
      "86/86 [==============================] - 2s 25ms/step - loss: 5742.2393 - val_loss: 3453.2905\n",
      "Epoch 5/25\n",
      "86/86 [==============================] - 2s 25ms/step - loss: 5145.6152 - val_loss: 3058.6331\n",
      "Epoch 6/25\n",
      "86/86 [==============================] - 2s 28ms/step - loss: 4621.6968 - val_loss: 2716.7627\n",
      "Epoch 7/25\n",
      "86/86 [==============================] - 2s 27ms/step - loss: 4159.2441 - val_loss: 2435.6350\n",
      "Epoch 8/25\n",
      "86/86 [==============================] - 2s 29ms/step - loss: 3762.0066 - val_loss: 2209.1360\n",
      "Epoch 9/25\n",
      "86/86 [==============================] - 2s 27ms/step - loss: 3425.4810 - val_loss: 2029.4069\n",
      "Epoch 10/25\n",
      "86/86 [==============================] - 2s 27ms/step - loss: 3140.3220 - val_loss: 1889.8712\n",
      "Epoch 11/25\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 2901.4705 - val_loss: 1784.3700\n",
      "Epoch 12/25\n",
      "86/86 [==============================] - 2s 27ms/step - loss: 2696.9756 - val_loss: 1703.0088\n",
      "Epoch 13/25\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 2519.0251 - val_loss: 1553.3325\n",
      "Epoch 14/25\n",
      "86/86 [==============================] - 2s 27ms/step - loss: 2259.8091 - val_loss: 1377.2083\n",
      "Epoch 15/25\n",
      "86/86 [==============================] - 2s 27ms/step - loss: 1982.0249 - val_loss: 1146.9182\n",
      "Epoch 16/25\n",
      "86/86 [==============================] - 2s 29ms/step - loss: 1706.7018 - val_loss: 965.9389\n",
      "Epoch 17/25\n",
      "86/86 [==============================] - 3s 29ms/step - loss: 1484.5697 - val_loss: 859.3844\n",
      "Epoch 18/25\n",
      "86/86 [==============================] - 3s 30ms/step - loss: 1311.3784 - val_loss: 773.7149\n",
      "Epoch 19/25\n",
      "86/86 [==============================] - 2s 29ms/step - loss: 1176.2692 - val_loss: 694.6135\n",
      "Epoch 20/25\n",
      "86/86 [==============================] - 2s 29ms/step - loss: 1049.4448 - val_loss: 628.6987\n",
      "Epoch 21/25\n",
      "86/86 [==============================] - 2s 28ms/step - loss: 937.3759 - val_loss: 571.0623\n",
      "Epoch 22/25\n",
      "86/86 [==============================] - 2s 27ms/step - loss: 844.7932 - val_loss: 501.3288\n",
      "Epoch 23/25\n",
      "86/86 [==============================] - 2s 28ms/step - loss: 734.0212 - val_loss: 486.4434\n",
      "Epoch 24/25\n",
      "86/86 [==============================] - 2s 27ms/step - loss: 654.0662 - val_loss: 393.9533\n",
      "Epoch 25/25\n",
      "86/86 [==============================] - 2s 29ms/step - loss: 602.9919 - val_loss: 391.3300\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "171/171 [==============================] - 10s 54ms/step - loss: 5137.4458 - val_loss: 1954.1781\n",
      "Epoch 2/25\n",
      "171/171 [==============================] - 9s 54ms/step - loss: 2423.1382 - val_loss: 1625.1089\n",
      "Epoch 3/25\n",
      "171/171 [==============================] - 9s 53ms/step - loss: 1931.9017 - val_loss: 1770.3921\n",
      "Epoch 4/25\n",
      "171/171 [==============================] - 9s 54ms/step - loss: 1551.3073 - val_loss: 963.4166\n",
      "Epoch 5/25\n",
      "171/171 [==============================] - 9s 55ms/step - loss: 805.9578 - val_loss: 784.4421\n",
      "Epoch 6/25\n",
      "171/171 [==============================] - 9s 55ms/step - loss: 611.1711 - val_loss: 607.6721\n",
      "Epoch 7/25\n",
      "171/171 [==============================] - 9s 55ms/step - loss: 524.9153 - val_loss: 687.6145\n",
      "Epoch 8/25\n",
      "171/171 [==============================] - 9s 54ms/step - loss: 475.6725 - val_loss: 692.8026\n",
      "Epoch 9/25\n",
      "171/171 [==============================] - 9s 54ms/step - loss: 444.2148 - val_loss: 494.0134\n",
      "Epoch 10/25\n",
      "171/171 [==============================] - 9s 52ms/step - loss: 406.6205 - val_loss: 813.8230\n",
      "Epoch 11/25\n",
      "171/171 [==============================] - 9s 53ms/step - loss: 409.9053 - val_loss: 376.5766\n",
      "Epoch 12/25\n",
      "171/171 [==============================] - 9s 55ms/step - loss: 334.3938 - val_loss: 365.6594\n",
      "Epoch 13/25\n",
      "171/171 [==============================] - 9s 52ms/step - loss: 318.6028 - val_loss: 681.4120\n",
      "Epoch 14/25\n",
      "171/171 [==============================] - 9s 53ms/step - loss: 318.8466 - val_loss: 343.8354\n",
      "Epoch 15/25\n",
      "171/171 [==============================] - 9s 54ms/step - loss: 287.6513 - val_loss: 415.1309\n",
      "Epoch 16/25\n",
      "171/171 [==============================] - 10s 57ms/step - loss: 256.1986 - val_loss: 310.7677\n",
      "Epoch 17/25\n",
      "171/171 [==============================] - 10s 57ms/step - loss: 271.9167 - val_loss: 310.6929\n",
      "Epoch 18/25\n",
      "171/171 [==============================] - 10s 57ms/step - loss: 261.4482 - val_loss: 456.3062\n",
      "Epoch 19/25\n",
      "171/171 [==============================] - 9s 54ms/step - loss: 262.2945 - val_loss: 326.4896\n",
      "Epoch 20/25\n",
      "171/171 [==============================] - 10s 56ms/step - loss: 246.0382 - val_loss: 396.3039\n",
      "Epoch 21/25\n",
      "171/171 [==============================] - 10s 59ms/step - loss: 242.8047 - val_loss: 383.2318\n",
      "Epoch 22/25\n",
      "171/171 [==============================] - 9s 55ms/step - loss: 249.5944 - val_loss: 295.2867\n",
      "Epoch 23/25\n",
      "171/171 [==============================] - 9s 54ms/step - loss: 235.5529 - val_loss: 322.4879\n",
      "Epoch 24/25\n",
      "171/171 [==============================] - 9s 55ms/step - loss: 236.7864 - val_loss: 387.0599\n",
      "Epoch 25/25\n",
      "171/171 [==============================] - 9s 53ms/step - loss: 225.9427 - val_loss: 314.5738\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "86/86 [==============================] - 3s 29ms/step - loss: 8528.6055 - val_loss: 5081.5752\n",
      "Epoch 2/25\n",
      "86/86 [==============================] - 2s 25ms/step - loss: 7133.4512 - val_loss: 4408.0503\n",
      "Epoch 3/25\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 6335.1724 - val_loss: 3858.2576\n",
      "Epoch 4/25\n",
      "86/86 [==============================] - 2s 27ms/step - loss: 5654.4014 - val_loss: 3395.8147\n",
      "Epoch 5/25\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 5062.9878 - val_loss: 3007.0627\n",
      "Epoch 6/25\n",
      "86/86 [==============================] - 2s 27ms/step - loss: 4549.9746 - val_loss: 2682.0125\n",
      "Epoch 7/25\n",
      "86/86 [==============================] - 2s 25ms/step - loss: 4107.8164 - val_loss: 2413.4663\n",
      "Epoch 8/25\n",
      "86/86 [==============================] - 2s 27ms/step - loss: 3723.8599 - val_loss: 2194.1416\n",
      "Epoch 9/25\n",
      "86/86 [==============================] - 2s 25ms/step - loss: 3392.1360 - val_loss: 2018.8237\n",
      "Epoch 10/25\n",
      "86/86 [==============================] - 2s 27ms/step - loss: 3108.3618 - val_loss: 1882.1997\n",
      "Epoch 11/25\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 2869.2969 - val_loss: 1778.7979\n",
      "Epoch 12/25\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 2668.4167 - val_loss: 1703.6635\n",
      "Epoch 13/25\n",
      "86/86 [==============================] - 2s 25ms/step - loss: 2506.1492 - val_loss: 1653.0000\n",
      "Epoch 14/25\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 2366.6133 - val_loss: 1622.2344\n",
      "Epoch 15/25\n",
      "86/86 [==============================] - 2s 25ms/step - loss: 2258.8420 - val_loss: 1607.7466\n",
      "Epoch 16/25\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 2169.6636 - val_loss: 1606.0032\n",
      "Epoch 17/25\n",
      "86/86 [==============================] - 2s 25ms/step - loss: 2097.8037 - val_loss: 1613.9297\n",
      "Epoch 18/25\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 2043.8885 - val_loss: 1628.9904\n",
      "Epoch 19/25\n",
      "86/86 [==============================] - 2s 25ms/step - loss: 2004.1433 - val_loss: 1649.1072\n",
      "Epoch 20/25\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 1971.4855 - val_loss: 1671.8577\n",
      "Epoch 21/25\n",
      "86/86 [==============================] - 2s 27ms/step - loss: 1945.1908 - val_loss: 1689.5197\n",
      "Epoch 22/25\n",
      "86/86 [==============================] - 2s 25ms/step - loss: 1883.9258 - val_loss: 1551.2681\n",
      "Epoch 23/25\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 1704.4939 - val_loss: 1159.6093\n",
      "Epoch 24/25\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 1154.1554 - val_loss: 705.5455\n",
      "Epoch 25/25\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 921.2819 - val_loss: 608.4102\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "682/682 [==============================] - 5s 7ms/step - loss: 5715.4014 - val_loss: 2239.3010\n",
      "Epoch 2/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 2722.5981 - val_loss: 1569.2292\n",
      "Epoch 3/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 1454.4990 - val_loss: 620.5720\n",
      "Epoch 4/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 644.0189 - val_loss: 569.5540\n",
      "Epoch 5/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 418.0562 - val_loss: 362.5053\n",
      "Epoch 6/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 317.1638 - val_loss: 364.4966\n",
      "Epoch 7/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 275.3154 - val_loss: 341.9785\n",
      "Epoch 8/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 256.1118 - val_loss: 326.3102\n",
      "Epoch 9/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 244.0792 - val_loss: 381.8810\n",
      "Epoch 10/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 235.3801 - val_loss: 358.7970\n",
      "Epoch 11/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 233.1267 - val_loss: 367.4923\n",
      "Epoch 12/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 232.3873 - val_loss: 310.9228\n",
      "Epoch 13/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 231.7571 - val_loss: 313.1102\n",
      "Epoch 14/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 224.5362 - val_loss: 311.7652\n",
      "Epoch 15/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 218.7927 - val_loss: 417.8338\n",
      "Epoch 16/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 221.6746 - val_loss: 283.2080\n",
      "Epoch 17/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 218.2432 - val_loss: 308.4026\n",
      "Epoch 18/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 211.3734 - val_loss: 325.4069\n",
      "Epoch 19/25\n",
      "682/682 [==============================] - 4s 7ms/step - loss: 203.6827 - val_loss: 250.9521\n",
      "Epoch 20/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 206.4325 - val_loss: 234.1705\n",
      "Epoch 21/25\n",
      "682/682 [==============================] - 4s 6ms/step - loss: 202.3981 - val_loss: 217.1172\n",
      "Epoch 22/25\n",
      "682/682 [==============================] - 5s 7ms/step - loss: 194.6301 - val_loss: 209.8436\n",
      "Epoch 23/25\n",
      "682/682 [==============================] - 5s 7ms/step - loss: 192.4053 - val_loss: 229.6588\n",
      "Epoch 24/25\n",
      "682/682 [==============================] - 5s 7ms/step - loss: 188.2403 - val_loss: 235.5316\n",
      "Epoch 25/25\n",
      "682/682 [==============================] - 5s 7ms/step - loss: 182.0597 - val_loss: 197.6632\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "341/341 [==============================] - 16s 46ms/step - loss: 3682.3857 - val_loss: 1630.0413\n",
      "Epoch 2/25\n",
      "341/341 [==============================] - 15s 45ms/step - loss: 1909.5690 - val_loss: 1845.3810\n",
      "Epoch 3/25\n",
      "341/341 [==============================] - 15s 45ms/step - loss: 1765.0189 - val_loss: 1280.0524\n",
      "Epoch 4/25\n",
      "341/341 [==============================] - 15s 45ms/step - loss: 666.9305 - val_loss: 718.6156\n",
      "Epoch 5/25\n",
      "341/341 [==============================] - 15s 44ms/step - loss: 425.2955 - val_loss: 379.0273\n",
      "Epoch 6/25\n",
      "341/341 [==============================] - 16s 46ms/step - loss: 332.4172 - val_loss: 389.9720\n",
      "Epoch 7/25\n",
      "341/341 [==============================] - 16s 46ms/step - loss: 303.3638 - val_loss: 419.1635\n",
      "Epoch 8/25\n",
      "341/341 [==============================] - 16s 46ms/step - loss: 283.5962 - val_loss: 370.1927\n",
      "Epoch 9/25\n",
      "341/341 [==============================] - 15s 44ms/step - loss: 270.1902 - val_loss: 355.6338\n",
      "Epoch 10/25\n",
      "341/341 [==============================] - 15s 45ms/step - loss: 272.7174 - val_loss: 371.0568\n",
      "Epoch 11/25\n",
      "341/341 [==============================] - 15s 44ms/step - loss: 257.3857 - val_loss: 345.4957\n",
      "Epoch 12/25\n",
      "341/341 [==============================] - 14s 42ms/step - loss: 245.1893 - val_loss: 318.6351\n",
      "Epoch 13/25\n",
      "341/341 [==============================] - 15s 43ms/step - loss: 271.0419 - val_loss: 415.9351\n",
      "Epoch 14/25\n",
      "341/341 [==============================] - 14s 41ms/step - loss: 248.2632 - val_loss: 300.9421\n",
      "Epoch 15/25\n",
      "341/341 [==============================] - 14s 41ms/step - loss: 232.1512 - val_loss: 396.8129\n",
      "Epoch 16/25\n",
      "341/341 [==============================] - 14s 42ms/step - loss: 230.6835 - val_loss: 271.1677\n",
      "Epoch 17/25\n",
      "341/341 [==============================] - 14s 42ms/step - loss: 250.2551 - val_loss: 266.0769\n",
      "Epoch 18/25\n",
      "341/341 [==============================] - 14s 41ms/step - loss: 219.0187 - val_loss: 319.8904\n",
      "Epoch 19/25\n",
      "341/341 [==============================] - 14s 42ms/step - loss: 212.7691 - val_loss: 266.4350\n",
      "Epoch 20/25\n",
      "341/341 [==============================] - 14s 42ms/step - loss: 212.9206 - val_loss: 218.8185\n",
      "Epoch 21/25\n",
      "341/341 [==============================] - 15s 43ms/step - loss: 198.5755 - val_loss: 228.8498\n",
      "Epoch 22/25\n",
      "341/341 [==============================] - 14s 41ms/step - loss: 192.4692 - val_loss: 193.0494\n",
      "Epoch 23/25\n",
      "341/341 [==============================] - 14s 40ms/step - loss: 188.6419 - val_loss: 282.2767\n",
      "Epoch 24/25\n",
      "341/341 [==============================] - 14s 40ms/step - loss: 186.2879 - val_loss: 208.4868\n",
      "Epoch 25/25\n",
      "341/341 [==============================] - 14s 41ms/step - loss: 186.3324 - val_loss: 215.1824\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "86/86 [==============================] - 10s 101ms/step - loss: 6371.4048 - val_loss: 2912.3904\n",
      "Epoch 2/25\n",
      "86/86 [==============================] - 9s 104ms/step - loss: 3793.3660 - val_loss: 1927.9968\n",
      "Epoch 3/25\n",
      "86/86 [==============================] - 11s 132ms/step - loss: 2651.1311 - val_loss: 1624.7052\n",
      "Epoch 4/25\n",
      "86/86 [==============================] - 11s 124ms/step - loss: 2147.7664 - val_loss: 1631.0671\n",
      "Epoch 5/25\n",
      "86/86 [==============================] - 11s 130ms/step - loss: 1940.9763 - val_loss: 1489.5175\n",
      "Epoch 6/25\n",
      "86/86 [==============================] - 11s 124ms/step - loss: 1487.0537 - val_loss: 1057.7864\n",
      "Epoch 7/25\n",
      "86/86 [==============================] - 11s 131ms/step - loss: 1039.0880 - val_loss: 647.0356\n",
      "Epoch 8/25\n",
      "86/86 [==============================] - 10s 122ms/step - loss: 671.8534 - val_loss: 603.5012\n",
      "Epoch 9/25\n",
      "86/86 [==============================] - 10s 119ms/step - loss: 579.5356 - val_loss: 535.6315\n",
      "Epoch 10/25\n",
      "86/86 [==============================] - 11s 125ms/step - loss: 512.0096 - val_loss: 527.9387\n",
      "Epoch 11/25\n",
      "86/86 [==============================] - 11s 126ms/step - loss: 468.0530 - val_loss: 735.7567\n",
      "Epoch 12/25\n",
      "86/86 [==============================] - 10s 121ms/step - loss: 448.5802 - val_loss: 454.0428\n",
      "Epoch 13/25\n",
      "86/86 [==============================] - 11s 130ms/step - loss: 461.6472 - val_loss: 707.6855\n",
      "Epoch 14/25\n",
      "86/86 [==============================] - 11s 131ms/step - loss: 392.5345 - val_loss: 435.8452\n",
      "Epoch 15/25\n",
      "86/86 [==============================] - 11s 133ms/step - loss: 372.1248 - val_loss: 601.9688\n",
      "Epoch 16/25\n",
      "86/86 [==============================] - 11s 132ms/step - loss: 376.1171 - val_loss: 343.7015\n",
      "Epoch 17/25\n",
      "86/86 [==============================] - 11s 128ms/step - loss: 338.1540 - val_loss: 400.7995\n",
      "Epoch 18/25\n",
      "86/86 [==============================] - 11s 132ms/step - loss: 334.4884 - val_loss: 764.8015\n",
      "Epoch 19/25\n",
      "86/86 [==============================] - 11s 125ms/step - loss: 306.1862 - val_loss: 357.2916\n",
      "Epoch 20/25\n",
      "86/86 [==============================] - 11s 133ms/step - loss: 282.6784 - val_loss: 502.9268\n",
      "Epoch 21/25\n",
      "86/86 [==============================] - 10s 121ms/step - loss: 287.6328 - val_loss: 360.3148\n",
      "Epoch 22/25\n",
      "86/86 [==============================] - 10s 121ms/step - loss: 278.3080 - val_loss: 331.7893\n",
      "Epoch 23/25\n",
      "86/86 [==============================] - 11s 127ms/step - loss: 262.9225 - val_loss: 394.2707\n",
      "Epoch 24/25\n",
      "86/86 [==============================] - 10s 118ms/step - loss: 244.4088 - val_loss: 357.1741\n",
      "Epoch 25/25\n",
      "86/86 [==============================] - 10s 121ms/step - loss: 261.3913 - val_loss: 394.3728\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "682/682 [==============================] - 15s 20ms/step - loss: 3992.4214 - val_loss: 1572.0875\n",
      "Epoch 2/25\n",
      "682/682 [==============================] - 15s 22ms/step - loss: 1117.1642 - val_loss: 570.1330\n",
      "Epoch 3/25\n",
      "682/682 [==============================] - 14s 20ms/step - loss: 522.6822 - val_loss: 601.4791\n",
      "Epoch 4/25\n",
      "682/682 [==============================] - 13s 19ms/step - loss: 428.7249 - val_loss: 958.2813\n",
      "Epoch 5/25\n",
      "682/682 [==============================] - 14s 21ms/step - loss: 358.9098 - val_loss: 320.0907\n",
      "Epoch 6/25\n",
      "682/682 [==============================] - 14s 20ms/step - loss: 299.4584 - val_loss: 349.5161\n",
      "Epoch 7/25\n",
      "682/682 [==============================] - 14s 20ms/step - loss: 269.8405 - val_loss: 365.8264\n",
      "Epoch 8/25\n",
      "682/682 [==============================] - 13s 20ms/step - loss: 249.4853 - val_loss: 313.4822\n",
      "Epoch 9/25\n",
      "682/682 [==============================] - 14s 21ms/step - loss: 238.1142 - val_loss: 383.1554\n",
      "Epoch 10/25\n",
      "682/682 [==============================] - 14s 21ms/step - loss: 225.2004 - val_loss: 328.1211\n",
      "Epoch 11/25\n",
      "682/682 [==============================] - 14s 20ms/step - loss: 220.7721 - val_loss: 276.5284\n",
      "Epoch 12/25\n",
      "682/682 [==============================] - 14s 21ms/step - loss: 220.4277 - val_loss: 255.7229\n",
      "Epoch 13/25\n",
      "682/682 [==============================] - 14s 21ms/step - loss: 220.2847 - val_loss: 278.7142\n",
      "Epoch 14/25\n",
      "682/682 [==============================] - 13s 20ms/step - loss: 209.1867 - val_loss: 305.1967\n",
      "Epoch 15/25\n",
      "682/682 [==============================] - 14s 20ms/step - loss: 204.6581 - val_loss: 315.7923\n",
      "Epoch 16/25\n",
      "682/682 [==============================] - 13s 19ms/step - loss: 206.0104 - val_loss: 228.1337\n",
      "Epoch 17/25\n",
      "682/682 [==============================] - 14s 20ms/step - loss: 201.2066 - val_loss: 267.1503\n",
      "Epoch 18/25\n",
      "682/682 [==============================] - 14s 21ms/step - loss: 197.8374 - val_loss: 251.5501\n",
      "Epoch 19/25\n",
      "682/682 [==============================] - 14s 20ms/step - loss: 192.5359 - val_loss: 240.8839\n",
      "Epoch 20/25\n",
      "682/682 [==============================] - 14s 21ms/step - loss: 193.7453 - val_loss: 220.5689\n",
      "Epoch 21/25\n",
      "682/682 [==============================] - 14s 21ms/step - loss: 188.6179 - val_loss: 194.9136\n",
      "Epoch 22/25\n",
      "682/682 [==============================] - 14s 21ms/step - loss: 183.8118 - val_loss: 192.3204\n",
      "Epoch 23/25\n",
      "682/682 [==============================] - 14s 21ms/step - loss: 182.6518 - val_loss: 238.2062\n",
      "Epoch 24/25\n",
      "682/682 [==============================] - 15s 22ms/step - loss: 176.5120 - val_loss: 220.3016\n",
      "Epoch 25/25\n",
      "682/682 [==============================] - 15s 22ms/step - loss: 173.1090 - val_loss: 252.8528\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "682/682 [==============================] - 15s 21ms/step - loss: 3960.1357 - val_loss: 1608.9353\n",
      "Epoch 2/25\n",
      "682/682 [==============================] - 14s 20ms/step - loss: 1859.7234 - val_loss: 871.2480\n",
      "Epoch 3/25\n",
      "682/682 [==============================] - 13s 20ms/step - loss: 711.8262 - val_loss: 736.9143\n",
      "Epoch 4/25\n",
      "682/682 [==============================] - 14s 21ms/step - loss: 449.7513 - val_loss: 708.7947\n",
      "Epoch 5/25\n",
      "682/682 [==============================] - 14s 21ms/step - loss: 356.4916 - val_loss: 363.3398\n",
      "Epoch 6/25\n",
      "682/682 [==============================] - 14s 21ms/step - loss: 295.1118 - val_loss: 302.6608\n",
      "Epoch 7/25\n",
      "682/682 [==============================] - 14s 20ms/step - loss: 268.8577 - val_loss: 370.3112\n",
      "Epoch 8/25\n",
      "682/682 [==============================] - 15s 21ms/step - loss: 252.1080 - val_loss: 278.7422\n",
      "Epoch 9/25\n",
      "682/682 [==============================] - 14s 21ms/step - loss: 237.4047 - val_loss: 400.5534\n",
      "Epoch 10/25\n",
      "682/682 [==============================] - 15s 22ms/step - loss: 235.4330 - val_loss: 309.8496\n",
      "Epoch 11/25\n",
      "682/682 [==============================] - 14s 21ms/step - loss: 226.9613 - val_loss: 310.3364\n",
      "Epoch 12/25\n",
      "682/682 [==============================] - 15s 22ms/step - loss: 223.8126 - val_loss: 252.2674\n",
      "Epoch 13/25\n",
      "682/682 [==============================] - 14s 21ms/step - loss: 224.7795 - val_loss: 253.0349\n",
      "Epoch 14/25\n",
      "682/682 [==============================] - 16s 23ms/step - loss: 215.1452 - val_loss: 251.2135\n",
      "Epoch 15/25\n",
      "682/682 [==============================] - 17s 25ms/step - loss: 209.4515 - val_loss: 279.0717\n",
      "Epoch 16/25\n",
      "682/682 [==============================] - 18s 27ms/step - loss: 214.2320 - val_loss: 224.4965\n",
      "Epoch 17/25\n",
      "682/682 [==============================] - 17s 24ms/step - loss: 209.4535 - val_loss: 233.6709\n",
      "Epoch 18/25\n",
      "682/682 [==============================] - 19s 27ms/step - loss: 205.3175 - val_loss: 293.7945\n",
      "Epoch 19/25\n",
      "682/682 [==============================] - 19s 27ms/step - loss: 202.2673 - val_loss: 245.6968\n",
      "Epoch 20/25\n",
      "682/682 [==============================] - 19s 28ms/step - loss: 205.2118 - val_loss: 227.8606\n",
      "Epoch 21/25\n",
      "682/682 [==============================] - 17s 25ms/step - loss: 198.4434 - val_loss: 209.1662\n",
      "Epoch 22/25\n",
      "682/682 [==============================] - 18s 27ms/step - loss: 194.9358 - val_loss: 207.6354\n",
      "Epoch 23/25\n",
      "682/682 [==============================] - 13s 19ms/step - loss: 192.6003 - val_loss: 232.9120\n",
      "Epoch 24/25\n",
      "682/682 [==============================] - 13s 19ms/step - loss: 188.1862 - val_loss: 239.8679\n",
      "Epoch 25/25\n",
      "682/682 [==============================] - 18s 27ms/step - loss: 185.1567 - val_loss: 256.3418\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "171/171 [==============================] - 6s 31ms/step - loss: 8066.9424 - val_loss: 4564.4302\n",
      "Epoch 2/25\n",
      "171/171 [==============================] - 5s 27ms/step - loss: 6180.4009 - val_loss: 3516.5637\n",
      "Epoch 3/25\n",
      "171/171 [==============================] - 4s 23ms/step - loss: 4941.8853 - val_loss: 2755.6968\n",
      "Epoch 4/25\n",
      "171/171 [==============================] - 4s 24ms/step - loss: 4014.2764 - val_loss: 2242.1584\n",
      "Epoch 5/25\n",
      "171/171 [==============================] - 4s 26ms/step - loss: 3325.6477 - val_loss: 1912.4100\n",
      "Epoch 6/25\n",
      "171/171 [==============================] - 4s 25ms/step - loss: 2828.5835 - val_loss: 1721.0721\n",
      "Epoch 7/25\n",
      "171/171 [==============================] - 4s 26ms/step - loss: 2484.4026 - val_loss: 1629.2133\n",
      "Epoch 8/25\n",
      "171/171 [==============================] - 4s 26ms/step - loss: 2250.2725 - val_loss: 1605.3654\n",
      "Epoch 9/25\n",
      "171/171 [==============================] - 5s 29ms/step - loss: 2101.1909 - val_loss: 1623.6500\n",
      "Epoch 10/25\n",
      "171/171 [==============================] - 5s 28ms/step - loss: 1992.7738 - val_loss: 1486.7216\n",
      "Epoch 11/25\n",
      "171/171 [==============================] - 5s 29ms/step - loss: 1690.1702 - val_loss: 1186.0155\n",
      "Epoch 12/25\n",
      "171/171 [==============================] - 5s 28ms/step - loss: 1120.3446 - val_loss: 654.7979\n",
      "Epoch 13/25\n",
      "171/171 [==============================] - 5s 30ms/step - loss: 847.0740 - val_loss: 611.2250\n",
      "Epoch 14/25\n",
      "171/171 [==============================] - 5s 26ms/step - loss: 669.1779 - val_loss: 453.5502\n",
      "Epoch 15/25\n",
      "171/171 [==============================] - 5s 28ms/step - loss: 556.6062 - val_loss: 415.4090\n",
      "Epoch 16/25\n",
      "171/171 [==============================] - 5s 28ms/step - loss: 443.4771 - val_loss: 326.4230\n",
      "Epoch 17/25\n",
      "171/171 [==============================] - 5s 28ms/step - loss: 387.0915 - val_loss: 375.3839\n",
      "Epoch 18/25\n",
      "171/171 [==============================] - 5s 28ms/step - loss: 353.6863 - val_loss: 336.2007\n",
      "Epoch 19/25\n",
      "171/171 [==============================] - 5s 30ms/step - loss: 327.6394 - val_loss: 304.4316\n",
      "Epoch 20/25\n",
      "171/171 [==============================] - 4s 26ms/step - loss: 294.4069 - val_loss: 393.0261\n",
      "Epoch 21/25\n",
      "171/171 [==============================] - 5s 29ms/step - loss: 297.0524 - val_loss: 337.3993\n",
      "Epoch 22/25\n",
      "171/171 [==============================] - 5s 30ms/step - loss: 301.8148 - val_loss: 317.5191\n",
      "Epoch 23/25\n",
      "171/171 [==============================] - 5s 26ms/step - loss: 282.2366 - val_loss: 350.4101\n",
      "Epoch 24/25\n",
      "171/171 [==============================] - 5s 27ms/step - loss: 272.4588 - val_loss: 327.8545\n",
      "Epoch 25/25\n",
      "171/171 [==============================] - 5s 29ms/step - loss: 272.5197 - val_loss: 295.2080\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "682/682 [==============================] - 6s 7ms/step - loss: 7180.2222 - val_loss: 3611.1309\n",
      "Epoch 2/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 4609.6216 - val_loss: 2311.0671\n",
      "Epoch 3/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 3148.7908 - val_loss: 1736.7340\n",
      "Epoch 4/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 2381.1140 - val_loss: 1605.4648\n",
      "Epoch 5/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 2039.0017 - val_loss: 1679.9048\n",
      "Epoch 6/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 1730.2883 - val_loss: 746.9250\n",
      "Epoch 7/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 771.8685 - val_loss: 560.3184\n",
      "Epoch 8/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 526.0588 - val_loss: 495.9273\n",
      "Epoch 9/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 423.1512 - val_loss: 342.7506\n",
      "Epoch 10/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 364.8556 - val_loss: 395.4157\n",
      "Epoch 11/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 314.1063 - val_loss: 324.9457\n",
      "Epoch 12/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 295.6209 - val_loss: 315.5670\n",
      "Epoch 13/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 279.7390 - val_loss: 350.2279\n",
      "Epoch 14/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 273.9083 - val_loss: 306.7023\n",
      "Epoch 15/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 264.8235 - val_loss: 347.6689\n",
      "Epoch 16/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 268.4474 - val_loss: 286.3955\n",
      "Epoch 17/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 267.6026 - val_loss: 297.1175\n",
      "Epoch 18/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 254.6564 - val_loss: 410.7765\n",
      "Epoch 19/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 253.8525 - val_loss: 265.3749\n",
      "Epoch 20/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 258.3116 - val_loss: 315.7343\n",
      "Epoch 21/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 247.8480 - val_loss: 287.9712\n",
      "Epoch 22/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 248.1857 - val_loss: 355.6234\n",
      "Epoch 23/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 245.3797 - val_loss: 283.4572\n",
      "Epoch 24/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 243.4112 - val_loss: 263.2172\n",
      "Epoch 25/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 239.8385 - val_loss: 266.4304\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "iteration  20\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "341/341 [==============================] - 3s 6ms/step - loss: 8336.3594 - val_loss: 4836.3779\n",
      "Epoch 2/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 6512.7876 - val_loss: 3743.9573\n",
      "Epoch 3/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 5240.5820 - val_loss: 2945.3040\n",
      "Epoch 4/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 4262.7246 - val_loss: 2376.8835\n",
      "Epoch 5/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 3504.9514 - val_loss: 1994.1932\n",
      "Epoch 6/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 2947.1086 - val_loss: 1760.3796\n",
      "Epoch 7/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 2469.9736 - val_loss: 1406.3114\n",
      "Epoch 8/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 1965.8682 - val_loss: 1020.9342\n",
      "Epoch 9/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 1420.1385 - val_loss: 770.7236\n",
      "Epoch 10/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 1103.1782 - val_loss: 633.2212\n",
      "Epoch 11/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 871.3188 - val_loss: 579.1355\n",
      "Epoch 12/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 685.8826 - val_loss: 485.9420\n",
      "Epoch 13/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 534.9269 - val_loss: 390.4077\n",
      "Epoch 14/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 437.8557 - val_loss: 352.5048\n",
      "Epoch 15/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 368.6284 - val_loss: 410.3897\n",
      "Epoch 16/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 325.7275 - val_loss: 338.4149\n",
      "Epoch 17/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 302.8891 - val_loss: 309.1447\n",
      "Epoch 18/25\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 277.8208 - val_loss: 367.9096\n",
      "Epoch 19/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 268.9402 - val_loss: 357.6188\n",
      "Epoch 20/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 264.5574 - val_loss: 364.6868\n",
      "Epoch 21/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 257.3150 - val_loss: 309.1614\n",
      "Epoch 22/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 261.2265 - val_loss: 320.3648\n",
      "Epoch 23/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 252.2762 - val_loss: 358.0407\n",
      "Epoch 24/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 250.8382 - val_loss: 341.0289\n",
      "Epoch 25/25\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 243.7302 - val_loss: 290.0700\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "171/171 [==============================] - 3s 10ms/step - loss: 8611.1436 - val_loss: 5255.0752\n",
      "Epoch 2/25\n",
      "171/171 [==============================] - 1s 8ms/step - loss: 7358.3633 - val_loss: 4581.4424\n",
      "Epoch 3/25\n",
      "171/171 [==============================] - 1s 8ms/step - loss: 6561.3018 - val_loss: 4020.4519\n",
      "Epoch 4/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 5868.2817 - val_loss: 3542.2122\n",
      "Epoch 5/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 5263.1694 - val_loss: 3135.3274\n",
      "Epoch 6/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 4730.3716 - val_loss: 2791.2725\n",
      "Epoch 7/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 4268.7515 - val_loss: 2503.2122\n",
      "Epoch 8/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 3862.7905 - val_loss: 2265.0874\n",
      "Epoch 9/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 3507.0247 - val_loss: 2072.3813\n",
      "Epoch 10/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 3209.4307 - val_loss: 1920.2593\n",
      "Epoch 11/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 2950.1753 - val_loss: 1803.6656\n",
      "Epoch 12/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 2743.0337 - val_loss: 1718.1346\n",
      "Epoch 13/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 2560.4868 - val_loss: 1659.7478\n",
      "Epoch 14/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 2407.5264 - val_loss: 1624.1674\n",
      "Epoch 15/25\n",
      "171/171 [==============================] - 1s 8ms/step - loss: 2291.1125 - val_loss: 1607.7102\n",
      "Epoch 16/25\n",
      "171/171 [==============================] - 1s 8ms/step - loss: 2196.9060 - val_loss: 1606.4192\n",
      "Epoch 17/25\n",
      "171/171 [==============================] - 1s 8ms/step - loss: 2124.8047 - val_loss: 1616.7119\n",
      "Epoch 18/25\n",
      "171/171 [==============================] - 1s 8ms/step - loss: 2064.7542 - val_loss: 1635.6317\n",
      "Epoch 19/25\n",
      "171/171 [==============================] - 1s 8ms/step - loss: 2017.5443 - val_loss: 1660.5286\n",
      "Epoch 20/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 1995.9468 - val_loss: 1687.8158\n",
      "Epoch 21/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 1971.0869 - val_loss: 1621.6093\n",
      "Epoch 22/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 1774.4475 - val_loss: 1371.3119\n",
      "Epoch 23/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 1555.5229 - val_loss: 881.6653\n",
      "Epoch 24/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 1030.1542 - val_loss: 663.4147\n",
      "Epoch 25/25\n",
      "171/171 [==============================] - 1s 7ms/step - loss: 871.4957 - val_loss: 639.9033\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "682/682 [==============================] - 4s 5ms/step - loss: 7160.7817 - val_loss: 3599.8784\n",
      "Epoch 2/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 4597.4077 - val_loss: 2305.1973\n",
      "Epoch 3/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 3141.9290 - val_loss: 1734.5524\n",
      "Epoch 4/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 2376.9634 - val_loss: 1605.5264\n",
      "Epoch 5/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 2037.6072 - val_loss: 1680.7334\n",
      "Epoch 6/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 1923.2688 - val_loss: 1782.0197\n",
      "Epoch 7/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 1900.0709 - val_loss: 1845.5238\n",
      "Epoch 8/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 1901.4553 - val_loss: 1862.6870\n",
      "Epoch 9/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 1700.1813 - val_loss: 638.6973\n",
      "Epoch 10/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 606.9774 - val_loss: 436.2446\n",
      "Epoch 11/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 419.6979 - val_loss: 363.3988\n",
      "Epoch 12/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 355.2285 - val_loss: 373.1010\n",
      "Epoch 13/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 331.5628 - val_loss: 432.7177\n",
      "Epoch 14/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 295.7483 - val_loss: 273.8412\n",
      "Epoch 15/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 276.1567 - val_loss: 395.3728\n",
      "Epoch 16/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 266.6732 - val_loss: 263.3997\n",
      "Epoch 17/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 262.2995 - val_loss: 265.2899\n",
      "Epoch 18/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 248.7040 - val_loss: 260.6004\n",
      "Epoch 19/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 245.0769 - val_loss: 287.6608\n",
      "Epoch 20/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 244.7126 - val_loss: 270.0185\n",
      "Epoch 21/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 239.0480 - val_loss: 226.8216\n",
      "Epoch 22/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 237.2978 - val_loss: 253.0077\n",
      "Epoch 23/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 229.8625 - val_loss: 250.3635\n",
      "Epoch 24/25\n",
      "682/682 [==============================] - 3s 5ms/step - loss: 229.2828 - val_loss: 255.1938\n",
      "Epoch 25/25\n",
      "682/682 [==============================] - 3s 4ms/step - loss: 226.0954 - val_loss: 245.8678\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "86/86 [==============================] - 3s 14ms/step - loss: 9283.3721 - val_loss: 5820.7388\n",
      "Epoch 2/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 8218.3721 - val_loss: 5365.8389\n",
      "Epoch 3/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 7710.1143 - val_loss: 5001.0405\n",
      "Epoch 4/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 7265.1357 - val_loss: 4673.3936\n",
      "Epoch 5/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 6858.6133 - val_loss: 4374.1021\n",
      "Epoch 6/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 6487.8374 - val_loss: 4098.5420\n",
      "Epoch 7/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 6135.7607 - val_loss: 3844.2444\n",
      "Epoch 8/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 5808.1245 - val_loss: 3609.0193\n",
      "Epoch 9/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 5496.1748 - val_loss: 3391.7336\n",
      "Epoch 10/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 5210.0215 - val_loss: 3191.6741\n",
      "Epoch 11/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 4938.3716 - val_loss: 3007.5259\n",
      "Epoch 12/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 4687.0801 - val_loss: 2838.2075\n",
      "Epoch 13/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 4451.7583 - val_loss: 2683.5793\n",
      "Epoch 14/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 4229.6484 - val_loss: 2542.2422\n",
      "Epoch 15/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 4024.6870 - val_loss: 2413.9148\n",
      "Epoch 16/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 3833.2446 - val_loss: 2297.6482\n",
      "Epoch 17/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 3657.8157 - val_loss: 2193.0583\n",
      "Epoch 18/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 3495.6384 - val_loss: 2098.9299\n",
      "Epoch 19/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 3335.2380 - val_loss: 2014.8344\n",
      "Epoch 20/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 3198.4370 - val_loss: 1940.7269\n",
      "Epoch 21/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 3064.7476 - val_loss: 1875.8071\n",
      "Epoch 22/25\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 2955.0901 - val_loss: 1819.3590\n",
      "Epoch 23/25\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 2842.4182 - val_loss: 1770.8662\n",
      "Epoch 24/25\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 2744.3704 - val_loss: 1729.3969\n",
      "Epoch 25/25\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 2660.1511 - val_loss: 1695.2800\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "682/682 [==============================] - 9s 13ms/step - loss: 4063.0754 - val_loss: 1606.0660\n",
      "Epoch 2/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 1321.2849 - val_loss: 578.0209\n",
      "Epoch 3/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 506.6787 - val_loss: 541.8831\n",
      "Epoch 4/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 371.1569 - val_loss: 1021.1069\n",
      "Epoch 5/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 288.6697 - val_loss: 291.4213\n",
      "Epoch 6/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 253.3909 - val_loss: 333.3431\n",
      "Epoch 7/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 233.3387 - val_loss: 293.2680\n",
      "Epoch 8/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 225.6192 - val_loss: 279.8898\n",
      "Epoch 9/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 219.4116 - val_loss: 306.1226\n",
      "Epoch 10/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 211.4100 - val_loss: 344.2840\n",
      "Epoch 11/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 211.1222 - val_loss: 283.1158\n",
      "Epoch 12/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 212.0165 - val_loss: 278.9979\n",
      "Epoch 13/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 209.3910 - val_loss: 320.7352\n",
      "Epoch 14/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 202.2490 - val_loss: 226.0768\n",
      "Epoch 15/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 199.2173 - val_loss: 336.8460\n",
      "Epoch 16/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 197.8566 - val_loss: 217.0891\n",
      "Epoch 17/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 193.6981 - val_loss: 239.7429\n",
      "Epoch 18/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 191.4509 - val_loss: 247.7840\n",
      "Epoch 19/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 184.7788 - val_loss: 243.0146\n",
      "Epoch 20/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 183.1271 - val_loss: 199.6228\n",
      "Epoch 21/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 178.8676 - val_loss: 173.4061\n",
      "Epoch 22/25\n",
      "682/682 [==============================] - 9s 13ms/step - loss: 172.8213 - val_loss: 186.1059\n",
      "Epoch 23/25\n",
      "682/682 [==============================] - 8s 12ms/step - loss: 169.3886 - val_loss: 239.4758\n",
      "Epoch 24/25\n",
      "682/682 [==============================] - 9s 13ms/step - loss: 164.2548 - val_loss: 191.4442\n",
      "Epoch 25/25\n",
      "682/682 [==============================] - 9s 14ms/step - loss: 161.6941 - val_loss: 210.1017\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "(21820, 30, 14) (21820, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "86/86 [==============================] - 9s 92ms/step - loss: 6298.3853 - val_loss: 2859.6711\n",
      "Epoch 2/25\n",
      "86/86 [==============================] - 7s 84ms/step - loss: 3736.2300 - val_loss: 1911.0948\n",
      "Epoch 3/25\n",
      "86/86 [==============================] - 8s 89ms/step - loss: 2615.7185 - val_loss: 1620.2853\n",
      "Epoch 4/25\n",
      "86/86 [==============================] - 8s 91ms/step - loss: 2121.7490 - val_loss: 1635.9164\n",
      "Epoch 5/25\n",
      "86/86 [==============================] - 8s 91ms/step - loss: 1944.3019 - val_loss: 1725.5441\n",
      "Epoch 6/25\n",
      "86/86 [==============================] - 8s 90ms/step - loss: 1850.9883 - val_loss: 1505.4371\n",
      "Epoch 7/25\n",
      "86/86 [==============================] - 8s 90ms/step - loss: 1357.9303 - val_loss: 921.8906\n",
      "Epoch 8/25\n",
      "86/86 [==============================] - 8s 92ms/step - loss: 844.3634 - val_loss: 732.2085\n",
      "Epoch 9/25\n",
      "86/86 [==============================] - 8s 91ms/step - loss: 627.8945 - val_loss: 544.9591\n",
      "Epoch 10/25\n",
      "86/86 [==============================] - 8s 98ms/step - loss: 563.8130 - val_loss: 638.5405\n",
      "Epoch 11/25\n",
      "86/86 [==============================] - 8s 98ms/step - loss: 484.8099 - val_loss: 447.1587\n",
      "Epoch 12/25\n",
      "86/86 [==============================] - 8s 98ms/step - loss: 421.5893 - val_loss: 473.6232\n",
      "Epoch 13/25\n",
      "86/86 [==============================] - 8s 94ms/step - loss: 378.1707 - val_loss: 836.9288\n",
      "Epoch 14/25\n",
      "86/86 [==============================] - 8s 93ms/step - loss: 414.9185 - val_loss: 509.8941\n",
      "Epoch 15/25\n",
      "86/86 [==============================] - 8s 97ms/step - loss: 375.4322 - val_loss: 555.4414\n",
      "Epoch 16/25\n",
      "86/86 [==============================] - 8s 94ms/step - loss: 345.5363 - val_loss: 344.5687\n",
      "Epoch 17/25\n",
      "86/86 [==============================] - 8s 92ms/step - loss: 297.7277 - val_loss: 328.0796\n",
      "Epoch 18/25\n",
      "86/86 [==============================] - 8s 93ms/step - loss: 285.0027 - val_loss: 483.4073\n",
      "Epoch 19/25\n",
      "86/86 [==============================] - 8s 96ms/step - loss: 280.6006 - val_loss: 327.6341\n",
      "Epoch 20/25\n",
      "86/86 [==============================] - 8s 91ms/step - loss: 269.9366 - val_loss: 362.7928\n",
      "Epoch 21/25\n",
      "86/86 [==============================] - 8s 92ms/step - loss: 263.0787 - val_loss: 361.3687\n",
      "Epoch 22/25\n",
      "86/86 [==============================] - 8s 92ms/step - loss: 271.4020 - val_loss: 314.5471\n",
      "Epoch 23/25\n",
      "86/86 [==============================] - 8s 92ms/step - loss: 252.6932 - val_loss: 503.7534\n",
      "Epoch 24/25\n",
      "86/86 [==============================] - 8s 91ms/step - loss: 242.5714 - val_loss: 326.9726\n",
      "Epoch 25/25\n",
      "86/86 [==============================] - 8s 90ms/step - loss: 265.0377 - val_loss: 344.0038\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "CPU times: user 1h 26min 58s, sys: 23min 43s, total: 1h 50min 42s\n",
      "Wall time: 2h 13min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = pd.DataFrame(columns=['RMSE', 'std_RMSE', \n",
    "                                'S_score','std_S_score',\n",
    "                                'MSE', 'std_MSE',\n",
    "                                'nodes', 'dropout',\n",
    "                                'activation', 'batch_size'])\n",
    "\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    if ITERATIONS < 10:\n",
    "        print('iteration ', i+1)\n",
    "    elif ((i+1) % 10 == 0):\n",
    "        print('iteration ', i+1)    \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    \n",
    "    # parameter's sample\n",
    "    weights_file = \"model_weight/weights_file_gru.h5\"\n",
    "    alpha = 0.3\n",
    "    sequence_length = 30\n",
    "    epochs = 25\n",
    "    nodes_per_layer = random.sample(nodes_list, 1)[0]\n",
    "    dropout = random.sample(dropouts, 1)[0]\n",
    "    activation = random.sample(activation_functions, 1)[0]\n",
    "    batch_size = random.sample(batch_size_list, 1)[0]\n",
    "    remaining_sensors = remaining_sensors\n",
    "    drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_gru_1layer(input_shape, nodes_per_layer[0], dropout,\n",
    "                             activation, weights_file)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "    \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14.059276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>338.142049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>197.663223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.669097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>375.602666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>215.182419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.343987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>422.703087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.749908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.494882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>453.122978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210.101654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.680173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>542.117846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>245.867767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE  nodes  \\\n",
       "12  14.059276       0.0  338.142049          0.0  197.663223      0.0   [64]   \n",
       "13  14.669097       0.0  375.602666          0.0  215.182419      0.0  [256]   \n",
       "4   14.343987       0.0  422.703087          0.0  205.749908      0.0  [128]   \n",
       "23  14.494882       0.0  453.122978          0.0  210.101654      0.0  [128]   \n",
       "21  15.680173       0.0  542.117846          0.0  245.867767      0.0   [32]   \n",
       "\n",
       "    dropout activation batch_size  \n",
       "12      0.1       tanh         32  \n",
       "13      0.3       tanh         64  \n",
       "4       0.1       tanh         32  \n",
       "23      0.1       tanh         32  \n",
       "21      0.1       tanh         32  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='S_score').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"fd003_result/optim_result_1layer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61249, 27) (41214, 26) (248, 1)\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "# Data loading\n",
    "\n",
    "train, test, y_test = prepare_data('FD004.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50','P30','Nf','Nc','Ps30','phi',\n",
    "                'NRf','NRc','BPR','htBleed','W31','W32'] # selection based on main_notebook\n",
    "\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_piecewise = 130\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "423/423 [==============================] - 23s 51ms/step - loss: 3340.7241 - val_loss: 2078.6707\n",
      "Epoch 2/25\n",
      "423/423 [==============================] - 22s 52ms/step - loss: 1249.3040 - val_loss: 1616.1948\n",
      "Epoch 3/25\n",
      "423/423 [==============================] - 22s 51ms/step - loss: 636.8220 - val_loss: 1842.1812\n",
      "Epoch 4/25\n",
      "423/423 [==============================] - 22s 52ms/step - loss: 523.3027 - val_loss: 2083.7278\n",
      "Epoch 5/25\n",
      "423/423 [==============================] - 23s 54ms/step - loss: 443.0068 - val_loss: 1945.6218\n",
      "Epoch 6/25\n",
      "423/423 [==============================] - 23s 53ms/step - loss: 389.4566 - val_loss: 1838.3813\n",
      "Epoch 7/25\n",
      "423/423 [==============================] - 22s 52ms/step - loss: 355.4983 - val_loss: 1785.2485\n",
      "Epoch 8/25\n",
      "423/423 [==============================] - 22s 51ms/step - loss: 351.1451 - val_loss: 1791.8473\n",
      "Epoch 9/25\n",
      "423/423 [==============================] - 22s 52ms/step - loss: 338.0566 - val_loss: 1716.9775\n",
      "Epoch 10/25\n",
      "423/423 [==============================] - 23s 53ms/step - loss: 326.3628 - val_loss: 1675.6965\n",
      "Epoch 11/25\n",
      "423/423 [==============================] - 22s 52ms/step - loss: 326.3300 - val_loss: 1642.8876\n",
      "Epoch 12/25\n",
      "423/423 [==============================] - 22s 52ms/step - loss: 327.9814 - val_loss: 1626.0487\n",
      "Epoch 13/25\n",
      "423/423 [==============================] - 22s 51ms/step - loss: 321.8765 - val_loss: 1656.2673\n",
      "Epoch 14/25\n",
      "423/423 [==============================] - 23s 53ms/step - loss: 306.5242 - val_loss: 1596.3508\n",
      "Epoch 15/25\n",
      "423/423 [==============================] - 22s 53ms/step - loss: 304.0855 - val_loss: 1515.4845\n",
      "Epoch 16/25\n",
      "423/423 [==============================] - 22s 52ms/step - loss: 305.1388 - val_loss: 1284.1694\n",
      "Epoch 17/25\n",
      "423/423 [==============================] - 23s 55ms/step - loss: 295.1212 - val_loss: 1057.4423\n",
      "Epoch 18/25\n",
      "423/423 [==============================] - 22s 52ms/step - loss: 286.7880 - val_loss: 786.6232\n",
      "Epoch 19/25\n",
      "423/423 [==============================] - 22s 52ms/step - loss: 289.5453 - val_loss: 476.4057\n",
      "Epoch 20/25\n",
      "423/423 [==============================] - 22s 51ms/step - loss: 285.0754 - val_loss: 363.7718\n",
      "Epoch 21/25\n",
      "423/423 [==============================] - 22s 52ms/step - loss: 278.8671 - val_loss: 318.7750\n",
      "Epoch 22/25\n",
      "423/423 [==============================] - 22s 52ms/step - loss: 274.5040 - val_loss: 320.9483\n",
      "Epoch 23/25\n",
      "423/423 [==============================] - 22s 51ms/step - loss: 263.1188 - val_loss: 372.5485\n",
      "Epoch 24/25\n",
      "423/423 [==============================] - 22s 51ms/step - loss: 260.1618 - val_loss: 371.3416\n",
      "Epoch 25/25\n",
      "423/423 [==============================] - 22s 53ms/step - loss: 255.7436 - val_loss: 289.7457\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 3809.8682 - val_loss: 1975.5739\n",
      "Epoch 2/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 1051.2325 - val_loss: 593.4877\n",
      "Epoch 3/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 531.9995 - val_loss: 376.3770\n",
      "Epoch 4/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 427.4885 - val_loss: 417.8801\n",
      "Epoch 5/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 393.4548 - val_loss: 345.9165\n",
      "Epoch 6/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 381.7466 - val_loss: 326.7800\n",
      "Epoch 7/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 368.6269 - val_loss: 309.9922\n",
      "Epoch 8/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 353.7898 - val_loss: 329.8772\n",
      "Epoch 9/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 343.6234 - val_loss: 709.2051\n",
      "Epoch 10/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 334.8457 - val_loss: 804.2941\n",
      "Epoch 11/25\n",
      "1689/1689 [==============================] - 11s 7ms/step - loss: 329.5448 - val_loss: 750.9310\n",
      "Epoch 12/25\n",
      "1689/1689 [==============================] - 12s 7ms/step - loss: 322.6210 - val_loss: 758.8789\n",
      "Epoch 13/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 318.0712 - val_loss: 737.3107\n",
      "Epoch 14/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 313.9637 - val_loss: 673.2883\n",
      "Epoch 15/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 313.1384 - val_loss: 683.0418\n",
      "Epoch 16/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 307.2805 - val_loss: 740.6257\n",
      "Epoch 17/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 304.4662 - val_loss: 744.2572\n",
      "Epoch 18/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 303.2517 - val_loss: 510.2720\n",
      "Epoch 19/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 300.9751 - val_loss: 367.8136\n",
      "Epoch 20/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 296.3240 - val_loss: 373.7735\n",
      "Epoch 21/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 292.6095 - val_loss: 292.4274\n",
      "Epoch 22/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 292.0273 - val_loss: 333.5980\n",
      "Epoch 23/25\n",
      "1689/1689 [==============================] - 11s 7ms/step - loss: 288.2863 - val_loss: 426.6584\n",
      "Epoch 24/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 288.5447 - val_loss: 454.3281\n",
      "Epoch 25/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 289.4744 - val_loss: 292.1819\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "1689/1689 [==============================] - 13s 7ms/step - loss: 3731.2375 - val_loss: 1978.7288\n",
      "Epoch 2/25\n",
      "1689/1689 [==============================] - 11s 7ms/step - loss: 1032.4052 - val_loss: 455.7758\n",
      "Epoch 3/25\n",
      "1689/1689 [==============================] - 12s 7ms/step - loss: 441.3741 - val_loss: 356.7725\n",
      "Epoch 4/25\n",
      "1689/1689 [==============================] - 11s 7ms/step - loss: 374.7266 - val_loss: 360.2799\n",
      "Epoch 5/25\n",
      "1689/1689 [==============================] - 12s 7ms/step - loss: 352.4222 - val_loss: 316.0251\n",
      "Epoch 6/25\n",
      "1689/1689 [==============================] - 11s 7ms/step - loss: 340.9068 - val_loss: 322.1036\n",
      "Epoch 7/25\n",
      "1689/1689 [==============================] - 11s 7ms/step - loss: 331.3650 - val_loss: 344.8689\n",
      "Epoch 8/25\n",
      "1689/1689 [==============================] - 12s 7ms/step - loss: 317.4343 - val_loss: 286.1646\n",
      "Epoch 9/25\n",
      "1689/1689 [==============================] - 11s 7ms/step - loss: 308.7217 - val_loss: 282.2812\n",
      "Epoch 10/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 298.3571 - val_loss: 271.6287\n",
      "Epoch 11/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 292.9281 - val_loss: 260.4071\n",
      "Epoch 12/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 286.0966 - val_loss: 263.7406\n",
      "Epoch 13/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 280.5715 - val_loss: 254.3540\n",
      "Epoch 14/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 275.9572 - val_loss: 255.7543\n",
      "Epoch 15/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 273.6397 - val_loss: 244.7702\n",
      "Epoch 16/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 272.6419 - val_loss: 250.0448\n",
      "Epoch 17/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 270.0073 - val_loss: 245.8541\n",
      "Epoch 18/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 267.8531 - val_loss: 250.3624\n",
      "Epoch 19/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 266.0085 - val_loss: 235.3677\n",
      "Epoch 20/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 262.9658 - val_loss: 252.6846\n",
      "Epoch 21/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 263.2213 - val_loss: 250.8788\n",
      "Epoch 22/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 260.4302 - val_loss: 246.4271\n",
      "Epoch 23/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 258.9564 - val_loss: 267.1299\n",
      "Epoch 24/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 257.2614 - val_loss: 242.8276\n",
      "Epoch 25/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 257.9528 - val_loss: 248.9944\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "212/212 [==============================] - 8s 27ms/step - loss: 7771.4683 - val_loss: 5157.0229\n",
      "Epoch 2/25\n",
      "212/212 [==============================] - 6s 27ms/step - loss: 5699.2124 - val_loss: 3890.6309\n",
      "Epoch 3/25\n",
      "212/212 [==============================] - 7s 31ms/step - loss: 4374.8438 - val_loss: 3042.1321\n",
      "Epoch 4/25\n",
      "212/212 [==============================] - 8s 37ms/step - loss: 3446.7266 - val_loss: 2502.6086\n",
      "Epoch 5/25\n",
      "212/212 [==============================] - 8s 36ms/step - loss: 2818.6550 - val_loss: 2188.4609\n",
      "Epoch 6/25\n",
      "212/212 [==============================] - 7s 34ms/step - loss: 2411.5386 - val_loss: 2031.8319\n",
      "Epoch 7/25\n",
      "212/212 [==============================] - 7s 34ms/step - loss: 2161.8628 - val_loss: 1975.7805\n",
      "Epoch 8/25\n",
      "212/212 [==============================] - 8s 36ms/step - loss: 2022.8149 - val_loss: 1976.6229\n",
      "Epoch 9/25\n",
      "212/212 [==============================] - 8s 37ms/step - loss: 1945.8427 - val_loss: 2003.1340\n",
      "Epoch 10/25\n",
      "212/212 [==============================] - 8s 38ms/step - loss: 1912.0345 - val_loss: 2034.8961\n",
      "Epoch 11/25\n",
      "212/212 [==============================] - 8s 38ms/step - loss: 1802.8682 - val_loss: 1618.5924\n",
      "Epoch 12/25\n",
      "212/212 [==============================] - 8s 38ms/step - loss: 1106.6266 - val_loss: 1007.0466\n",
      "Epoch 13/25\n",
      "212/212 [==============================] - 8s 35ms/step - loss: 785.1544 - val_loss: 945.4293\n",
      "Epoch 14/25\n",
      "212/212 [==============================] - 8s 40ms/step - loss: 628.5126 - val_loss: 825.2115\n",
      "Epoch 15/25\n",
      "212/212 [==============================] - 8s 39ms/step - loss: 540.4299 - val_loss: 794.7758\n",
      "Epoch 16/25\n",
      "212/212 [==============================] - 8s 38ms/step - loss: 486.5124 - val_loss: 681.1882\n",
      "Epoch 17/25\n",
      "212/212 [==============================] - 9s 41ms/step - loss: 454.9362 - val_loss: 504.3629\n",
      "Epoch 18/25\n",
      "212/212 [==============================] - 9s 41ms/step - loss: 412.8059 - val_loss: 478.0636\n",
      "Epoch 19/25\n",
      "212/212 [==============================] - 9s 43ms/step - loss: 402.9816 - val_loss: 539.1232\n",
      "Epoch 20/25\n",
      "212/212 [==============================] - 8s 39ms/step - loss: 373.8984 - val_loss: 515.7526\n",
      "Epoch 21/25\n",
      "212/212 [==============================] - 8s 39ms/step - loss: 363.1832 - val_loss: 546.0396\n",
      "Epoch 22/25\n",
      "212/212 [==============================] - 8s 40ms/step - loss: 353.5620 - val_loss: 559.4094\n",
      "Epoch 23/25\n",
      "212/212 [==============================] - 8s 40ms/step - loss: 350.5002 - val_loss: 464.4597\n",
      "Epoch 24/25\n",
      "212/212 [==============================] - 8s 39ms/step - loss: 334.0960 - val_loss: 471.9838\n",
      "Epoch 25/25\n",
      "212/212 [==============================] - 8s 39ms/step - loss: 332.5839 - val_loss: 429.0242\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "212/212 [==============================] - 19s 83ms/step - loss: 6385.2256 - val_loss: 3617.1255\n",
      "Epoch 2/25\n",
      "212/212 [==============================] - 17s 81ms/step - loss: 3658.3428 - val_loss: 2385.5583\n",
      "Epoch 3/25\n",
      "212/212 [==============================] - 16s 77ms/step - loss: 2496.6714 - val_loss: 2003.7296\n",
      "Epoch 4/25\n",
      "212/212 [==============================] - 16s 77ms/step - loss: 2053.0781 - val_loss: 1870.7147\n",
      "Epoch 5/25\n",
      "212/212 [==============================] - 16s 74ms/step - loss: 1388.7278 - val_loss: 1137.9650\n",
      "Epoch 6/25\n",
      "212/212 [==============================] - 17s 80ms/step - loss: 831.5061 - val_loss: 831.4689\n",
      "Epoch 7/25\n",
      "212/212 [==============================] - 16s 76ms/step - loss: 622.3765 - val_loss: 635.1766\n",
      "Epoch 8/25\n",
      "212/212 [==============================] - 16s 77ms/step - loss: 522.4671 - val_loss: 609.5078\n",
      "Epoch 9/25\n",
      "212/212 [==============================] - 16s 77ms/step - loss: 452.1087 - val_loss: 472.0766\n",
      "Epoch 10/25\n",
      "212/212 [==============================] - 15s 73ms/step - loss: 403.7086 - val_loss: 552.0920\n",
      "Epoch 11/25\n",
      "212/212 [==============================] - 16s 74ms/step - loss: 387.3783 - val_loss: 498.2152\n",
      "Epoch 12/25\n",
      "212/212 [==============================] - 16s 74ms/step - loss: 385.0768 - val_loss: 345.6158\n",
      "Epoch 13/25\n",
      "212/212 [==============================] - 15s 72ms/step - loss: 359.0330 - val_loss: 468.8488\n",
      "Epoch 14/25\n",
      "212/212 [==============================] - 15s 73ms/step - loss: 358.0159 - val_loss: 416.9778\n",
      "Epoch 15/25\n",
      "212/212 [==============================] - 16s 74ms/step - loss: 348.4228 - val_loss: 342.5565\n",
      "Epoch 16/25\n",
      "212/212 [==============================] - 16s 74ms/step - loss: 351.3118 - val_loss: 385.6992\n",
      "Epoch 17/25\n",
      "212/212 [==============================] - 16s 76ms/step - loss: 354.9661 - val_loss: 404.5370\n",
      "Epoch 18/25\n",
      "212/212 [==============================] - 15s 73ms/step - loss: 340.9371 - val_loss: 340.9499\n",
      "Epoch 19/25\n",
      "212/212 [==============================] - 16s 77ms/step - loss: 332.7174 - val_loss: 341.3211\n",
      "Epoch 20/25\n",
      "212/212 [==============================] - 16s 76ms/step - loss: 332.8730 - val_loss: 405.9610\n",
      "Epoch 21/25\n",
      "212/212 [==============================] - 16s 76ms/step - loss: 331.7221 - val_loss: 441.6506\n",
      "Epoch 22/25\n",
      "212/212 [==============================] - 16s 77ms/step - loss: 331.0543 - val_loss: 335.6247\n",
      "Epoch 23/25\n",
      "212/212 [==============================] - 17s 79ms/step - loss: 327.1917 - val_loss: 317.3980\n",
      "Epoch 24/25\n",
      "212/212 [==============================] - 16s 75ms/step - loss: 323.0923 - val_loss: 340.6180\n",
      "Epoch 25/25\n",
      "212/212 [==============================] - 16s 78ms/step - loss: 321.0080 - val_loss: 305.0471\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "212/212 [==============================] - 10s 41ms/step - loss: 7618.4644 - val_loss: 5016.1191\n",
      "Epoch 2/25\n",
      "212/212 [==============================] - 8s 37ms/step - loss: 5550.3799 - val_loss: 3782.2078\n",
      "Epoch 3/25\n",
      "212/212 [==============================] - 7s 35ms/step - loss: 4261.6318 - val_loss: 2965.5007\n",
      "Epoch 4/25\n",
      "212/212 [==============================] - 8s 38ms/step - loss: 3370.3621 - val_loss: 2453.7864\n",
      "Epoch 5/25\n",
      "212/212 [==============================] - 8s 38ms/step - loss: 2776.8982 - val_loss: 2161.4607\n",
      "Epoch 6/25\n",
      "212/212 [==============================] - 8s 38ms/step - loss: 2393.5720 - val_loss: 2020.3760\n",
      "Epoch 7/25\n",
      "212/212 [==============================] - 8s 36ms/step - loss: 2167.4871 - val_loss: 1973.6782\n",
      "Epoch 8/25\n",
      "212/212 [==============================] - 8s 36ms/step - loss: 2042.5485 - val_loss: 1978.4790\n",
      "Epoch 9/25\n",
      "212/212 [==============================] - 8s 37ms/step - loss: 1973.2144 - val_loss: 2005.1915\n",
      "Epoch 10/25\n",
      "212/212 [==============================] - 8s 36ms/step - loss: 1946.8274 - val_loss: 2035.0509\n",
      "Epoch 11/25\n",
      "212/212 [==============================] - 7s 35ms/step - loss: 1930.0431 - val_loss: 2059.6917\n",
      "Epoch 12/25\n",
      "212/212 [==============================] - 8s 36ms/step - loss: 1931.4279 - val_loss: 2074.4661\n",
      "Epoch 13/25\n",
      "212/212 [==============================] - 7s 34ms/step - loss: 1815.2747 - val_loss: 1707.7198\n",
      "Epoch 14/25\n",
      "212/212 [==============================] - 7s 33ms/step - loss: 992.5314 - val_loss: 1024.7279\n",
      "Epoch 15/25\n",
      "212/212 [==============================] - 8s 36ms/step - loss: 728.1675 - val_loss: 1011.0696\n",
      "Epoch 16/25\n",
      "212/212 [==============================] - 7s 35ms/step - loss: 606.3734 - val_loss: 1007.8086\n",
      "Epoch 17/25\n",
      "212/212 [==============================] - 7s 34ms/step - loss: 543.8700 - val_loss: 766.5860\n",
      "Epoch 18/25\n",
      "212/212 [==============================] - 7s 35ms/step - loss: 466.8957 - val_loss: 807.9277\n",
      "Epoch 19/25\n",
      "212/212 [==============================] - 7s 32ms/step - loss: 431.6903 - val_loss: 609.9431\n",
      "Epoch 20/25\n",
      "212/212 [==============================] - 7s 35ms/step - loss: 406.8688 - val_loss: 596.1049\n",
      "Epoch 21/25\n",
      "212/212 [==============================] - 7s 32ms/step - loss: 394.6945 - val_loss: 479.8013\n",
      "Epoch 22/25\n",
      "212/212 [==============================] - 8s 36ms/step - loss: 385.5823 - val_loss: 437.8267\n",
      "Epoch 23/25\n",
      "212/212 [==============================] - 7s 34ms/step - loss: 375.1335 - val_loss: 433.3199\n",
      "Epoch 24/25\n",
      "212/212 [==============================] - 7s 35ms/step - loss: 372.8513 - val_loss: 501.5992\n",
      "Epoch 25/25\n",
      "212/212 [==============================] - 8s 36ms/step - loss: 374.3205 - val_loss: 472.3268\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "845/845 [==============================] - 8s 8ms/step - loss: 6809.1714 - val_loss: 3966.8120\n",
      "Epoch 2/25\n",
      "845/845 [==============================] - 7s 9ms/step - loss: 3995.6987 - val_loss: 2524.5933\n",
      "Epoch 3/25\n",
      "845/845 [==============================] - 7s 8ms/step - loss: 2666.1174 - val_loss: 2026.8280\n",
      "Epoch 4/25\n",
      "845/845 [==============================] - 7s 9ms/step - loss: 1945.6664 - val_loss: 1073.8754\n",
      "Epoch 5/25\n",
      "845/845 [==============================] - 7s 9ms/step - loss: 1037.4595 - val_loss: 688.8528\n",
      "Epoch 6/25\n",
      "845/845 [==============================] - 7s 8ms/step - loss: 726.6984 - val_loss: 521.9256\n",
      "Epoch 7/25\n",
      "845/845 [==============================] - 7s 9ms/step - loss: 585.8908 - val_loss: 467.3757\n",
      "Epoch 8/25\n",
      "845/845 [==============================] - 7s 9ms/step - loss: 505.2263 - val_loss: 380.5733\n",
      "Epoch 9/25\n",
      "845/845 [==============================] - 7s 9ms/step - loss: 465.7408 - val_loss: 338.9881\n",
      "Epoch 10/25\n",
      "845/845 [==============================] - 7s 9ms/step - loss: 449.4054 - val_loss: 351.7398\n",
      "Epoch 11/25\n",
      "845/845 [==============================] - 7s 8ms/step - loss: 442.9303 - val_loss: 318.1195\n",
      "Epoch 12/25\n",
      "845/845 [==============================] - 7s 8ms/step - loss: 439.2939 - val_loss: 319.9955\n",
      "Epoch 13/25\n",
      "845/845 [==============================] - 7s 8ms/step - loss: 430.0753 - val_loss: 344.1531\n",
      "Epoch 14/25\n",
      "845/845 [==============================] - 8s 9ms/step - loss: 425.0773 - val_loss: 325.9092\n",
      "Epoch 15/25\n",
      "845/845 [==============================] - 8s 10ms/step - loss: 423.7993 - val_loss: 356.5518\n",
      "Epoch 16/25\n",
      "845/845 [==============================] - 8s 10ms/step - loss: 421.0883 - val_loss: 328.0208\n",
      "Epoch 17/25\n",
      "845/845 [==============================] - 8s 9ms/step - loss: 415.3356 - val_loss: 315.0096\n",
      "Epoch 18/25\n",
      "845/845 [==============================] - 8s 9ms/step - loss: 410.3253 - val_loss: 342.4643\n",
      "Epoch 19/25\n",
      "845/845 [==============================] - 9s 10ms/step - loss: 412.6205 - val_loss: 353.1312\n",
      "Epoch 20/25\n",
      "845/845 [==============================] - 9s 10ms/step - loss: 399.2877 - val_loss: 343.9366\n",
      "Epoch 21/25\n",
      "845/845 [==============================] - 9s 10ms/step - loss: 400.9445 - val_loss: 352.4531\n",
      "Epoch 22/25\n",
      "845/845 [==============================] - 9s 10ms/step - loss: 395.5954 - val_loss: 361.3570\n",
      "Epoch 23/25\n",
      "845/845 [==============================] - 9s 10ms/step - loss: 388.3663 - val_loss: 388.3519\n",
      "Epoch 24/25\n",
      "845/845 [==============================] - 7s 8ms/step - loss: 383.5654 - val_loss: 367.3165\n",
      "Epoch 25/25\n",
      "845/845 [==============================] - 5s 6ms/step - loss: 382.6681 - val_loss: 363.5669\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "423/423 [==============================] - 19s 41ms/step - loss: 4852.6582 - val_loss: 2325.0010\n",
      "Epoch 2/25\n",
      "423/423 [==============================] - 21s 51ms/step - loss: 2243.3025 - val_loss: 1985.5199\n",
      "Epoch 3/25\n",
      "423/423 [==============================] - 20s 48ms/step - loss: 1917.1770 - val_loss: 2076.6660\n",
      "Epoch 4/25\n",
      "423/423 [==============================] - 22s 51ms/step - loss: 1407.4139 - val_loss: 973.2726\n",
      "Epoch 5/25\n",
      "423/423 [==============================] - 21s 50ms/step - loss: 608.7537 - val_loss: 654.6994\n",
      "Epoch 6/25\n",
      "423/423 [==============================] - 22s 53ms/step - loss: 515.6682 - val_loss: 660.2905\n",
      "Epoch 7/25\n",
      "423/423 [==============================] - 22s 52ms/step - loss: 447.9243 - val_loss: 501.1836\n",
      "Epoch 8/25\n",
      "423/423 [==============================] - 23s 53ms/step - loss: 381.1964 - val_loss: 462.0052\n",
      "Epoch 9/25\n",
      "423/423 [==============================] - 22s 53ms/step - loss: 364.3679 - val_loss: 409.6735\n",
      "Epoch 10/25\n",
      "423/423 [==============================] - 25s 58ms/step - loss: 348.2179 - val_loss: 382.4232\n",
      "Epoch 11/25\n",
      "423/423 [==============================] - 19s 46ms/step - loss: 350.7975 - val_loss: 323.2746\n",
      "Epoch 12/25\n",
      "423/423 [==============================] - 12s 28ms/step - loss: 343.3344 - val_loss: 319.8450\n",
      "Epoch 13/25\n",
      "423/423 [==============================] - 12s 29ms/step - loss: 332.2654 - val_loss: 370.1078\n",
      "Epoch 14/25\n",
      "423/423 [==============================] - 13s 32ms/step - loss: 323.8965 - val_loss: 316.4492\n",
      "Epoch 15/25\n",
      "423/423 [==============================] - 13s 30ms/step - loss: 325.0080 - val_loss: 310.9161\n",
      "Epoch 16/25\n",
      "423/423 [==============================] - 12s 29ms/step - loss: 319.6058 - val_loss: 344.1805\n",
      "Epoch 17/25\n",
      "423/423 [==============================] - 12s 29ms/step - loss: 309.9714 - val_loss: 381.6633\n",
      "Epoch 18/25\n",
      "423/423 [==============================] - 15s 35ms/step - loss: 298.9720 - val_loss: 778.7141\n",
      "Epoch 19/25\n",
      "423/423 [==============================] - 13s 32ms/step - loss: 295.6660 - val_loss: 840.0143\n",
      "Epoch 20/25\n",
      "423/423 [==============================] - 14s 32ms/step - loss: 294.5695 - val_loss: 1050.6113\n",
      "Epoch 21/25\n",
      "423/423 [==============================] - 14s 32ms/step - loss: 292.9274 - val_loss: 1004.6933\n",
      "Epoch 22/25\n",
      "423/423 [==============================] - 14s 32ms/step - loss: 284.4358 - val_loss: 1059.4604\n",
      "Epoch 23/25\n",
      "423/423 [==============================] - 14s 33ms/step - loss: 284.1040 - val_loss: 419.9949\n",
      "Epoch 24/25\n",
      "423/423 [==============================] - 14s 33ms/step - loss: 279.6646 - val_loss: 465.5284\n",
      "Epoch 25/25\n",
      "423/423 [==============================] - 15s 34ms/step - loss: 273.5707 - val_loss: 489.5870\n",
      "8/8 [==============================] - 1s 4ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "212/212 [==============================] - 6s 25ms/step - loss: 7604.1011 - val_loss: 5018.5586\n",
      "Epoch 2/25\n",
      "212/212 [==============================] - 5s 24ms/step - loss: 5541.9263 - val_loss: 3772.1465\n",
      "Epoch 3/25\n",
      "212/212 [==============================] - 5s 24ms/step - loss: 4249.0493 - val_loss: 2956.9763\n",
      "Epoch 4/25\n",
      "212/212 [==============================] - 5s 23ms/step - loss: 3359.8623 - val_loss: 2447.8772\n",
      "Epoch 5/25\n",
      "212/212 [==============================] - 5s 23ms/step - loss: 2769.4099 - val_loss: 2157.9890\n",
      "Epoch 6/25\n",
      "212/212 [==============================] - 5s 23ms/step - loss: 2388.7910 - val_loss: 2018.7958\n",
      "Epoch 7/25\n",
      "212/212 [==============================] - 5s 24ms/step - loss: 2164.1511 - val_loss: 1973.3936\n",
      "Epoch 8/25\n",
      "212/212 [==============================] - 5s 24ms/step - loss: 2040.9780 - val_loss: 1978.9034\n",
      "Epoch 9/25\n",
      "212/212 [==============================] - 5s 23ms/step - loss: 1972.6873 - val_loss: 2002.6318\n",
      "Epoch 10/25\n",
      "212/212 [==============================] - 5s 23ms/step - loss: 1767.4330 - val_loss: 1579.5403\n",
      "Epoch 11/25\n",
      "212/212 [==============================] - 5s 24ms/step - loss: 1095.6660 - val_loss: 774.5812\n",
      "Epoch 12/25\n",
      "212/212 [==============================] - 5s 24ms/step - loss: 785.7883 - val_loss: 584.8939\n",
      "Epoch 13/25\n",
      "212/212 [==============================] - 5s 24ms/step - loss: 643.8179 - val_loss: 486.8736\n",
      "Epoch 14/25\n",
      "212/212 [==============================] - 5s 24ms/step - loss: 544.6967 - val_loss: 456.7924\n",
      "Epoch 15/25\n",
      "212/212 [==============================] - 5s 24ms/step - loss: 478.7058 - val_loss: 376.9470\n",
      "Epoch 16/25\n",
      "212/212 [==============================] - 5s 24ms/step - loss: 442.2531 - val_loss: 390.9221\n",
      "Epoch 17/25\n",
      "212/212 [==============================] - 5s 24ms/step - loss: 430.2033 - val_loss: 354.6695\n",
      "Epoch 18/25\n",
      "212/212 [==============================] - 5s 24ms/step - loss: 409.2741 - val_loss: 417.0486\n",
      "Epoch 19/25\n",
      "212/212 [==============================] - 5s 23ms/step - loss: 399.2633 - val_loss: 360.3226\n",
      "Epoch 20/25\n",
      "212/212 [==============================] - 5s 23ms/step - loss: 388.9613 - val_loss: 370.4032\n",
      "Epoch 21/25\n",
      "212/212 [==============================] - 5s 24ms/step - loss: 386.4981 - val_loss: 339.5042\n",
      "Epoch 22/25\n",
      "212/212 [==============================] - 5s 25ms/step - loss: 380.1123 - val_loss: 321.4383\n",
      "Epoch 23/25\n",
      "212/212 [==============================] - 5s 22ms/step - loss: 373.1688 - val_loss: 325.5331\n",
      "Epoch 24/25\n",
      "212/212 [==============================] - 5s 23ms/step - loss: 367.7273 - val_loss: 394.6873\n",
      "Epoch 25/25\n",
      "212/212 [==============================] - 5s 23ms/step - loss: 365.4770 - val_loss: 309.5460\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "iteration  10\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "423/423 [==============================] - 4s 7ms/step - loss: 7927.6338 - val_loss: 5340.3613\n",
      "Epoch 2/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 5896.7124 - val_loss: 4030.4739\n",
      "Epoch 3/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 4526.2007 - val_loss: 3132.1511\n",
      "Epoch 4/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 3546.6719 - val_loss: 2546.9270\n",
      "Epoch 5/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 2862.7034 - val_loss: 2199.9722\n",
      "Epoch 6/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 2423.2788 - val_loss: 2028.0239\n",
      "Epoch 7/25\n",
      "423/423 [==============================] - 3s 6ms/step - loss: 2151.3811 - val_loss: 1973.0194\n",
      "Epoch 8/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 2009.2950 - val_loss: 1984.0825\n",
      "Epoch 9/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 1941.4027 - val_loss: 2021.1836\n",
      "Epoch 10/25\n",
      "423/423 [==============================] - 3s 6ms/step - loss: 1886.1254 - val_loss: 1764.9666\n",
      "Epoch 11/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 1236.8429 - val_loss: 855.8870\n",
      "Epoch 12/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 735.8148 - val_loss: 684.9772\n",
      "Epoch 13/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 588.7210 - val_loss: 582.0505\n",
      "Epoch 14/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 492.1524 - val_loss: 488.5477\n",
      "Epoch 15/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 430.0890 - val_loss: 391.7469\n",
      "Epoch 16/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 397.9012 - val_loss: 369.8903\n",
      "Epoch 17/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 380.6169 - val_loss: 370.1533\n",
      "Epoch 18/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 363.2408 - val_loss: 349.6735\n",
      "Epoch 19/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 359.1522 - val_loss: 342.9942\n",
      "Epoch 20/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 351.9081 - val_loss: 385.8099\n",
      "Epoch 21/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 343.0542 - val_loss: 319.9412\n",
      "Epoch 22/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 340.1242 - val_loss: 343.8313\n",
      "Epoch 23/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 340.2362 - val_loss: 327.0300\n",
      "Epoch 24/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 335.5430 - val_loss: 330.2098\n",
      "Epoch 25/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 331.7892 - val_loss: 312.5251\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "1689/1689 [==============================] - 41s 22ms/step - loss: 1613.1357 - val_loss: 1029.8328\n",
      "Epoch 2/25\n",
      "1689/1689 [==============================] - 38s 23ms/step - loss: 446.2466 - val_loss: 439.2316\n",
      "Epoch 3/25\n",
      "1689/1689 [==============================] - 39s 23ms/step - loss: 377.1208 - val_loss: 356.9365\n",
      "Epoch 4/25\n",
      "1689/1689 [==============================] - 38s 22ms/step - loss: 353.4620 - val_loss: 624.7043\n",
      "Epoch 5/25\n",
      "1689/1689 [==============================] - 39s 23ms/step - loss: 334.7211 - val_loss: 542.6109\n",
      "Epoch 6/25\n",
      "1689/1689 [==============================] - 39s 23ms/step - loss: 297.5382 - val_loss: 551.3669\n",
      "Epoch 7/25\n",
      "1689/1689 [==============================] - 39s 23ms/step - loss: 283.8352 - val_loss: 485.4811\n",
      "Epoch 8/25\n",
      "1689/1689 [==============================] - 39s 23ms/step - loss: 275.8595 - val_loss: 458.3276\n",
      "Epoch 9/25\n",
      "1689/1689 [==============================] - 39s 23ms/step - loss: 270.8148 - val_loss: 428.8268\n",
      "Epoch 10/25\n",
      "1689/1689 [==============================] - 39s 23ms/step - loss: 264.7595 - val_loss: 405.1854\n",
      "Epoch 11/25\n",
      "1689/1689 [==============================] - 39s 23ms/step - loss: 260.0710 - val_loss: 363.0778\n",
      "Epoch 12/25\n",
      "1689/1689 [==============================] - 39s 23ms/step - loss: 252.7158 - val_loss: 372.5781\n",
      "Epoch 13/25\n",
      "1689/1689 [==============================] - 39s 23ms/step - loss: 249.4373 - val_loss: 271.9093\n",
      "Epoch 14/25\n",
      "1689/1689 [==============================] - 39s 23ms/step - loss: 248.7642 - val_loss: 484.2430\n",
      "Epoch 15/25\n",
      "1689/1689 [==============================] - 40s 24ms/step - loss: 245.4062 - val_loss: 442.2818\n",
      "Epoch 16/25\n",
      "1689/1689 [==============================] - 39s 23ms/step - loss: 243.7537 - val_loss: 326.4054\n",
      "Epoch 17/25\n",
      "1689/1689 [==============================] - 37s 22ms/step - loss: 241.4172 - val_loss: 462.3680\n",
      "Epoch 18/25\n",
      "1689/1689 [==============================] - 37s 22ms/step - loss: 239.2117 - val_loss: 432.0933\n",
      "Epoch 19/25\n",
      "1689/1689 [==============================] - 37s 22ms/step - loss: 236.6635 - val_loss: 439.8740\n",
      "Epoch 20/25\n",
      "1689/1689 [==============================] - 38s 22ms/step - loss: 237.2675 - val_loss: 408.6684\n",
      "Epoch 21/25\n",
      "1689/1689 [==============================] - 38s 22ms/step - loss: 233.4740 - val_loss: 357.4469\n",
      "Epoch 22/25\n",
      "1689/1689 [==============================] - 38s 22ms/step - loss: 231.2288 - val_loss: 426.1590\n",
      "Epoch 23/25\n",
      "1689/1689 [==============================] - 38s 22ms/step - loss: 229.7206 - val_loss: 430.9437\n",
      "Epoch 24/25\n",
      "1689/1689 [==============================] - 38s 22ms/step - loss: 229.7082 - val_loss: 411.5126\n",
      "Epoch 25/25\n",
      "1689/1689 [==============================] - 38s 22ms/step - loss: 230.5623 - val_loss: 403.0104\n",
      "8/8 [==============================] - 1s 9ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "845/845 [==============================] - 39s 45ms/step - loss: 2418.3489 - val_loss: 792.5450\n",
      "Epoch 2/25\n",
      "845/845 [==============================] - 37s 44ms/step - loss: 604.8569 - val_loss: 692.2294\n",
      "Epoch 3/25\n",
      "845/845 [==============================] - 35s 42ms/step - loss: 426.5342 - val_loss: 544.3362\n",
      "Epoch 4/25\n",
      "845/845 [==============================] - 35s 42ms/step - loss: 372.1393 - val_loss: 529.8655\n",
      "Epoch 5/25\n",
      "845/845 [==============================] - 36s 42ms/step - loss: 358.8327 - val_loss: 473.7307\n",
      "Epoch 6/25\n",
      "845/845 [==============================] - 35s 41ms/step - loss: 342.3634 - val_loss: 428.9157\n",
      "Epoch 7/25\n",
      "845/845 [==============================] - 36s 42ms/step - loss: 323.7361 - val_loss: 341.4131\n",
      "Epoch 8/25\n",
      "845/845 [==============================] - 37s 43ms/step - loss: 300.2926 - val_loss: 322.6751\n",
      "Epoch 9/25\n",
      "845/845 [==============================] - 35s 42ms/step - loss: 290.8998 - val_loss: 307.9839\n",
      "Epoch 10/25\n",
      "845/845 [==============================] - 36s 42ms/step - loss: 282.8455 - val_loss: 293.1860\n",
      "Epoch 11/25\n",
      "845/845 [==============================] - 35s 41ms/step - loss: 280.3931 - val_loss: 265.6158\n",
      "Epoch 12/25\n",
      "845/845 [==============================] - 36s 43ms/step - loss: 279.9293 - val_loss: 257.4996\n",
      "Epoch 13/25\n",
      "845/845 [==============================] - 35s 42ms/step - loss: 264.1407 - val_loss: 281.4560\n",
      "Epoch 14/25\n",
      "845/845 [==============================] - 35s 42ms/step - loss: 267.3091 - val_loss: 265.6542\n",
      "Epoch 15/25\n",
      "845/845 [==============================] - 37s 43ms/step - loss: 261.7262 - val_loss: 260.5481\n",
      "Epoch 16/25\n",
      "845/845 [==============================] - 36s 42ms/step - loss: 266.0729 - val_loss: 254.4919\n",
      "Epoch 17/25\n",
      "845/845 [==============================] - 37s 44ms/step - loss: 263.9938 - val_loss: 237.1979\n",
      "Epoch 18/25\n",
      "845/845 [==============================] - 37s 43ms/step - loss: 256.2999 - val_loss: 258.5682\n",
      "Epoch 19/25\n",
      "845/845 [==============================] - 36s 42ms/step - loss: 255.3427 - val_loss: 265.9840\n",
      "Epoch 20/25\n",
      "845/845 [==============================] - 36s 42ms/step - loss: 252.4959 - val_loss: 259.5665\n",
      "Epoch 21/25\n",
      "845/845 [==============================] - 37s 43ms/step - loss: 244.2645 - val_loss: 278.8826\n",
      "Epoch 22/25\n",
      "845/845 [==============================] - 36s 42ms/step - loss: 251.5894 - val_loss: 292.1636\n",
      "Epoch 23/25\n",
      "845/845 [==============================] - 37s 44ms/step - loss: 254.4200 - val_loss: 321.3351\n",
      "Epoch 24/25\n",
      "845/845 [==============================] - 34s 40ms/step - loss: 249.4133 - val_loss: 242.3815\n",
      "Epoch 25/25\n",
      "845/845 [==============================] - 33s 39ms/step - loss: 243.5596 - val_loss: 224.8805\n",
      "8/8 [==============================] - 1s 11ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "1689/1689 [==============================] - 12s 6ms/step - loss: 3822.2466 - val_loss: 1645.0256\n",
      "Epoch 2/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 841.4704 - val_loss: 502.8954\n",
      "Epoch 3/25\n",
      "1689/1689 [==============================] - 11s 7ms/step - loss: 476.1702 - val_loss: 461.8390\n",
      "Epoch 4/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 382.6577 - val_loss: 450.5804\n",
      "Epoch 5/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 352.6707 - val_loss: 392.6559\n",
      "Epoch 6/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 339.3824 - val_loss: 411.0936\n",
      "Epoch 7/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 331.7978 - val_loss: 420.1592\n",
      "Epoch 8/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 322.3656 - val_loss: 388.1237\n",
      "Epoch 9/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 312.0656 - val_loss: 394.8815\n",
      "Epoch 10/25\n",
      "1689/1689 [==============================] - 13s 7ms/step - loss: 301.2393 - val_loss: 346.6395\n",
      "Epoch 11/25\n",
      "1689/1689 [==============================] - 11s 7ms/step - loss: 293.8141 - val_loss: 363.8190\n",
      "Epoch 12/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 284.5803 - val_loss: 319.1693\n",
      "Epoch 13/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 279.0187 - val_loss: 288.7762\n",
      "Epoch 14/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 274.5652 - val_loss: 272.9174\n",
      "Epoch 15/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 272.4001 - val_loss: 254.7416\n",
      "Epoch 16/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 269.6940 - val_loss: 257.0501\n",
      "Epoch 17/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 268.6032 - val_loss: 263.2834\n",
      "Epoch 18/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 267.3806 - val_loss: 353.0016\n",
      "Epoch 19/25\n",
      "1689/1689 [==============================] - 11s 7ms/step - loss: 266.3018 - val_loss: 288.5078\n",
      "Epoch 20/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 261.7875 - val_loss: 335.4831\n",
      "Epoch 21/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 262.5345 - val_loss: 291.9703\n",
      "Epoch 22/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 258.5843 - val_loss: 296.4108\n",
      "Epoch 23/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 258.2375 - val_loss: 310.8486\n",
      "Epoch 24/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 258.2414 - val_loss: 263.5421\n",
      "Epoch 25/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 257.5185 - val_loss: 264.8279\n",
      "8/8 [==============================] - 1s 3ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "1689/1689 [==============================] - 28s 16ms/step - loss: 2559.8442 - val_loss: 929.2248\n",
      "Epoch 2/25\n",
      "1689/1689 [==============================] - 27s 16ms/step - loss: 543.9605 - val_loss: 470.1230\n",
      "Epoch 3/25\n",
      "1689/1689 [==============================] - 25s 15ms/step - loss: 412.0917 - val_loss: 397.6722\n",
      "Epoch 4/25\n",
      "1689/1689 [==============================] - 26s 15ms/step - loss: 358.9899 - val_loss: 392.7247\n",
      "Epoch 5/25\n",
      "1689/1689 [==============================] - 26s 15ms/step - loss: 340.6745 - val_loss: 357.3470\n",
      "Epoch 6/25\n",
      "1689/1689 [==============================] - 25s 15ms/step - loss: 328.0331 - val_loss: 375.7203\n",
      "Epoch 7/25\n",
      "1689/1689 [==============================] - 27s 16ms/step - loss: 314.0483 - val_loss: 337.2295\n",
      "Epoch 8/25\n",
      "1689/1689 [==============================] - 31s 18ms/step - loss: 305.9530 - val_loss: 390.0928\n",
      "Epoch 9/25\n",
      "1689/1689 [==============================] - 30s 18ms/step - loss: 294.0935 - val_loss: 305.9409\n",
      "Epoch 10/25\n",
      "1689/1689 [==============================] - 32s 19ms/step - loss: 284.4601 - val_loss: 311.0443\n",
      "Epoch 11/25\n",
      "1689/1689 [==============================] - 31s 19ms/step - loss: 276.2538 - val_loss: 248.0948\n",
      "Epoch 12/25\n",
      "1689/1689 [==============================] - 39s 23ms/step - loss: 263.0813 - val_loss: 242.6375\n",
      "Epoch 13/25\n",
      "1689/1689 [==============================] - 37s 22ms/step - loss: 257.7584 - val_loss: 261.5280\n",
      "Epoch 14/25\n",
      "1689/1689 [==============================] - 31s 18ms/step - loss: 255.0206 - val_loss: 247.6693\n",
      "Epoch 15/25\n",
      "1689/1689 [==============================] - 30s 18ms/step - loss: 251.7072 - val_loss: 237.2027\n",
      "Epoch 16/25\n",
      "1689/1689 [==============================] - 30s 18ms/step - loss: 247.6257 - val_loss: 245.8886\n",
      "Epoch 17/25\n",
      "1689/1689 [==============================] - 30s 18ms/step - loss: 246.5767 - val_loss: 261.5340\n",
      "Epoch 18/25\n",
      "1689/1689 [==============================] - 32s 19ms/step - loss: 244.5973 - val_loss: 253.7070\n",
      "Epoch 19/25\n",
      "1689/1689 [==============================] - 31s 18ms/step - loss: 241.5595 - val_loss: 229.6026\n",
      "Epoch 20/25\n",
      "1689/1689 [==============================] - 30s 18ms/step - loss: 241.2713 - val_loss: 233.6637\n",
      "Epoch 21/25\n",
      "1689/1689 [==============================] - 32s 19ms/step - loss: 237.8652 - val_loss: 268.3822\n",
      "Epoch 22/25\n",
      "1689/1689 [==============================] - 30s 18ms/step - loss: 237.7035 - val_loss: 266.4629\n",
      "Epoch 23/25\n",
      "1689/1689 [==============================] - 29s 17ms/step - loss: 237.1980 - val_loss: 226.2220\n",
      "Epoch 24/25\n",
      "1689/1689 [==============================] - 30s 18ms/step - loss: 234.5552 - val_loss: 213.1712\n",
      "Epoch 25/25\n",
      "1689/1689 [==============================] - 30s 18ms/step - loss: 235.3917 - val_loss: 240.0649\n",
      "8/8 [==============================] - 1s 4ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "845/845 [==============================] - 45s 52ms/step - loss: 2492.0432 - val_loss: 938.9418\n",
      "Epoch 2/25\n",
      "845/845 [==============================] - 45s 54ms/step - loss: 616.9320 - val_loss: 1538.3979\n",
      "Epoch 3/25\n",
      "845/845 [==============================] - 44s 52ms/step - loss: 420.5851 - val_loss: 1563.2551\n",
      "Epoch 4/25\n",
      "845/845 [==============================] - 45s 53ms/step - loss: 364.8613 - val_loss: 1679.3461\n",
      "Epoch 5/25\n",
      "845/845 [==============================] - 45s 53ms/step - loss: 348.1513 - val_loss: 1675.7805\n",
      "Epoch 6/25\n",
      "845/845 [==============================] - 44s 52ms/step - loss: 334.2510 - val_loss: 1672.4481\n",
      "Epoch 7/25\n",
      "845/845 [==============================] - 45s 53ms/step - loss: 322.7722 - val_loss: 1691.1284\n",
      "Epoch 8/25\n",
      "845/845 [==============================] - 49s 58ms/step - loss: 299.8827 - val_loss: 1705.7994\n",
      "Epoch 9/25\n",
      "845/845 [==============================] - 40s 48ms/step - loss: 288.8155 - val_loss: 1605.7200\n",
      "Epoch 10/25\n",
      "845/845 [==============================] - 46s 54ms/step - loss: 275.3997 - val_loss: 1581.5570\n",
      "Epoch 11/25\n",
      "845/845 [==============================] - 49s 57ms/step - loss: 270.0920 - val_loss: 1512.3961\n",
      "Epoch 12/25\n",
      "845/845 [==============================] - 49s 58ms/step - loss: 265.4037 - val_loss: 1523.6600\n",
      "Epoch 13/25\n",
      "845/845 [==============================] - 53s 63ms/step - loss: 254.2878 - val_loss: 1494.8795\n",
      "Epoch 14/25\n",
      "845/845 [==============================] - 44s 52ms/step - loss: 251.1947 - val_loss: 1539.7960\n",
      "Epoch 15/25\n",
      "845/845 [==============================] - 33s 39ms/step - loss: 250.9453 - val_loss: 1560.9453\n",
      "Epoch 16/25\n",
      "845/845 [==============================] - 35s 42ms/step - loss: 244.7359 - val_loss: 783.1128\n",
      "Epoch 17/25\n",
      "845/845 [==============================] - 31s 37ms/step - loss: 244.4739 - val_loss: 400.9409\n",
      "Epoch 18/25\n",
      "845/845 [==============================] - 31s 37ms/step - loss: 241.2668 - val_loss: 581.4243\n",
      "Epoch 19/25\n",
      "845/845 [==============================] - 30s 36ms/step - loss: 239.7586 - val_loss: 441.3019\n",
      "Epoch 20/25\n",
      "845/845 [==============================] - 32s 37ms/step - loss: 233.8879 - val_loss: 306.2093\n",
      "Epoch 21/25\n",
      "845/845 [==============================] - 32s 38ms/step - loss: 231.8431 - val_loss: 285.0652\n",
      "Epoch 22/25\n",
      "845/845 [==============================] - 32s 38ms/step - loss: 237.4129 - val_loss: 231.4903\n",
      "Epoch 23/25\n",
      "845/845 [==============================] - 33s 39ms/step - loss: 227.6870 - val_loss: 310.7120\n",
      "Epoch 24/25\n",
      "845/845 [==============================] - 33s 39ms/step - loss: 227.3585 - val_loss: 281.5425\n",
      "Epoch 25/25\n",
      "845/845 [==============================] - 32s 38ms/step - loss: 226.2079 - val_loss: 248.0562\n",
      "8/8 [==============================] - 1s 8ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "845/845 [==============================] - 32s 37ms/step - loss: 2599.6760 - val_loss: 2110.9551\n",
      "Epoch 2/25\n",
      "845/845 [==============================] - 30s 35ms/step - loss: 1885.4731 - val_loss: 2292.5603\n",
      "Epoch 3/25\n",
      "845/845 [==============================] - 30s 35ms/step - loss: 1562.3788 - val_loss: 1351.6135\n",
      "Epoch 4/25\n",
      "845/845 [==============================] - 30s 35ms/step - loss: 568.3908 - val_loss: 1859.2163\n",
      "Epoch 5/25\n",
      "845/845 [==============================] - 30s 36ms/step - loss: 434.7784 - val_loss: 2118.0146\n",
      "Epoch 6/25\n",
      "845/845 [==============================] - 31s 37ms/step - loss: 382.1377 - val_loss: 2184.3850\n",
      "Epoch 7/25\n",
      "845/845 [==============================] - 32s 37ms/step - loss: 363.6281 - val_loss: 2321.6316\n",
      "Epoch 8/25\n",
      "845/845 [==============================] - 32s 38ms/step - loss: 339.3044 - val_loss: 2360.0090\n",
      "Epoch 9/25\n",
      "845/845 [==============================] - 33s 39ms/step - loss: 337.9706 - val_loss: 2285.3127\n",
      "Epoch 10/25\n",
      "845/845 [==============================] - 35s 42ms/step - loss: 326.3864 - val_loss: 2182.1045\n",
      "Epoch 11/25\n",
      "845/845 [==============================] - 31s 37ms/step - loss: 316.6363 - val_loss: 2041.2035\n",
      "Epoch 12/25\n",
      "845/845 [==============================] - 32s 38ms/step - loss: 316.6466 - val_loss: 2052.9167\n",
      "Epoch 13/25\n",
      "845/845 [==============================] - 31s 37ms/step - loss: 302.6631 - val_loss: 2008.2198\n",
      "Epoch 14/25\n",
      "845/845 [==============================] - 31s 37ms/step - loss: 296.8520 - val_loss: 1781.2561\n",
      "Epoch 15/25\n",
      "845/845 [==============================] - 31s 37ms/step - loss: 288.3563 - val_loss: 1773.1844\n",
      "Epoch 16/25\n",
      "845/845 [==============================] - 30s 35ms/step - loss: 281.6100 - val_loss: 1702.0085\n",
      "Epoch 17/25\n",
      "845/845 [==============================] - 30s 36ms/step - loss: 275.9210 - val_loss: 1672.5880\n",
      "Epoch 18/25\n",
      "845/845 [==============================] - 30s 36ms/step - loss: 270.9033 - val_loss: 1655.4155\n",
      "Epoch 19/25\n",
      "845/845 [==============================] - 30s 36ms/step - loss: 265.0165 - val_loss: 1649.1709\n",
      "Epoch 20/25\n",
      "845/845 [==============================] - 32s 37ms/step - loss: 258.0464 - val_loss: 1555.5717\n",
      "Epoch 21/25\n",
      "845/845 [==============================] - 32s 38ms/step - loss: 254.2283 - val_loss: 1598.3047\n",
      "Epoch 22/25\n",
      "845/845 [==============================] - 31s 37ms/step - loss: 253.8221 - val_loss: 1698.1471\n",
      "Epoch 23/25\n",
      "845/845 [==============================] - 31s 37ms/step - loss: 245.9221 - val_loss: 1794.2673\n",
      "Epoch 24/25\n",
      "845/845 [==============================] - 32s 37ms/step - loss: 244.7281 - val_loss: 1784.8351\n",
      "Epoch 25/25\n",
      "845/845 [==============================] - 32s 37ms/step - loss: 244.4205 - val_loss: 1689.2418\n",
      "8/8 [==============================] - 1s 7ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "845/845 [==============================] - 6s 5ms/step - loss: 6988.3691 - val_loss: 4095.6411\n",
      "Epoch 2/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 4120.8525 - val_loss: 2587.2021\n",
      "Epoch 3/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 2729.2805 - val_loss: 2043.2762\n",
      "Epoch 4/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 2162.8574 - val_loss: 1977.6666\n",
      "Epoch 5/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 1765.2275 - val_loss: 913.1210\n",
      "Epoch 6/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 845.9125 - val_loss: 670.5142\n",
      "Epoch 7/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 611.1245 - val_loss: 656.7409\n",
      "Epoch 8/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 525.6085 - val_loss: 613.7805\n",
      "Epoch 9/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 482.2666 - val_loss: 587.5283\n",
      "Epoch 10/25\n",
      "845/845 [==============================] - 5s 5ms/step - loss: 463.9003 - val_loss: 613.5139\n",
      "Epoch 11/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 455.2665 - val_loss: 613.7379\n",
      "Epoch 12/25\n",
      "845/845 [==============================] - 5s 5ms/step - loss: 449.3116 - val_loss: 623.3279\n",
      "Epoch 13/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 433.5064 - val_loss: 628.2799\n",
      "Epoch 14/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 428.5335 - val_loss: 633.8343\n",
      "Epoch 15/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 426.5731 - val_loss: 680.8800\n",
      "Epoch 16/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 422.6546 - val_loss: 655.7441\n",
      "Epoch 17/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 415.3910 - val_loss: 510.8696\n",
      "Epoch 18/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 408.8757 - val_loss: 517.6860\n",
      "Epoch 19/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 410.4632 - val_loss: 529.2190\n",
      "Epoch 20/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 398.8761 - val_loss: 553.3036\n",
      "Epoch 21/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 399.8448 - val_loss: 558.6898\n",
      "Epoch 22/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 397.3834 - val_loss: 545.0662\n",
      "Epoch 23/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 391.0958 - val_loss: 573.7404\n",
      "Epoch 24/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 383.2339 - val_loss: 583.4085\n",
      "Epoch 25/25\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 382.8391 - val_loss: 527.1139\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "845/845 [==============================] - 12s 12ms/step - loss: 5226.8164 - val_loss: 2464.3196\n",
      "Epoch 2/25\n",
      "845/845 [==============================] - 7s 8ms/step - loss: 2330.1611 - val_loss: 1741.9957\n",
      "Epoch 3/25\n",
      "845/845 [==============================] - 6s 8ms/step - loss: 1045.2620 - val_loss: 589.2512\n",
      "Epoch 4/25\n",
      "845/845 [==============================] - 6s 8ms/step - loss: 540.7296 - val_loss: 454.1358\n",
      "Epoch 5/25\n",
      "845/845 [==============================] - 6s 8ms/step - loss: 424.6485 - val_loss: 355.6291\n",
      "Epoch 6/25\n",
      "845/845 [==============================] - 7s 8ms/step - loss: 372.6527 - val_loss: 364.9444\n",
      "Epoch 7/25\n",
      "845/845 [==============================] - 7s 8ms/step - loss: 355.8911 - val_loss: 340.5771\n",
      "Epoch 8/25\n",
      "845/845 [==============================] - 6s 8ms/step - loss: 352.9894 - val_loss: 315.0251\n",
      "Epoch 9/25\n",
      "845/845 [==============================] - 6s 8ms/step - loss: 339.0641 - val_loss: 287.2343\n",
      "Epoch 10/25\n",
      "845/845 [==============================] - 6s 8ms/step - loss: 331.0182 - val_loss: 302.6754\n",
      "Epoch 11/25\n",
      "845/845 [==============================] - 7s 8ms/step - loss: 323.3552 - val_loss: 291.5579\n",
      "Epoch 12/25\n",
      "845/845 [==============================] - 6s 8ms/step - loss: 324.2117 - val_loss: 289.4847\n",
      "Epoch 13/25\n",
      "845/845 [==============================] - 6s 8ms/step - loss: 314.9977 - val_loss: 326.9433\n",
      "Epoch 14/25\n",
      "845/845 [==============================] - 6s 8ms/step - loss: 311.6021 - val_loss: 299.1110\n",
      "Epoch 15/25\n",
      "845/845 [==============================] - 6s 8ms/step - loss: 303.3620 - val_loss: 294.5745\n",
      "Epoch 16/25\n",
      "845/845 [==============================] - 6s 8ms/step - loss: 303.1837 - val_loss: 302.1982\n",
      "Epoch 17/25\n",
      "845/845 [==============================] - 6s 8ms/step - loss: 298.8467 - val_loss: 289.3651\n",
      "Epoch 18/25\n",
      "845/845 [==============================] - 7s 8ms/step - loss: 290.8060 - val_loss: 292.2997\n",
      "Epoch 19/25\n",
      "845/845 [==============================] - 6s 8ms/step - loss: 287.8966 - val_loss: 311.8489\n",
      "Epoch 20/25\n",
      "845/845 [==============================] - 7s 8ms/step - loss: 284.3384 - val_loss: 296.9160\n",
      "Epoch 21/25\n",
      "845/845 [==============================] - 7s 8ms/step - loss: 282.5620 - val_loss: 275.8236\n",
      "Epoch 22/25\n",
      "845/845 [==============================] - 6s 8ms/step - loss: 281.7320 - val_loss: 298.2433\n",
      "Epoch 23/25\n",
      "845/845 [==============================] - 6s 8ms/step - loss: 274.7469 - val_loss: 337.6165\n",
      "Epoch 24/25\n",
      "845/845 [==============================] - 6s 8ms/step - loss: 274.0067 - val_loss: 303.5383\n",
      "Epoch 25/25\n",
      "845/845 [==============================] - 6s 8ms/step - loss: 272.9701 - val_loss: 244.7757\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "1689/1689 [==============================] - 40s 23ms/step - loss: 1651.6102 - val_loss: 1219.9514\n",
      "Epoch 2/25\n",
      "1689/1689 [==============================] - 38s 22ms/step - loss: 470.0027 - val_loss: 370.0154\n",
      "Epoch 3/25\n",
      "1689/1689 [==============================] - 38s 22ms/step - loss: 391.9535 - val_loss: 424.6642\n",
      "Epoch 4/25\n",
      "1689/1689 [==============================] - 38s 22ms/step - loss: 363.6620 - val_loss: 439.5667\n",
      "Epoch 5/25\n",
      "1689/1689 [==============================] - 38s 22ms/step - loss: 339.6234 - val_loss: 288.2768\n",
      "Epoch 6/25\n",
      "1689/1689 [==============================] - 40s 24ms/step - loss: 309.2712 - val_loss: 295.0605\n",
      "Epoch 7/25\n",
      "1689/1689 [==============================] - 41s 24ms/step - loss: 296.1594 - val_loss: 250.5309\n",
      "Epoch 8/25\n",
      "1689/1689 [==============================] - 40s 24ms/step - loss: 288.1393 - val_loss: 382.8705\n",
      "Epoch 9/25\n",
      "1689/1689 [==============================] - 38s 23ms/step - loss: 283.5374 - val_loss: 355.0960\n",
      "Epoch 10/25\n",
      "1689/1689 [==============================] - 35s 21ms/step - loss: 274.3539 - val_loss: 295.4659\n",
      "Epoch 11/25\n",
      "1689/1689 [==============================] - 36s 22ms/step - loss: 271.3210 - val_loss: 300.7692\n",
      "Epoch 12/25\n",
      "1689/1689 [==============================] - 42s 25ms/step - loss: 262.4740 - val_loss: 295.2768\n",
      "Epoch 13/25\n",
      "1689/1689 [==============================] - 36s 21ms/step - loss: 264.2725 - val_loss: 262.3818\n",
      "Epoch 14/25\n",
      "1689/1689 [==============================] - 36s 21ms/step - loss: 258.7209 - val_loss: 292.1529\n",
      "Epoch 15/25\n",
      "1689/1689 [==============================] - 37s 22ms/step - loss: 256.0560 - val_loss: 285.2755\n",
      "Epoch 16/25\n",
      "1689/1689 [==============================] - 36s 22ms/step - loss: 253.8060 - val_loss: 274.4393\n",
      "Epoch 17/25\n",
      "1689/1689 [==============================] - 36s 22ms/step - loss: 249.2496 - val_loss: 298.7996\n",
      "Epoch 18/25\n",
      "1689/1689 [==============================] - 36s 22ms/step - loss: 250.2059 - val_loss: 257.9015\n",
      "Epoch 19/25\n",
      "1689/1689 [==============================] - 37s 22ms/step - loss: 249.3577 - val_loss: 318.0630\n",
      "Epoch 20/25\n",
      "1689/1689 [==============================] - 37s 22ms/step - loss: 243.4068 - val_loss: 277.3246\n",
      "Epoch 21/25\n",
      "1689/1689 [==============================] - 36s 21ms/step - loss: 243.2546 - val_loss: 267.3133\n",
      "Epoch 22/25\n",
      "1689/1689 [==============================] - 37s 22ms/step - loss: 240.9550 - val_loss: 277.2286\n",
      "Epoch 23/25\n",
      "1689/1689 [==============================] - 35s 21ms/step - loss: 239.1557 - val_loss: 321.6088\n",
      "Epoch 24/25\n",
      "1689/1689 [==============================] - 44s 26ms/step - loss: 239.8898 - val_loss: 257.4967\n",
      "Epoch 25/25\n",
      "1689/1689 [==============================] - 45s 27ms/step - loss: 241.8232 - val_loss: 276.5985\n",
      "8/8 [==============================] - 1s 9ms/step\n",
      "iteration  20\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "423/423 [==============================] - 5s 8ms/step - loss: 7951.9844 - val_loss: 5325.5908\n",
      "Epoch 2/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 5880.6445 - val_loss: 4016.2083\n",
      "Epoch 3/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 4520.0347 - val_loss: 3122.4607\n",
      "Epoch 4/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 3548.0100 - val_loss: 2541.2705\n",
      "Epoch 5/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 2872.8528 - val_loss: 2197.7112\n",
      "Epoch 6/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 2439.4866 - val_loss: 2027.8154\n",
      "Epoch 7/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 2114.2610 - val_loss: 1645.1986\n",
      "Epoch 8/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 1510.5098 - val_loss: 1032.2356\n",
      "Epoch 9/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 1039.5787 - val_loss: 738.5504\n",
      "Epoch 10/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 831.3267 - val_loss: 604.5712\n",
      "Epoch 11/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 636.3911 - val_loss: 447.1561\n",
      "Epoch 12/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 533.9920 - val_loss: 396.0591\n",
      "Epoch 13/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 479.5081 - val_loss: 412.3155\n",
      "Epoch 14/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 436.3142 - val_loss: 339.5083\n",
      "Epoch 15/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 407.6876 - val_loss: 325.0704\n",
      "Epoch 16/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 394.1743 - val_loss: 327.8069\n",
      "Epoch 17/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 388.3945 - val_loss: 336.6970\n",
      "Epoch 18/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 380.1919 - val_loss: 321.3289\n",
      "Epoch 19/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 374.8889 - val_loss: 320.6428\n",
      "Epoch 20/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 371.6409 - val_loss: 375.4994\n",
      "Epoch 21/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 364.9236 - val_loss: 305.9327\n",
      "Epoch 22/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 364.8898 - val_loss: 320.9708\n",
      "Epoch 23/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 364.5353 - val_loss: 333.8662\n",
      "Epoch 24/25\n",
      "423/423 [==============================] - 3s 8ms/step - loss: 358.9515 - val_loss: 324.5739\n",
      "Epoch 25/25\n",
      "423/423 [==============================] - 3s 7ms/step - loss: 359.3269 - val_loss: 312.3825\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "1689/1689 [==============================] - 28s 16ms/step - loss: 2478.5549 - val_loss: 1202.8389\n",
      "Epoch 2/25\n",
      "1689/1689 [==============================] - 29s 17ms/step - loss: 629.5198 - val_loss: 908.9179\n",
      "Epoch 3/25\n",
      "1689/1689 [==============================] - 25s 15ms/step - loss: 458.1296 - val_loss: 572.0875\n",
      "Epoch 4/25\n",
      "1689/1689 [==============================] - 26s 15ms/step - loss: 372.3324 - val_loss: 410.0697\n",
      "Epoch 5/25\n",
      "1689/1689 [==============================] - 27s 16ms/step - loss: 334.2920 - val_loss: 342.7857\n",
      "Epoch 6/25\n",
      "1689/1689 [==============================] - 26s 15ms/step - loss: 317.4350 - val_loss: 342.8092\n",
      "Epoch 7/25\n",
      "1689/1689 [==============================] - 29s 17ms/step - loss: 305.2591 - val_loss: 340.6219\n",
      "Epoch 8/25\n",
      "1689/1689 [==============================] - 31s 19ms/step - loss: 296.2497 - val_loss: 304.7260\n",
      "Epoch 9/25\n",
      "1689/1689 [==============================] - 31s 18ms/step - loss: 289.4370 - val_loss: 298.2039\n",
      "Epoch 10/25\n",
      "1689/1689 [==============================] - 31s 18ms/step - loss: 280.1027 - val_loss: 288.7967\n",
      "Epoch 11/25\n",
      "1689/1689 [==============================] - 32s 19ms/step - loss: 267.2237 - val_loss: 260.8047\n",
      "Epoch 12/25\n",
      "1689/1689 [==============================] - 31s 19ms/step - loss: 254.2633 - val_loss: 251.4458\n",
      "Epoch 13/25\n",
      "1689/1689 [==============================] - 31s 18ms/step - loss: 245.2306 - val_loss: 261.8731\n",
      "Epoch 14/25\n",
      "1689/1689 [==============================] - 32s 19ms/step - loss: 241.8200 - val_loss: 258.8512\n",
      "Epoch 15/25\n",
      "1689/1689 [==============================] - 31s 18ms/step - loss: 240.2201 - val_loss: 258.2989\n",
      "Epoch 16/25\n",
      "1689/1689 [==============================] - 31s 18ms/step - loss: 235.7992 - val_loss: 246.6814\n",
      "Epoch 17/25\n",
      "1689/1689 [==============================] - 32s 19ms/step - loss: 233.2050 - val_loss: 241.9786\n",
      "Epoch 18/25\n",
      "1689/1689 [==============================] - 31s 18ms/step - loss: 231.9173 - val_loss: 245.8136\n",
      "Epoch 19/25\n",
      "1689/1689 [==============================] - 31s 18ms/step - loss: 229.5620 - val_loss: 238.1593\n",
      "Epoch 20/25\n",
      "1689/1689 [==============================] - 31s 18ms/step - loss: 228.5551 - val_loss: 247.6072\n",
      "Epoch 21/25\n",
      "1689/1689 [==============================] - 31s 19ms/step - loss: 225.3830 - val_loss: 250.9583\n",
      "Epoch 22/25\n",
      "1689/1689 [==============================] - 31s 18ms/step - loss: 224.7436 - val_loss: 228.3846\n",
      "Epoch 23/25\n",
      "1689/1689 [==============================] - 31s 18ms/step - loss: 222.9455 - val_loss: 256.4898\n",
      "Epoch 24/25\n",
      "1689/1689 [==============================] - 31s 19ms/step - loss: 222.5695 - val_loss: 212.9835\n",
      "Epoch 25/25\n",
      "1689/1689 [==============================] - 32s 19ms/step - loss: 221.8852 - val_loss: 232.5817\n",
      "8/8 [==============================] - 1s 7ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "212/212 [==============================] - 16s 70ms/step - loss: 6300.3687 - val_loss: 3548.6904\n",
      "Epoch 2/25\n",
      "212/212 [==============================] - 15s 73ms/step - loss: 3579.8127 - val_loss: 2348.6260\n",
      "Epoch 3/25\n",
      "212/212 [==============================] - 15s 72ms/step - loss: 2456.6587 - val_loss: 1999.0029\n",
      "Epoch 4/25\n",
      "212/212 [==============================] - 15s 73ms/step - loss: 2044.5653 - val_loss: 1983.6487\n",
      "Epoch 5/25\n",
      "212/212 [==============================] - 15s 71ms/step - loss: 1923.0375 - val_loss: 2038.5676\n",
      "Epoch 6/25\n",
      "212/212 [==============================] - 16s 75ms/step - loss: 1899.1193 - val_loss: 2065.0227\n",
      "Epoch 7/25\n",
      "212/212 [==============================] - 16s 73ms/step - loss: 1532.0659 - val_loss: 2064.7786\n",
      "Epoch 8/25\n",
      "212/212 [==============================] - 15s 70ms/step - loss: 728.2420 - val_loss: 1926.6545\n",
      "Epoch 9/25\n",
      "212/212 [==============================] - 14s 68ms/step - loss: 544.3749 - val_loss: 1878.4196\n",
      "Epoch 10/25\n",
      "212/212 [==============================] - 15s 69ms/step - loss: 467.9503 - val_loss: 1946.4923\n",
      "Epoch 11/25\n",
      "212/212 [==============================] - 15s 69ms/step - loss: 427.7952 - val_loss: 1879.1105\n",
      "Epoch 12/25\n",
      "212/212 [==============================] - 15s 73ms/step - loss: 395.7503 - val_loss: 1883.8722\n",
      "Epoch 13/25\n",
      "212/212 [==============================] - 16s 75ms/step - loss: 370.3102 - val_loss: 1952.4154\n",
      "Epoch 14/25\n",
      "212/212 [==============================] - 15s 71ms/step - loss: 356.4369 - val_loss: 2048.1306\n",
      "Epoch 15/25\n",
      "212/212 [==============================] - 16s 75ms/step - loss: 352.4098 - val_loss: 1973.3507\n",
      "Epoch 16/25\n",
      "212/212 [==============================] - 16s 77ms/step - loss: 336.4059 - val_loss: 2060.9111\n",
      "Epoch 17/25\n",
      "212/212 [==============================] - 18s 84ms/step - loss: 339.6212 - val_loss: 2035.9867\n",
      "Epoch 18/25\n",
      "212/212 [==============================] - 17s 83ms/step - loss: 329.8719 - val_loss: 2030.7225\n",
      "Epoch 19/25\n",
      "212/212 [==============================] - 18s 83ms/step - loss: 323.7639 - val_loss: 2036.3146\n",
      "Epoch 20/25\n",
      "212/212 [==============================] - 17s 81ms/step - loss: 329.8394 - val_loss: 2069.4792\n",
      "Epoch 21/25\n",
      "212/212 [==============================] - 16s 76ms/step - loss: 330.6884 - val_loss: 2040.8611\n",
      "Epoch 22/25\n",
      "212/212 [==============================] - 17s 79ms/step - loss: 315.9676 - val_loss: 2040.6404\n",
      "Epoch 23/25\n",
      "212/212 [==============================] - 17s 80ms/step - loss: 313.4238 - val_loss: 2050.9231\n",
      "Epoch 24/25\n",
      "212/212 [==============================] - 17s 80ms/step - loss: 314.6163 - val_loss: 2112.3484\n",
      "Epoch 25/25\n",
      "212/212 [==============================] - 18s 83ms/step - loss: 310.2239 - val_loss: 2081.6116\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "423/423 [==============================] - 23s 51ms/step - loss: 3339.4094 - val_loss: 2003.7274\n",
      "Epoch 2/25\n",
      "423/423 [==============================] - 35s 82ms/step - loss: 1309.5736 - val_loss: 1903.2745\n",
      "Epoch 3/25\n",
      "423/423 [==============================] - 37s 88ms/step - loss: 602.9659 - val_loss: 1012.4302\n",
      "Epoch 4/25\n",
      "423/423 [==============================] - 41s 97ms/step - loss: 525.6546 - val_loss: 794.9191\n",
      "Epoch 5/25\n",
      "423/423 [==============================] - 40s 94ms/step - loss: 460.2984 - val_loss: 468.6968\n",
      "Epoch 6/25\n",
      "423/423 [==============================] - 21s 50ms/step - loss: 373.3133 - val_loss: 370.0180\n",
      "Epoch 7/25\n",
      "423/423 [==============================] - 20s 48ms/step - loss: 354.7160 - val_loss: 327.7698\n",
      "Epoch 8/25\n",
      "423/423 [==============================] - 21s 49ms/step - loss: 344.5638 - val_loss: 430.7973\n",
      "Epoch 9/25\n",
      "423/423 [==============================] - 21s 49ms/step - loss: 344.1638 - val_loss: 445.0978\n",
      "Epoch 10/25\n",
      "423/423 [==============================] - 20s 48ms/step - loss: 331.6878 - val_loss: 334.9615\n",
      "Epoch 11/25\n",
      "423/423 [==============================] - 22s 51ms/step - loss: 324.7915 - val_loss: 317.1993\n",
      "Epoch 12/25\n",
      "423/423 [==============================] - 19s 45ms/step - loss: 322.0800 - val_loss: 344.2343\n",
      "Epoch 13/25\n",
      "423/423 [==============================] - 20s 48ms/step - loss: 322.4692 - val_loss: 933.9222\n",
      "Epoch 14/25\n",
      "423/423 [==============================] - 21s 49ms/step - loss: 308.0474 - val_loss: 701.5198\n",
      "Epoch 15/25\n",
      "423/423 [==============================] - 20s 46ms/step - loss: 300.3210 - val_loss: 440.8679\n",
      "Epoch 16/25\n",
      "423/423 [==============================] - 20s 48ms/step - loss: 294.3488 - val_loss: 589.8472\n",
      "Epoch 17/25\n",
      "423/423 [==============================] - 21s 49ms/step - loss: 282.5320 - val_loss: 492.7312\n",
      "Epoch 18/25\n",
      "423/423 [==============================] - 20s 47ms/step - loss: 272.6028 - val_loss: 430.2605\n",
      "Epoch 19/25\n",
      "423/423 [==============================] - 20s 48ms/step - loss: 270.9580 - val_loss: 485.2005\n",
      "Epoch 20/25\n",
      "423/423 [==============================] - 21s 49ms/step - loss: 265.0676 - val_loss: 279.8178\n",
      "Epoch 21/25\n",
      "423/423 [==============================] - 20s 48ms/step - loss: 260.8683 - val_loss: 390.3053\n",
      "Epoch 22/25\n",
      "423/423 [==============================] - 20s 47ms/step - loss: 257.4464 - val_loss: 857.8935\n",
      "Epoch 23/25\n",
      "423/423 [==============================] - 20s 48ms/step - loss: 254.3647 - val_loss: 786.7748\n",
      "Epoch 24/25\n",
      "423/423 [==============================] - 20s 47ms/step - loss: 256.5407 - val_loss: 742.7914\n",
      "Epoch 25/25\n",
      "423/423 [==============================] - 20s 48ms/step - loss: 249.7016 - val_loss: 746.3301\n",
      "8/8 [==============================] - 1s 7ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "1689/1689 [==============================] - 12s 6ms/step - loss: 3773.4209 - val_loss: 1977.1721\n",
      "Epoch 2/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 1304.4955 - val_loss: 549.3964\n",
      "Epoch 3/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 468.4823 - val_loss: 413.9812\n",
      "Epoch 4/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 380.9988 - val_loss: 389.1178\n",
      "Epoch 5/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 356.2112 - val_loss: 418.2707\n",
      "Epoch 6/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 343.2376 - val_loss: 444.8484\n",
      "Epoch 7/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 333.6285 - val_loss: 435.6984\n",
      "Epoch 8/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 321.5473 - val_loss: 321.6526\n",
      "Epoch 9/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 312.4125 - val_loss: 293.2722\n",
      "Epoch 10/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 302.9574 - val_loss: 296.5132\n",
      "Epoch 11/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 293.9190 - val_loss: 266.8312\n",
      "Epoch 12/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 286.4594 - val_loss: 262.1184\n",
      "Epoch 13/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 281.2411 - val_loss: 320.2895\n",
      "Epoch 14/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 276.7906 - val_loss: 308.9735\n",
      "Epoch 15/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 275.4609 - val_loss: 310.3483\n",
      "Epoch 16/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 273.4091 - val_loss: 286.0784\n",
      "Epoch 17/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 271.2721 - val_loss: 310.2765\n",
      "Epoch 18/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 269.1928 - val_loss: 282.4773\n",
      "Epoch 19/25\n",
      "1689/1689 [==============================] - 11s 6ms/step - loss: 266.7745 - val_loss: 267.8199\n",
      "Epoch 20/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 262.5682 - val_loss: 274.3350\n",
      "Epoch 21/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 262.6582 - val_loss: 262.1553\n",
      "Epoch 22/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 260.1859 - val_loss: 266.3189\n",
      "Epoch 23/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 259.1811 - val_loss: 272.7274\n",
      "Epoch 24/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 257.0263 - val_loss: 263.0170\n",
      "Epoch 25/25\n",
      "1689/1689 [==============================] - 10s 6ms/step - loss: 257.4177 - val_loss: 239.3536\n",
      "8/8 [==============================] - 1s 2ms/step\n",
      "(54028, 30, 14) (54028, 1) (248, 30, 14)\n",
      "Epoch 1/25\n",
      "1689/1689 [==============================] - 20s 11ms/step - loss: 2791.1936 - val_loss: 2058.5012\n",
      "Epoch 2/25\n",
      "1689/1689 [==============================] - 19s 11ms/step - loss: 704.4773 - val_loss: 438.8161\n",
      "Epoch 3/25\n",
      "1689/1689 [==============================] - 19s 11ms/step - loss: 389.1759 - val_loss: 348.6073\n",
      "Epoch 4/25\n",
      "1689/1689 [==============================] - 19s 11ms/step - loss: 342.3702 - val_loss: 337.8492\n",
      "Epoch 5/25\n",
      "1689/1689 [==============================] - 20s 12ms/step - loss: 324.7843 - val_loss: 311.4864\n",
      "Epoch 6/25\n",
      "1689/1689 [==============================] - 20s 12ms/step - loss: 314.1617 - val_loss: 326.3397\n",
      "Epoch 7/25\n",
      "1689/1689 [==============================] - 18s 11ms/step - loss: 298.8792 - val_loss: 300.3216\n",
      "Epoch 8/25\n",
      "1689/1689 [==============================] - 19s 11ms/step - loss: 288.2302 - val_loss: 269.4778\n",
      "Epoch 9/25\n",
      "1689/1689 [==============================] - 18s 11ms/step - loss: 274.9269 - val_loss: 234.9132\n",
      "Epoch 10/25\n",
      "1689/1689 [==============================] - 19s 11ms/step - loss: 263.7462 - val_loss: 247.1808\n",
      "Epoch 11/25\n",
      "1689/1689 [==============================] - 19s 11ms/step - loss: 259.2708 - val_loss: 233.5077\n",
      "Epoch 12/25\n",
      "1689/1689 [==============================] - 19s 11ms/step - loss: 252.4475 - val_loss: 222.2201\n",
      "Epoch 13/25\n",
      "1689/1689 [==============================] - 19s 11ms/step - loss: 247.8789 - val_loss: 243.7153\n",
      "Epoch 14/25\n",
      "1689/1689 [==============================] - 19s 11ms/step - loss: 247.1070 - val_loss: 244.1180\n",
      "Epoch 15/25\n",
      "1689/1689 [==============================] - 19s 11ms/step - loss: 244.9373 - val_loss: 243.6429\n",
      "Epoch 16/25\n",
      "1689/1689 [==============================] - 19s 11ms/step - loss: 241.1107 - val_loss: 225.0376\n",
      "Epoch 17/25\n",
      "1689/1689 [==============================] - 20s 12ms/step - loss: 239.8162 - val_loss: 237.9587\n",
      "Epoch 18/25\n",
      "1689/1689 [==============================] - 20s 12ms/step - loss: 238.2177 - val_loss: 246.0933\n",
      "Epoch 19/25\n",
      "1689/1689 [==============================] - 22s 13ms/step - loss: 236.0390 - val_loss: 225.0515\n",
      "Epoch 20/25\n",
      "1689/1689 [==============================] - 20s 12ms/step - loss: 235.9650 - val_loss: 238.9662\n",
      "Epoch 21/25\n",
      "1689/1689 [==============================] - 18s 11ms/step - loss: 233.3543 - val_loss: 229.5977\n",
      "Epoch 22/25\n",
      "1689/1689 [==============================] - 20s 12ms/step - loss: 231.6171 - val_loss: 245.6728\n",
      "Epoch 23/25\n",
      "1689/1689 [==============================] - 19s 11ms/step - loss: 231.3616 - val_loss: 237.5143\n",
      "Epoch 24/25\n",
      "1689/1689 [==============================] - 19s 11ms/step - loss: 228.9028 - val_loss: 241.7466\n",
      "Epoch 25/25\n",
      "1689/1689 [==============================] - 19s 11ms/step - loss: 231.5176 - val_loss: 235.3224\n",
      "8/8 [==============================] - 1s 3ms/step\n",
      "CPU times: user 4h 14min 55s, sys: 1h 2min 35s, total: 5h 17min 31s\n",
      "Wall time: 3h 6min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = pd.DataFrame(columns=['RMSE', 'std_RMSE', \n",
    "                                'S_score','std_S_score',\n",
    "                                'MSE', 'std_MSE',\n",
    "                                'nodes', 'dropout',\n",
    "                                'activation', 'batch_size'])\n",
    "\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    if ITERATIONS < 10:\n",
    "        print('iteration ', i+1)\n",
    "    elif ((i+1) % 10 == 0):\n",
    "        print('iteration ', i+1)    \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    \n",
    "    # parameter's sample\n",
    "    weights_file = \"model_weight/weights_file_gru.h5\"\n",
    "    alpha = 0.3\n",
    "    sequence_length = 30\n",
    "    epochs = 25\n",
    "    nodes_per_layer = random.sample(nodes_list, 1)[0]\n",
    "    dropout = random.sample(dropouts, 1)[0]\n",
    "    activation = random.sample(activation_functions, 1)[0]\n",
    "    batch_size = random.sample(batch_size_list, 1)[0]\n",
    "    remaining_sensors = remaining_sensors\n",
    "    drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_gru_1layer(input_shape, nodes_per_layer[0], dropout,\n",
    "                             activation, weights_file)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "    \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.996016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1068.145958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>224.880478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.494028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1149.516955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>240.064926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.779557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1208.643000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>248.994431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15.471055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1210.752320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>239.353577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.749801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1215.751911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>248.056229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE  nodes  \\\n",
       "11  14.996016       0.0  1068.145958          0.0  224.880478      0.0  [256]   \n",
       "13  15.494028       0.0  1149.516955          0.0  240.064926      0.0  [128]   \n",
       "2   15.779557       0.0  1208.643000          0.0  248.994431      0.0   [64]   \n",
       "23  15.471055       0.0  1210.752320          0.0  239.353577      0.0   [64]   \n",
       "14  15.749801       0.0  1215.751911          0.0  248.056229      0.0  [256]   \n",
       "\n",
       "    dropout activation batch_size  \n",
       "11      0.3       tanh         64  \n",
       "13      0.2       tanh         32  \n",
       "2       0.2       tanh         32  \n",
       "23      0.2       tanh         32  \n",
       "14      0.1       tanh         64  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='S_score').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"fd004_result/optim_result_1layer.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
