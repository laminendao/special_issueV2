{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "okay\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lime.lime_tabular import RecurrentTabularExplainer\n",
    "from tqdm import tqdm\n",
    "import keras\n",
    "from sp_modif.model_function import *\n",
    "from sp_modif.methods import *\n",
    "from sp_modif.data_prep import *\n",
    "from sp_modif.evaluator import *\n",
    "from sp_modif.SHAP import *\n",
    "from sp_modif.L2X import *\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"okay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "def set_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "# Appeler la fonction pour fixer le seed\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate explanations\n",
    "# Function explanation for lime\n",
    "\n",
    "def get_lime_explanation(data, e, L2X=False) :\n",
    "    # e  = fn = model.predict\n",
    "    # Iniatialisation\n",
    "    df_expplanation = pd.DataFrame(columns=[str(i) for i in range(data.shape[1]*data.shape[2])])\n",
    "\n",
    "    # Get explanations\n",
    "    for row in range(data.shape[0]) : \n",
    "        explanation = lime_explainer.explain_instance(data[row],\n",
    "                                                      e,\n",
    "                                                      num_features=data.shape[1]*data.shape[2]) \n",
    "        # fn = model.predict, initialize lime_explainer = Reccurent()\n",
    "        lime_values = explanation.local_exp[1]\n",
    "        # Add explanation in df_explanation\n",
    "        lime_dict = {}\n",
    "        for tup in lime_values :\n",
    "            lime_dict[str(tup[0])] = tup[1]\n",
    "        df_expplanation.loc[len(df_expplanation)] = lime_dict\n",
    "    \n",
    "    return df_expplanation\n",
    "\n",
    "# # Function explanation for others\n",
    "def get_explainations(data, e, L2X = False) :\n",
    "    \n",
    "    # df diemnsion\n",
    "    if L2X==True :\n",
    "        X_to_def_col = data[0:1]\n",
    "        explanation_test = e.explain(X_to_def_col.reshape((X_to_def_col.shape[0], -1)))\n",
    "        num_columns = explanation_test.flatten().shape[0]\n",
    "        \n",
    "    else : \n",
    "        explanation_test = e.explain(data[0:1])\n",
    "        num_columns = explanation_test.flatten().shape[0]\n",
    "    \n",
    "    # Iniatialisation\n",
    "    df_expplanation = pd.DataFrame(columns=[str(i) for i in range(num_columns)])\n",
    "\n",
    "    # Get explanations\n",
    "    for row in range(data.shape[0]) :\n",
    "        if L2X==True:\n",
    "            X_row = data[row:row+1]\n",
    "            explanation = e.explain(X_row.reshape((X_row.shape[0], -1)))\n",
    "        else :\n",
    "            explanation = e.explain(data[row:row+1])\n",
    "        # Add explanation in df_explanation\n",
    "        explanation = explanation.flatten()\n",
    "        feature_dict = {}\n",
    "        for i in range(num_columns) :\n",
    "            feature_dict[str(i)] = explanation[i]\n",
    "        df_expplanation.loc[len(df_expplanation)] = feature_dict\n",
    "    \n",
    "    return df_expplanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61249, 27) (41214, 26) (248, 1)\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "# Data loading\n",
    "train, test, y_test = prepare_data('FD004.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "# train['RUL'].clip(upper=125, inplace=True)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50','P30','Nf','Nc','Ps30','phi',\n",
    "                'NRf','NRc','BPR','htBleed','W31','W32'] # selection based on main_notebook\n",
    "\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model004(input_shape,weights_file, nb_layers = None):\n",
    "    cb = keras.callbacks.EarlyStopping(monitor='loss', patience=4)\n",
    "    if nb_layers==1:\n",
    "        '''\n",
    "        node = 256, activation = tanh, dropout = 0.3, bs = 64\n",
    "        '''\n",
    "        nodes_per_layer = 256\n",
    "        activation_value= 'tanh'\n",
    "        dropout = 0.3\n",
    "        bs = 64\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(nodes_per_layer, activation=activation_value, input_shape=input_shape))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "    elif nb_layers == 2 :\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(256, activation='tanh', input_shape=input_shape, return_sequences=True))\n",
    "        model.add(LSTM(64, activation='tanh'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(256, activation = 'relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "    elif nb_layers==3:\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(256, activation='tanh', input_shape=input_shape, return_sequences=True))\n",
    "        model.add(LSTM(64, activation='tanh', return_sequences=True))\n",
    "        Dropout(0.3)\n",
    "        # Dense(256, activation = 'relu')\n",
    "        model.add(LSTM(32, activation='tanh'))\n",
    "        model.add(Dense(256, activation = 'relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(1))\n",
    "    \n",
    "    elif nb_layers==4:\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(256, activation='tanh', input_shape=input_shape, return_sequences=True))\n",
    "        model.add(LSTM(64, activation='tanh', return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(64, activation='tanh', return_sequences=True))\n",
    "        model.add(LSTM(32, activation='tanh'))\n",
    "        model.add(Dense(256, activation = 'relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "    model.save_weights(weights_file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {\n",
    "#     'model_1l' : model004,\n",
    "#     'model_2l' : create_model2C004(),\n",
    "#     'model_3l' : create_model3C004(),\n",
    "#     'model_4l' : create_model4C004(),\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52783, 35, 14) (52783, 1) (248, 35, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "# Data prepration\n",
    "sequence_length = 35\n",
    "alpha = 0.2\n",
    "upper = 125\n",
    "    \n",
    "learning_rate_ = 0.001\n",
    "dropout = 0.4\n",
    "activation = 'tanh'\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "train = rul_piecewise_fct(train, upper)\n",
    "\n",
    "X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "# create sequences train, test\n",
    "train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "test_rul = rul_piecewise_fct(y_test,upper)\n",
    "print(train_array.shape, label_array.shape, test_array.shape)\n",
    "\n",
    "\n",
    "\n",
    "for md in tqdm.tqdm(range(1,5)):\n",
    "    #Model creation\n",
    "    weights_file = \"fd004_result/\" + str(md) + 'lstm_hyper_parameter_weights.h5'\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model004(input_shape=input_shape, weights_file=weights_file, nb_layers=md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=644x306>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualkeras.layered_view(model, scale_xy=1, scale_z=1, max_z=100, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52783, 35, 14) (52783, 1) (248, 35, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "825/825 [==============================] - 40s 43ms/step - loss: 2299.0552 - val_loss: 2032.4381\n",
      "Epoch 2/20\n",
      "825/825 [==============================] - 38s 47ms/step - loss: 1772.2677 - val_loss: 2074.6436\n",
      "Epoch 3/20\n",
      "825/825 [==============================] - 38s 46ms/step - loss: 1772.8175 - val_loss: 2082.2363\n",
      "Epoch 4/20\n",
      "825/825 [==============================] - 37s 45ms/step - loss: 1770.1199 - val_loss: 2098.2375\n",
      "Epoch 5/20\n",
      "825/825 [==============================] - 39s 47ms/step - loss: 1454.6582 - val_loss: 688.9807\n",
      "Epoch 6/20\n",
      "825/825 [==============================] - 40s 48ms/step - loss: 401.9694 - val_loss: 337.3282\n",
      "Epoch 7/20\n",
      "478/825 [================>.............] - ETA: 16s - loss: 319.7933"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [04:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Model fitting\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/device:GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 42\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_rul\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# callbacks=[cb],\u001b[39;49;00m\n\u001b[0;32m     47\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     mse\u001b[38;5;241m.\u001b[39mappend(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     50\u001b[0m     y_hat_val_split \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_array)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "# Data prepration\n",
    "sequence_length = 35\n",
    "alpha = 0.2\n",
    "upper = 125\n",
    "    \n",
    "learning_rate_ = 0.001\n",
    "dropout = 0.4\n",
    "activation = 'tanh'\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "train = rul_piecewise_fct(train, upper)\n",
    "\n",
    "X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "# create sequences train, test\n",
    "train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "test_rul = rul_piecewise_fct(y_test,upper)\n",
    "print(train_array.shape, label_array.shape, test_array.shape)\n",
    "\n",
    "\n",
    "\n",
    "for md in tqdm.tqdm(range(1,5)):\n",
    "    #Model creation\n",
    "    weights_file = \"fd004_result/\" + str(md) + 'lstm_hyper_parameter_weights.h5'\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model004(input_shape=input_shape, weights_file=weights_file, nb_layers=md)\n",
    "        \n",
    "    mse_val = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    mse = []\n",
    "    results = pd.DataFrame()\n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "\n",
    "        #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "        #  'nodes':str(nodes_per_layer),\n",
    "         'dropout':dropout, 'activation':activation, 'batch_size':batch_size,\n",
    "         'TW' : sequence_length, 'alpha' : alpha, 'upper' : upper}\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results.to_csv('fd004_result/nblayers/performance.csv')\n",
    "    \n",
    "    with tf.device('/device:GPU:0'):\n",
    "            for rd in range(5):\n",
    "\n",
    "                # Get explanation and calculate the score\n",
    "                # Echantillonage\n",
    "                n_individus = test_array.shape[0]\n",
    "\n",
    "                # # Choisir aléatoirement 5 indices d'individus\n",
    "                np.random.seed(rd)\n",
    "                indices_choisis = np.random.choice(n_individus, size=5, replace=False)\n",
    "\n",
    "                # Sélectionner les données correspondant aux indices choisis\n",
    "                test_array_sampling = test_array[indices_choisis, :, :]\n",
    "                label_array_sampling = y_test.values[indices_choisis, :]\n",
    "\n",
    "                # Afficher les dimensions des données sélectionnées\n",
    "                print(test_array_sampling.shape, label_array_sampling.shape)\n",
    "                \n",
    "                # distance matrix XX'\n",
    "                X_dist = pd.DataFrame(squareform(pdist(test_array_sampling.reshape((test_array_sampling.shape[0], -1)))))\n",
    "\n",
    "                # LIME\n",
    "                lime_explainer = RecurrentTabularExplainer(test_array, training_labels=label_array,\n",
    "                                                                feature_names=remaining_sensors,\n",
    "                                                                mode = 'regression',\n",
    "                                                                )\n",
    "                lime_values = get_lime_explanation(test_array_sampling, e = model.predict)\n",
    "                lime_values.shape\n",
    "                Lime_dist = pd.DataFrame(squareform(pdist(lime_values))) # Lime values explanation matrix\n",
    "\n",
    "                #Lime's metrics\n",
    "                list_metrics_lime = {}\n",
    "                list_metrics_lime['identity'] = identity(X_dist, Lime_dist)\n",
    "                list_metrics_lime['separability'] = separability(X_dist, Lime_dist)\n",
    "                list_metrics_lime['stability'] = stability(X_dist, Lime_dist)\n",
    "                list_metrics_lime['coherence'], list_metrics_lime['completness'], list_metrics_lime['congruence'] = coherence(model=model.predict, \n",
    "                                                                explainer = get_lime_explanation,\n",
    "                                                            samples=test_array_sampling,\n",
    "                                                                targets=label_array, e = model.predict)\n",
    "                list_metrics_lime['selectivity'] = selectivity(model=model.predict, explainer = get_lime_explanation,\n",
    "                                                            samples=test_array_sampling, e_x = model.predict)\n",
    "                list_metrics_lime['accumen'] = acumen(get_lime_explanation, test_array_sampling, e=model.predict)\n",
    "                list_metrics_lime['Verm_stability'] = stability_Velmurugan(get_lime_explanation, test_array_sampling,\n",
    "                                                                            e=model.predict, top_features=200)\n",
    "                list_metrics_lime['fidelity'], list_metrics_lime['sparsity'] = fidelity(model=model.predict, \n",
    "                                                                explainer = get_lime_explanation,\n",
    "                                                                samples=test_array_sampling,\n",
    "                                                                e = model.predict, L2X=True)\n",
    "                list_metrics_lime['instability'] = instability(model=model.predict, \n",
    "                                                                explainer = get_lime_explanation,\n",
    "                                                                samples=test_array_sampling,\n",
    "                                                                e = model.predict, L2X=True)\n",
    "                \n",
    "                list_metrics_lime['alpha'] = alpha\n",
    "                list_metrics_lime['upper'] = upper\n",
    "                list_metrics_lime['sequence_length'] = sequence_length\n",
    "                list_metrics_lime['explainer'] = 'lime'\n",
    "                list_metrics_lime['model'] = md\n",
    "\n",
    "                df_metrics = pd.concat([df_metrics, pd.DataFrame([list_metrics_lime])])\n",
    "\n",
    "                # SHAP\n",
    "                e = KernelSHAP(model)\n",
    "                shapvalues = get_explainations(test_array_sampling, e)\n",
    "                shapvalues.shape\n",
    "\n",
    "                list_metrics_shap = {}\n",
    "                shap_dist = pd.DataFrame(squareform(pdist(shapvalues))) # shap values explanation matrix\n",
    "\n",
    "                list_metrics_shap['identity'] = identity(X_dist, shap_dist)\n",
    "                list_metrics_shap['separability'] = separability(X_dist, shap_dist)\n",
    "                list_metrics_shap['stability'] = stability(X_dist, shap_dist)\n",
    "                list_metrics_shap['coherence'], list_metrics_shap['completness'], list_metrics_shap['congruence'] = coherence(model=model.predict, \n",
    "                                                                explainer = get_explainations,\n",
    "                                                            samples=test_array_sampling,\n",
    "                                                                targets=label_array, e = e)\n",
    "                list_metrics_shap['selectivity'] = selectivity(model=model.predict, explainer = get_explainations,\n",
    "                                                samples=test_array_sampling, e_x=e)\n",
    "                list_metrics_shap['accumen'] = acumen(get_explainations, test_array_sampling, e=e)\n",
    "                list_metrics_shap['Verm_stability'] = stability_Velmurugan(get_explainations, test_array_sampling,\n",
    "                                                                            e=e, top_features=200)\n",
    "                list_metrics_shap['fidelity'], list_metrics_shap['sparsity']= fidelity(model=model.predict, \n",
    "                                                                explainer = get_explainations,\n",
    "                                                                samples=test_array_sampling,\n",
    "                                                                e = e)\n",
    "                list_metrics_shap['instability']= instability(model=model.predict, \n",
    "                                                                explainer = get_explainations,\n",
    "                                                                samples=test_array_sampling,\n",
    "                                                                e = e)\n",
    "                \n",
    "                list_metrics_shap['alpha'] = alpha\n",
    "                list_metrics_shap['upper'] = upper\n",
    "                list_metrics_shap['sequence_length'] = sequence_length\n",
    "                list_metrics_shap['explainer'] = 'shap'\n",
    "                list_metrics_shap['model'] = md\n",
    "\n",
    "                df_metrics = pd.concat([df_metrics, pd.DataFrame([list_metrics_shap])])\n",
    "                \n",
    "                # L2X\n",
    "                e = L2X(model.predict, test_array_sampling)\n",
    "                l2xvalues = get_explainations(test_array_sampling, e, L2X=True)\n",
    "                l2xvalues.shape\n",
    "\n",
    "                # L2X's metrics\n",
    "                list_metrics_l2x = {}\n",
    "                l2x_dist = pd.DataFrame(squareform(pdist(l2xvalues))) # Lime values explanation matrix\n",
    "\n",
    "                list_metrics_l2x['identity'] = identity(X_dist, l2x_dist)\n",
    "                list_metrics_l2x['separability'] = separability(X_dist, l2x_dist)\n",
    "                list_metrics_l2x['stability'] = stability(X_dist, l2x_dist)\n",
    "                list_metrics_l2x['coherence'], list_metrics_l2x['completness'], list_metrics_l2x['congruence'] = coherence(model=model.predict, explainer = get_explainations,\n",
    "                                                            samples=test_array_sampling, targets=label_array_sampling, e = e, L2X=True)\n",
    "                list_metrics_l2x['selectivity'] = selectivity(model=model.predict, explainer = get_explainations,\n",
    "                                                samples=test_array_sampling, e_x=e, L2X=True)\n",
    "                list_metrics_l2x['accumen'] = acumen(get_explainations, test_array_sampling, e=e, L2X=True)\n",
    "                list_metrics_l2x['Verm_stability'] = stability_Velmurugan(get_explainations, test_array_sampling,\n",
    "                                                                            e=e, top_features=200, L2X=True)\n",
    "                list_metrics_l2x['fidelity'], list_metrics_l2x['sparsity']= fidelity(model=model.predict, \n",
    "                                                                explainer = get_explainations,\n",
    "                                                                samples=test_array_sampling,\n",
    "                                                                e = e, L2X=True)\n",
    "                list_metrics_l2x['instability'] = instability(model=model.predict, \n",
    "                                                                explainer = get_explainations,\n",
    "                                                                samples=test_array_sampling,\n",
    "                                                                e = e, L2X=True)\n",
    "                \n",
    "                list_metrics_l2x['alpha'] = alpha\n",
    "                list_metrics_l2x['upper'] = upper\n",
    "                list_metrics_l2x['sequence_length'] = sequence_length\n",
    "                list_metrics_l2x['explainer'] = 'l2x'\n",
    "                list_metrics_l2x['model'] = md\n",
    "\n",
    "\n",
    "                df_metrics = pd.concat([df_metrics, pd.DataFrame([list_metrics_l2x])])\n",
    "            df_metrics.to_csv('fd004_result/nblayers/quality_by_layer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for kk in range(1,5):\n",
    "    print(kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identity</th>\n",
       "      <th>separability</th>\n",
       "      <th>stability</th>\n",
       "      <th>coherence</th>\n",
       "      <th>completness</th>\n",
       "      <th>congruence</th>\n",
       "      <th>selectivity</th>\n",
       "      <th>accumen</th>\n",
       "      <th>Verm_stability</th>\n",
       "      <th>fidelity</th>\n",
       "      <th>sparsity</th>\n",
       "      <th>instability</th>\n",
       "      <th>alpha</th>\n",
       "      <th>upper</th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>explainer</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.396575</td>\n",
       "      <td>0.859858</td>\n",
       "      <td>0.231712</td>\n",
       "      <td>0.600059</td>\n",
       "      <td>0.010349</td>\n",
       "      <td>0.745336</td>\n",
       "      <td>0.038609</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.496626</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.079117</td>\n",
       "      <td>1.384176</td>\n",
       "      <td>0.152816</td>\n",
       "      <td>0.602597</td>\n",
       "      <td>0.484440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.691115</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.525450</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.317976</td>\n",
       "      <td>4.816301</td>\n",
       "      <td>0.157744</td>\n",
       "      <td>0.620165</td>\n",
       "      <td>0.019596</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026011</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.656410</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.232725</td>\n",
       "      <td>2.564634</td>\n",
       "      <td>0.215667</td>\n",
       "      <td>0.663364</td>\n",
       "      <td>0.079706</td>\n",
       "      <td>0.604761</td>\n",
       "      <td>0.084158</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.331964</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>0.676175</td>\n",
       "      <td>0.005961</td>\n",
       "      <td>0.653576</td>\n",
       "      <td>0.490018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.719074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.185049</td>\n",
       "      <td>17.869755</td>\n",
       "      <td>0.162726</td>\n",
       "      <td>0.628326</td>\n",
       "      <td>0.041394</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.207279</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.560001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.170843</td>\n",
       "      <td>0.572378</td>\n",
       "      <td>0.199016</td>\n",
       "      <td>0.679362</td>\n",
       "      <td>0.057615</td>\n",
       "      <td>0.564015</td>\n",
       "      <td>0.103174</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.399297</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.128458</td>\n",
       "      <td>0.720116</td>\n",
       "      <td>0.208400</td>\n",
       "      <td>0.657258</td>\n",
       "      <td>0.450789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.405441</td>\n",
       "      <td>0.007326</td>\n",
       "      <td>0.338765</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.202384</td>\n",
       "      <td>29.547411</td>\n",
       "      <td>0.226149</td>\n",
       "      <td>0.637773</td>\n",
       "      <td>0.043963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.265515</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.381330</td>\n",
       "      <td>0.115990</td>\n",
       "      <td>0.286515</td>\n",
       "      <td>0.590704</td>\n",
       "      <td>0.178128</td>\n",
       "      <td>0.596612</td>\n",
       "      <td>0.060922</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.349753</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.140749</td>\n",
       "      <td>0.804550</td>\n",
       "      <td>0.281498</td>\n",
       "      <td>0.513080</td>\n",
       "      <td>0.484771</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.566952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.517477</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.313606</td>\n",
       "      <td>0.693807</td>\n",
       "      <td>0.246518</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>0.031413</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.046175</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.560001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.330370</td>\n",
       "      <td>0.860088</td>\n",
       "      <td>0.337014</td>\n",
       "      <td>0.599503</td>\n",
       "      <td>0.009541</td>\n",
       "      <td>0.782007</td>\n",
       "      <td>-0.046700</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.524686</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.034732</td>\n",
       "      <td>1.151014</td>\n",
       "      <td>0.063282</td>\n",
       "      <td>0.557506</td>\n",
       "      <td>0.466202</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.610121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499842</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.292617</td>\n",
       "      <td>2.407085</td>\n",
       "      <td>0.295503</td>\n",
       "      <td>0.584588</td>\n",
       "      <td>0.017468</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022505</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.600001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286827</td>\n",
       "      <td>49.396999</td>\n",
       "      <td>0.239891</td>\n",
       "      <td>0.693365</td>\n",
       "      <td>0.144881</td>\n",
       "      <td>0.786082</td>\n",
       "      <td>0.036295</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.524357</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_2l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.287563</td>\n",
       "      <td>30.262339</td>\n",
       "      <td>0.241532</td>\n",
       "      <td>0.808652</td>\n",
       "      <td>0.443156</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.285056</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.505712</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_2l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.413946</td>\n",
       "      <td>0.191605</td>\n",
       "      <td>0.328493</td>\n",
       "      <td>0.640844</td>\n",
       "      <td>0.061138</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.163585</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_2l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.213815</td>\n",
       "      <td>1.634544</td>\n",
       "      <td>0.184792</td>\n",
       "      <td>0.744335</td>\n",
       "      <td>0.142312</td>\n",
       "      <td>0.704590</td>\n",
       "      <td>0.089636</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.476701</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_2l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.214613</td>\n",
       "      <td>0.915946</td>\n",
       "      <td>0.186282</td>\n",
       "      <td>0.893791</td>\n",
       "      <td>0.439193</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.309285</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.628534</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_2l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.318113</td>\n",
       "      <td>0.111813</td>\n",
       "      <td>0.265716</td>\n",
       "      <td>0.747613</td>\n",
       "      <td>0.018422</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.248680</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_2l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.140671</td>\n",
       "      <td>51.445030</td>\n",
       "      <td>0.173164</td>\n",
       "      <td>0.751131</td>\n",
       "      <td>0.037064</td>\n",
       "      <td>0.710701</td>\n",
       "      <td>0.093718</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.524846</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_2l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.141408</td>\n",
       "      <td>142.847015</td>\n",
       "      <td>0.173968</td>\n",
       "      <td>0.895421</td>\n",
       "      <td>0.459303</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.365618</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.457830</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_2l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.365631</td>\n",
       "      <td>0.294537</td>\n",
       "      <td>0.340095</td>\n",
       "      <td>0.802410</td>\n",
       "      <td>0.046826</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.330937</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.599999</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_2l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.321116</td>\n",
       "      <td>0.310650</td>\n",
       "      <td>0.275597</td>\n",
       "      <td>0.747427</td>\n",
       "      <td>0.227450</td>\n",
       "      <td>0.759597</td>\n",
       "      <td>0.029217</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.565500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_2l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.322820</td>\n",
       "      <td>0.132435</td>\n",
       "      <td>0.276430</td>\n",
       "      <td>0.926621</td>\n",
       "      <td>0.422606</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.280649</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.626375</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_2l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.484291</td>\n",
       "      <td>0.183390</td>\n",
       "      <td>0.374326</td>\n",
       "      <td>0.712550</td>\n",
       "      <td>0.029064</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.129006</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_2l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.299393</td>\n",
       "      <td>0.418075</td>\n",
       "      <td>0.345499</td>\n",
       "      <td>0.750007</td>\n",
       "      <td>0.113486</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>-0.144682</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.436453</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_2l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.300909</td>\n",
       "      <td>0.180376</td>\n",
       "      <td>0.346522</td>\n",
       "      <td>0.919166</td>\n",
       "      <td>0.409761</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.335593</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.634184</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_2l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.407335</td>\n",
       "      <td>3.954531</td>\n",
       "      <td>0.447895</td>\n",
       "      <td>0.708375</td>\n",
       "      <td>0.041908</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.159735</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_2l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.322608</td>\n",
       "      <td>0.071242</td>\n",
       "      <td>0.272324</td>\n",
       "      <td>0.628197</td>\n",
       "      <td>0.088495</td>\n",
       "      <td>0.735149</td>\n",
       "      <td>0.038030</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.536160</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_3l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.319824</td>\n",
       "      <td>10.688745</td>\n",
       "      <td>0.268687</td>\n",
       "      <td>0.717622</td>\n",
       "      <td>0.652257</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.152323</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.599451</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_3l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.372576</td>\n",
       "      <td>0.196655</td>\n",
       "      <td>0.290260</td>\n",
       "      <td>0.589952</td>\n",
       "      <td>0.019817</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.154123</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_3l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.231930</td>\n",
       "      <td>0.016463</td>\n",
       "      <td>0.204488</td>\n",
       "      <td>0.681206</td>\n",
       "      <td>0.157505</td>\n",
       "      <td>0.633284</td>\n",
       "      <td>0.120818</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.602038</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_3l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.231275</td>\n",
       "      <td>2.276539</td>\n",
       "      <td>0.201275</td>\n",
       "      <td>0.768503</td>\n",
       "      <td>0.594128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.254026</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.742730</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_3l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.276183</td>\n",
       "      <td>32.979453</td>\n",
       "      <td>0.239957</td>\n",
       "      <td>0.703609</td>\n",
       "      <td>0.018569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.258656</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_3l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.153478</td>\n",
       "      <td>0.021941</td>\n",
       "      <td>0.185211</td>\n",
       "      <td>0.682628</td>\n",
       "      <td>0.121541</td>\n",
       "      <td>0.635321</td>\n",
       "      <td>0.121144</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.507946</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_3l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.154346</td>\n",
       "      <td>4.107413</td>\n",
       "      <td>0.183162</td>\n",
       "      <td>0.771354</td>\n",
       "      <td>0.596844</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.355516</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.471422</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_3l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.310220</td>\n",
       "      <td>0.349036</td>\n",
       "      <td>0.314580</td>\n",
       "      <td>0.636831</td>\n",
       "      <td>0.024440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.347283</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_3l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375816</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.314196</td>\n",
       "      <td>0.734227</td>\n",
       "      <td>0.238544</td>\n",
       "      <td>0.682179</td>\n",
       "      <td>-0.096898</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.636268</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_3l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.373052</td>\n",
       "      <td>0.069501</td>\n",
       "      <td>0.312838</td>\n",
       "      <td>0.788572</td>\n",
       "      <td>0.592514</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.045359</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.687091</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_3l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.436983</td>\n",
       "      <td>30.658408</td>\n",
       "      <td>0.343664</td>\n",
       "      <td>0.704721</td>\n",
       "      <td>0.024147</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050481</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.657142</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_3l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.331824</td>\n",
       "      <td>0.005546</td>\n",
       "      <td>0.353083</td>\n",
       "      <td>0.751769</td>\n",
       "      <td>0.121676</td>\n",
       "      <td>0.739224</td>\n",
       "      <td>-0.126878</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.534503</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_3l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.327148</td>\n",
       "      <td>0.268169</td>\n",
       "      <td>0.351547</td>\n",
       "      <td>0.783827</td>\n",
       "      <td>0.595743</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.098432</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.379065</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_3l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.385358</td>\n",
       "      <td>108.238703</td>\n",
       "      <td>0.402994</td>\n",
       "      <td>0.546876</td>\n",
       "      <td>0.015560</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.085075</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_3l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.337246</td>\n",
       "      <td>0.230087</td>\n",
       "      <td>0.276676</td>\n",
       "      <td>0.724088</td>\n",
       "      <td>0.128661</td>\n",
       "      <td>0.704590</td>\n",
       "      <td>0.062579</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.669631</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_4l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.337247</td>\n",
       "      <td>0.200072</td>\n",
       "      <td>0.276677</td>\n",
       "      <td>0.831078</td>\n",
       "      <td>0.491046</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.370966</td>\n",
       "      <td>0.014652</td>\n",
       "      <td>0.261549</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_4l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.367535</td>\n",
       "      <td>0.332972</td>\n",
       "      <td>0.297435</td>\n",
       "      <td>0.680814</td>\n",
       "      <td>0.022018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.173178</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_4l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.259650</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.226046</td>\n",
       "      <td>0.841007</td>\n",
       "      <td>0.105615</td>\n",
       "      <td>0.600687</td>\n",
       "      <td>0.123129</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.526363</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_4l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.259651</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.226046</td>\n",
       "      <td>0.977756</td>\n",
       "      <td>0.490495</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.254806</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.685159</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_4l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.280018</td>\n",
       "      <td>148.463716</td>\n",
       "      <td>0.240457</td>\n",
       "      <td>0.798284</td>\n",
       "      <td>0.008661</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.228387</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_4l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.164440</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.187174</td>\n",
       "      <td>0.814851</td>\n",
       "      <td>0.096954</td>\n",
       "      <td>0.590500</td>\n",
       "      <td>0.142939</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.583460</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_4l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.164441</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.187174</td>\n",
       "      <td>0.962847</td>\n",
       "      <td>0.492330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.342434</td>\n",
       "      <td>0.014652</td>\n",
       "      <td>0.430916</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_4l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.317756</td>\n",
       "      <td>0.321815</td>\n",
       "      <td>0.305375</td>\n",
       "      <td>0.857340</td>\n",
       "      <td>0.051670</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333818</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.371428</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_4l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.401509</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.328080</td>\n",
       "      <td>0.788322</td>\n",
       "      <td>0.079119</td>\n",
       "      <td>0.655694</td>\n",
       "      <td>0.084108</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.527633</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_4l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.401510</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.328080</td>\n",
       "      <td>0.977368</td>\n",
       "      <td>0.469431</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.168029</td>\n",
       "      <td>0.016484</td>\n",
       "      <td>0.683058</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_4l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434162</td>\n",
       "      <td>357.527775</td>\n",
       "      <td>0.340779</td>\n",
       "      <td>0.750899</td>\n",
       "      <td>0.034936</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.038516</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_4l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361995</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.353505</td>\n",
       "      <td>0.826449</td>\n",
       "      <td>0.069798</td>\n",
       "      <td>0.682179</td>\n",
       "      <td>-0.080713</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.456017</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>lime</td>\n",
       "      <td>model_4l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361996</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.353505</td>\n",
       "      <td>0.976690</td>\n",
       "      <td>0.479009</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.370540</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.390347</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>shap</td>\n",
       "      <td>model_4l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.397401</td>\n",
       "      <td>261.227208</td>\n",
       "      <td>0.389399</td>\n",
       "      <td>0.746747</td>\n",
       "      <td>0.027523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.072443</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.2</td>\n",
       "      <td>125</td>\n",
       "      <td>39</td>\n",
       "      <td>l2x</td>\n",
       "      <td>model_4l</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   identity  separability  stability  coherence  completness  congruence  \\\n",
       "0       1.0           1.0        1.0   0.396575     0.859858    0.231712   \n",
       "0       1.0           1.0        1.0   0.079117     1.384176    0.152816   \n",
       "0       1.0           1.0        1.0   0.317976     4.816301    0.157744   \n",
       "0       1.0           1.0        1.0   0.232725     2.564634    0.215667   \n",
       "0       1.0           1.0        1.0   0.004815     0.676175    0.005961   \n",
       "0       1.0           1.0        1.0   0.185049    17.869755    0.162726   \n",
       "0       1.0           1.0        1.0   0.170843     0.572378    0.199016   \n",
       "0       1.0           1.0        1.0   0.128458     0.720116    0.208400   \n",
       "0       1.0           1.0        1.0   0.202384    29.547411    0.226149   \n",
       "0       1.0           1.0        1.0   0.381330     0.115990    0.286515   \n",
       "0       1.0           1.0        1.0   0.140749     0.804550    0.281498   \n",
       "0       1.0           1.0        1.0   0.313606     0.693807    0.246518   \n",
       "0       1.0           1.0        1.0   0.330370     0.860088    0.337014   \n",
       "0       1.0           1.0        1.0   0.034732     1.151014    0.063282   \n",
       "0       1.0           1.0        1.0   0.292617     2.407085    0.295503   \n",
       "0       1.0           1.0        1.0   0.286827    49.396999    0.239891   \n",
       "0       1.0           1.0        1.0   0.287563    30.262339    0.241532   \n",
       "0       1.0           1.0        1.0   0.413946     0.191605    0.328493   \n",
       "0       1.0           1.0        1.0   0.213815     1.634544    0.184792   \n",
       "0       1.0           1.0        1.0   0.214613     0.915946    0.186282   \n",
       "0       1.0           1.0        1.0   0.318113     0.111813    0.265716   \n",
       "0       1.0           1.0        1.0   0.140671    51.445030    0.173164   \n",
       "0       1.0           1.0        1.0   0.141408   142.847015    0.173968   \n",
       "0       1.0           1.0        1.0   0.365631     0.294537    0.340095   \n",
       "0       1.0           1.0        1.0   0.321116     0.310650    0.275597   \n",
       "0       1.0           1.0        1.0   0.322820     0.132435    0.276430   \n",
       "0       1.0           1.0        1.0   0.484291     0.183390    0.374326   \n",
       "0       1.0           1.0        1.0   0.299393     0.418075    0.345499   \n",
       "0       1.0           1.0        1.0   0.300909     0.180376    0.346522   \n",
       "0       1.0           1.0        1.0   0.407335     3.954531    0.447895   \n",
       "0       1.0           1.0        1.0   0.322608     0.071242    0.272324   \n",
       "0       1.0           1.0        1.0   0.319824    10.688745    0.268687   \n",
       "0       1.0           1.0        1.0   0.372576     0.196655    0.290260   \n",
       "0       1.0           1.0        1.0   0.231930     0.016463    0.204488   \n",
       "0       1.0           1.0        1.0   0.231275     2.276539    0.201275   \n",
       "0       1.0           1.0        1.0   0.276183    32.979453    0.239957   \n",
       "0       1.0           1.0        1.0   0.153478     0.021941    0.185211   \n",
       "0       1.0           1.0        1.0   0.154346     4.107413    0.183162   \n",
       "0       1.0           1.0        1.0   0.310220     0.349036    0.314580   \n",
       "0       1.0           1.0        1.0   0.375816     0.000466    0.314196   \n",
       "0       1.0           1.0        1.0   0.373052     0.069501    0.312838   \n",
       "0       1.0           1.0        1.0   0.436983    30.658408    0.343664   \n",
       "0       1.0           1.0        1.0   0.331824     0.005546    0.353083   \n",
       "0       1.0           1.0        1.0   0.327148     0.268169    0.351547   \n",
       "0       1.0           1.0        1.0   0.385358   108.238703    0.402994   \n",
       "0       1.0           1.0        1.0   0.337246     0.230087    0.276676   \n",
       "0       1.0           1.0        1.0   0.337247     0.200072    0.276677   \n",
       "0       1.0           1.0        1.0   0.367535     0.332972    0.297435   \n",
       "0       1.0           1.0        1.0   0.259650     0.000254    0.226046   \n",
       "0       1.0           1.0        1.0   0.259651     0.000017    0.226046   \n",
       "0       1.0           1.0        1.0   0.280018   148.463716    0.240457   \n",
       "0       1.0           1.0        1.0   0.164440     0.001003    0.187174   \n",
       "0       1.0           1.0        1.0   0.164441     0.000119    0.187174   \n",
       "0       1.0           1.0        1.0   0.317756     0.321815    0.305375   \n",
       "0       1.0           1.0        1.0   0.401509     0.000072    0.328080   \n",
       "0       1.0           1.0        1.0   0.401510     0.000018    0.328080   \n",
       "0       1.0           1.0        1.0   0.434162   357.527775    0.340779   \n",
       "0       1.0           1.0        1.0   0.361995     0.000110    0.353505   \n",
       "0       1.0           1.0        1.0   0.361996     0.000029    0.353505   \n",
       "0       1.0           1.0        1.0   0.397401   261.227208    0.389399   \n",
       "\n",
       "   selectivity   accumen  Verm_stability  fidelity  sparsity  instability  \\\n",
       "0     0.600059  0.010349        0.745336  0.038609  0.018315     0.496626   \n",
       "0     0.602597  0.484440        1.000000  0.691115  0.018315     0.525450   \n",
       "0     0.620165  0.019596        1.000000  0.026011  0.018315     0.656410   \n",
       "0     0.663364  0.079706        0.604761  0.084158  0.018315     0.331964   \n",
       "0     0.653576  0.490018        1.000000  0.719074  0.000000     0.501500   \n",
       "0     0.628326  0.041394        1.000000  0.207279  0.018315     0.560001   \n",
       "0     0.679362  0.057615        0.564015  0.103174  0.018315     0.399297   \n",
       "0     0.657258  0.450789        1.000000  0.405441  0.007326     0.338765   \n",
       "0     0.637773  0.043963        1.000000  0.265515  0.018315     0.466667   \n",
       "0     0.590704  0.178128        0.596612  0.060922  0.018315     0.349753   \n",
       "0     0.513080  0.484771        1.000000  0.566952  0.000000     0.517477   \n",
       "0     0.674184  0.031413        1.000000 -0.046175  0.018315     0.560001   \n",
       "0     0.599503  0.009541        0.782007 -0.046700  0.018315     0.524686   \n",
       "0     0.557506  0.466202        1.000000  0.610121  0.000000     0.499842   \n",
       "0     0.584588  0.017468        1.000000  0.022505  0.018315     0.600001   \n",
       "0     0.693365  0.144881        0.786082  0.036295  0.018315     0.524357   \n",
       "0     0.808652  0.443156        1.000000  0.285056  0.018315     0.505712   \n",
       "0     0.640844  0.061138        1.000000  0.163585  0.018315     0.666667   \n",
       "0     0.744335  0.142312        0.704590  0.089636  0.018315     0.476701   \n",
       "0     0.893791  0.439193        1.000000  0.309285  0.018315     0.628534   \n",
       "0     0.747613  0.018422        1.000000  0.248680  0.018315     0.560000   \n",
       "0     0.751131  0.037064        0.710701  0.093718  0.018315     0.524846   \n",
       "0     0.895421  0.459303        1.000000  0.365618  0.018315     0.457830   \n",
       "0     0.802410  0.046826        1.000000  0.330937  0.018315     0.599999   \n",
       "0     0.747427  0.227450        0.759597  0.029217  0.018315     0.565500   \n",
       "0     0.926621  0.422606        1.000000  0.280649  0.018315     0.626375   \n",
       "0     0.712550  0.029064        1.000000  0.129006  0.018315     0.520000   \n",
       "0     0.750007  0.113486        0.769784 -0.144682  0.018315     0.436453   \n",
       "0     0.919166  0.409761        1.000000  0.335593  0.018315     0.634184   \n",
       "0     0.708375  0.041908        1.000000  0.159735  0.018315     0.620000   \n",
       "0     0.628197  0.088495        0.735149  0.038030  0.018315     0.536160   \n",
       "0     0.717622  0.652257        1.000000  0.152323  0.018315     0.599451   \n",
       "0     0.589952  0.019817        1.000000  0.154123  0.018315     0.675000   \n",
       "0     0.681206  0.157505        0.633284  0.120818  0.018315     0.602038   \n",
       "0     0.768503  0.594128        1.000000  0.254026  0.018315     0.742730   \n",
       "0     0.703609  0.018569        1.000000  0.258656  0.018315     0.375000   \n",
       "0     0.682628  0.121541        0.635321  0.121144  0.018315     0.507946   \n",
       "0     0.771354  0.596844        1.000000  0.355516  0.018315     0.471422   \n",
       "0     0.636831  0.024440        1.000000  0.347283  0.018315     0.600000   \n",
       "0     0.734227  0.238544        0.682179 -0.096898  0.018315     0.636268   \n",
       "0     0.788572  0.592514        1.000000  0.045359  0.018315     0.687091   \n",
       "0     0.704721  0.024147        1.000000  0.050481  0.018315     0.657142   \n",
       "0     0.751769  0.121676        0.739224 -0.126878  0.018315     0.534503   \n",
       "0     0.783827  0.595743        1.000000  0.098432  0.018315     0.379065   \n",
       "0     0.546876  0.015560        1.000000  0.085075  0.018315     0.475000   \n",
       "0     0.724088  0.128661        0.704590  0.062579  0.018315     0.669631   \n",
       "0     0.831078  0.491046        1.000000  0.370966  0.014652     0.261549   \n",
       "0     0.680814  0.022018        1.000000  0.173178  0.018315     0.650000   \n",
       "0     0.841007  0.105615        0.600687  0.123129  0.018315     0.526363   \n",
       "0     0.977756  0.490495        1.000000  0.254806  0.018315     0.685159   \n",
       "0     0.798284  0.008661        1.000000  0.228387  0.018315     0.300000   \n",
       "0     0.814851  0.096954        0.590500  0.142939  0.018315     0.583460   \n",
       "0     0.962847  0.492330        1.000000  0.342434  0.014652     0.430916   \n",
       "0     0.857340  0.051670        1.000000  0.333818  0.018315     0.371428   \n",
       "0     0.788322  0.079119        0.655694  0.084108  0.018315     0.527633   \n",
       "0     0.977368  0.469431        1.000000  0.168029  0.016484     0.683058   \n",
       "0     0.750899  0.034936        1.000000 -0.038516  0.018315     0.525000   \n",
       "0     0.826449  0.069798        0.682179 -0.080713  0.018315     0.456017   \n",
       "0     0.976690  0.479009        1.000000  0.370540  0.018315     0.390347   \n",
       "0     0.746747  0.027523        1.000000  0.072443  0.018315     0.514286   \n",
       "\n",
       "   alpha  upper  sequence_length explainer     model  \n",
       "0    0.2    125               39      lime  model_1l  \n",
       "0    0.2    125               39      shap  model_1l  \n",
       "0    0.2    125               39       l2x  model_1l  \n",
       "0    0.2    125               39      lime  model_1l  \n",
       "0    0.2    125               39      shap  model_1l  \n",
       "0    0.2    125               39       l2x  model_1l  \n",
       "0    0.2    125               39      lime  model_1l  \n",
       "0    0.2    125               39      shap  model_1l  \n",
       "0    0.2    125               39       l2x  model_1l  \n",
       "0    0.2    125               39      lime  model_1l  \n",
       "0    0.2    125               39      shap  model_1l  \n",
       "0    0.2    125               39       l2x  model_1l  \n",
       "0    0.2    125               39      lime  model_1l  \n",
       "0    0.2    125               39      shap  model_1l  \n",
       "0    0.2    125               39       l2x  model_1l  \n",
       "0    0.2    125               39      lime  model_2l  \n",
       "0    0.2    125               39      shap  model_2l  \n",
       "0    0.2    125               39       l2x  model_2l  \n",
       "0    0.2    125               39      lime  model_2l  \n",
       "0    0.2    125               39      shap  model_2l  \n",
       "0    0.2    125               39       l2x  model_2l  \n",
       "0    0.2    125               39      lime  model_2l  \n",
       "0    0.2    125               39      shap  model_2l  \n",
       "0    0.2    125               39       l2x  model_2l  \n",
       "0    0.2    125               39      lime  model_2l  \n",
       "0    0.2    125               39      shap  model_2l  \n",
       "0    0.2    125               39       l2x  model_2l  \n",
       "0    0.2    125               39      lime  model_2l  \n",
       "0    0.2    125               39      shap  model_2l  \n",
       "0    0.2    125               39       l2x  model_2l  \n",
       "0    0.2    125               39      lime  model_3l  \n",
       "0    0.2    125               39      shap  model_3l  \n",
       "0    0.2    125               39       l2x  model_3l  \n",
       "0    0.2    125               39      lime  model_3l  \n",
       "0    0.2    125               39      shap  model_3l  \n",
       "0    0.2    125               39       l2x  model_3l  \n",
       "0    0.2    125               39      lime  model_3l  \n",
       "0    0.2    125               39      shap  model_3l  \n",
       "0    0.2    125               39       l2x  model_3l  \n",
       "0    0.2    125               39      lime  model_3l  \n",
       "0    0.2    125               39      shap  model_3l  \n",
       "0    0.2    125               39       l2x  model_3l  \n",
       "0    0.2    125               39      lime  model_3l  \n",
       "0    0.2    125               39      shap  model_3l  \n",
       "0    0.2    125               39       l2x  model_3l  \n",
       "0    0.2    125               39      lime  model_4l  \n",
       "0    0.2    125               39      shap  model_4l  \n",
       "0    0.2    125               39       l2x  model_4l  \n",
       "0    0.2    125               39      lime  model_4l  \n",
       "0    0.2    125               39      shap  model_4l  \n",
       "0    0.2    125               39       l2x  model_4l  \n",
       "0    0.2    125               39      lime  model_4l  \n",
       "0    0.2    125               39      shap  model_4l  \n",
       "0    0.2    125               39       l2x  model_4l  \n",
       "0    0.2    125               39      lime  model_4l  \n",
       "0    0.2    125               39      shap  model_4l  \n",
       "0    0.2    125               39       l2x  model_4l  \n",
       "0    0.2    125               39      lime  model_4l  \n",
       "0    0.2    125               39      shap  model_4l  \n",
       "0    0.2    125               39       l2x  model_4l  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
