{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b84a044e",
   "metadata": {},
   "source": [
    "## Sommaire: <a class=\"anchor\" id=\"sommaire\"></a>\n",
    "* [Sommaire](#sommaire)\n",
    "* [Preambule](#prem)\n",
    "     * [Package Loading](#package)\n",
    "     * [Functions](#function)\n",
    "* [LSTM](#lstm)\n",
    "    * [1.FD001](#fd001)\n",
    "        * [1.1 Data loading](#fd001dataload)\n",
    "        * [1.2 Model selection](#fd001modelselect)\n",
    "    * [2.FD002](#fd002)\n",
    "         * [2.1 Data loading](#fd002dataload)\n",
    "         * [2.2 Model selection](#fd002modelselect)\n",
    "    * [3.FD003](#fd003)\n",
    "         * [3.1 Data loading](#fd003dataload)\n",
    "         * [3.2 Model selection](#fd003modelselect)\n",
    "    * [4.FD004](#fd004)\n",
    "         * [4.1 Data loading](#fd004dataload)\n",
    "         * [4.2 Model selection](#fd004modelselect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcceb1c6",
   "metadata": {},
   "source": [
    "## Preambule <a class=\"anchor\" id=\"prem\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac967e2a",
   "metadata": {},
   "source": [
    "### Packages <a class = \"anchor\" id = \"package\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c230556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import random\n",
    "# import keras\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation\n",
    "from scipy import optimize\n",
    "from methods import *\n",
    "import warnings\n",
    "from tensorflow.keras import optimizers\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9745aa",
   "metadata": {},
   "source": [
    "## Sommaire: <a class=\"anchor\" id=\"sommaire\"></a>\n",
    "* [Sommaire](#sommaire)\n",
    "* [Preambule](#prem)\n",
    "     * [Package Loading](#package)\n",
    "     * [Functions](#function)\n",
    "* [LSTM](#lstm)\n",
    "    * [1.FD001](#fd001)\n",
    "        * [1.1 Data loading](#fd001dataload)\n",
    "        * [1.2 Model selection](#fd001modelselect)\n",
    "    * [2.FD002](#fd002)\n",
    "         * [2.1 Data loading](#fd002dataload)\n",
    "         * [2.2 Model selection](#fd002modelselect)\n",
    "    * [3.FD003](#fd003)\n",
    "         * [3.1 Data loading](#fd003dataload)\n",
    "         * [3.2 Model selection](#fd003modelselect)\n",
    "    * [4.FD004](#fd004)\n",
    "         * [4.1 Data loading](#fd004dataload)\n",
    "         * [4.2 Model selection](#fd004modelselect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a3dc25",
   "metadata": {},
   "source": [
    "### Functions <a class = \"anchor\" id=\"function\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49c05c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for model architecture\n",
    "# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
    "\n",
    "# cb = keras.callbacks.EarlyStopping(monitor='loss', patience=20)\n",
    "def create_model2C(input_shape, nodes_per_layer, dropout, activation):\n",
    "\n",
    "    # weights_file = 'weights_file.h5'\n",
    "    \n",
    "    model = Sequential([LSTM(nodes_per_layer[0], activation='tanh', return_sequences=True),\n",
    "                        # Dropout(dropout),\n",
    "                        LSTM(nodes_per_layer[1], activation=activation),\n",
    "                        Dense(nodes_per_layer[0], activation = 'relu'),\n",
    "                        Dropout(dropout),\n",
    "                        Dense(1)\n",
    "                        ])\n",
    "    # model.save_weights(weights_file)\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model3C(input_shape, nodes_per_layer, dropout, activation):\n",
    "    \n",
    "    # weights_file = 'weights_file.h5'\n",
    "    \n",
    "    model = Sequential([LSTM(nodes_per_layer[0], activation='tanh', return_sequences=True),\n",
    "                        # Dropout(dropout),\n",
    "                        LSTM(nodes_per_layer[1], activation=activation, return_sequences=True),\n",
    "                        LSTM(64, activation=activation),\n",
    "                        Dense(nodes_per_layer[0], activation = 'relu'),\n",
    "                        Dropout(dropout),\n",
    "                        Dense(1)\n",
    "                        ])\n",
    "    # model.save_weights(weights_file)\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model4C(input_shape, nodes_per_layer, dropout, activation):\n",
    "    \n",
    "    # weights_file = 'weights_file.h5'\n",
    "    \n",
    "    model = Sequential([LSTM(nodes_per_layer[0], activation='tanh', return_sequences=True),\n",
    "                        # Dropout(dropout),\n",
    "                        LSTM(nodes_per_layer[1], activation=activation, return_sequences=True),\n",
    "                        LSTM(64, activation=activation, return_sequences=True),\n",
    "                        LSTM(64, activation=activation),\n",
    "                        Dense(64, activation = 'relu'),\n",
    "                        Dense(nodes_per_layer[0], activation = 'relu'),\n",
    "                        Dropout(dropout),\n",
    "                        Dense(1)\n",
    "                        ])\n",
    "    # model.save_weights(weights_file)\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def model001_2C(input_shape, nodes_per_layer, dropout, activation):\n",
    "    '''\n",
    "    node = 256, activation = tanh, dropout = 0.3, bs = 64\n",
    "    '''\n",
    "    weights_file = \"weights_file.h5\"\n",
    "\n",
    "    cb = keras.callbacks.EarlyStopping(monitor='loss', patience=20)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=nodes_per_layer[0], activation='sigmoid', return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(nodes_per_layer[1], activation=activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=Adam(learning_rate=0.01))\n",
    "    model.save_weights(weights_file)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function for the rectified RUL\n",
    "def rul_piecewise_fct(X_train, rul):\n",
    "    \n",
    "    X_train['RUL'].clip(upper=rul, inplace=True)\n",
    "    \n",
    "    return X_train\n",
    "\n",
    "# Function for data preprocessing\n",
    "def prep_data(train, test, drop_sensors, remaining_sensors, alpha):\n",
    "    \n",
    "    X_train_interim = add_operating_condition(train.drop(drop_sensors, axis=1))\n",
    "    X_test_interim = add_operating_condition(test.drop(drop_sensors, axis=1))\n",
    "\n",
    "    X_train_interim, X_test_interim = condition_scaler(X_train_interim, X_test_interim, remaining_sensors)\n",
    "\n",
    "    X_train_interim = exponential_smoothing(X_train_interim, remaining_sensors, 0, alpha)\n",
    "    X_test_interim = exponential_smoothing(X_test_interim, remaining_sensors, 0, alpha)\n",
    "    \n",
    "    return X_train_interim, X_test_interim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f6c01",
   "metadata": {},
   "source": [
    "# LSTM <a class=\"anchor\" id=\"lstm\">  </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bf6218",
   "metadata": {},
   "source": [
    "### FD001  <a class=\"anchor\" id=\"fd001\">  </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b85109",
   "metadata": {},
   "source": [
    "#### Data loading <a id = \"fd001dataload\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "519c11f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20631, 27) (13096, 26) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "train, test, y_test = prepare_data('FD001.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50','P30','Nf','Nc','Ps30','phi',\n",
    "                'NRf','NRc','BPR','htBleed','W31','W32'] # selection based on main_notebook\n",
    "\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "train = rul_piecewise_fct(train, 130)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb2a5d0",
   "metadata": {},
   "source": [
    "#### Model selection <a id = \"fd001modelselect\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8674c5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3584"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lower alpha's perform better, so we can ditch a few high ones to reduce the search space\n",
    "alpha_list = [0.01, 0.05] + list(np.arange(10,60+1,10)/100)\n",
    "\n",
    "sequence_list = list(np.arange(10,40+1,5))\n",
    "epoch_list = list(np.arange(5,20+1,5))\n",
    "nodes_list = [[256, 32], [256, 64]] \n",
    "\n",
    "# lowest dropout=0.1, because I know zero dropout will yield better training results but worse generalization\n",
    "dropouts = list(np.arange(1,3)/10)  \n",
    "\n",
    "# again, earlier testing revealed relu performed significantly worse, so I removed it from the options\n",
    "activation_functions = ['tanh', 'tanh']\n",
    "batch_size_list = [64, 128]\n",
    "sensor_list = [sensor_names]\n",
    "\n",
    "tuning_options = np.prod([len(alpha_list),\n",
    "                          len(sequence_list),\n",
    "                          len(epoch_list),\n",
    "                          len(nodes_list),\n",
    "                          len(dropouts),\n",
    "                          len(activation_functions),\n",
    "                          len(batch_size_list),\n",
    "                          len(sensor_list)])\n",
    "tuning_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e24d04c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 1\n",
    "SEED = 0\n",
    "rul_piecewise = 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "812b4593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1\n",
      "(17731, 30, 14) (17731, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "278/278 [==============================] - 20s 64ms/step - loss: 2655.6743 - val_loss: 1727.4628\n",
      "Epoch 2/25\n",
      "278/278 [==============================] - 18s 64ms/step - loss: 1890.8826 - val_loss: 1679.0699\n",
      "Epoch 3/25\n",
      "278/278 [==============================] - 18s 63ms/step - loss: 1886.8331 - val_loss: 1722.1522\n",
      "Epoch 4/25\n",
      "278/278 [==============================] - 17s 61ms/step - loss: 1831.5364 - val_loss: 738.6825\n",
      "Epoch 5/25\n",
      "278/278 [==============================] - 19s 68ms/step - loss: 444.4911 - val_loss: 232.7926\n",
      "Epoch 6/25\n",
      "278/278 [==============================] - 18s 65ms/step - loss: 269.7959 - val_loss: 501.3882\n",
      "Epoch 7/25\n",
      "278/278 [==============================] - 17s 62ms/step - loss: 251.8006 - val_loss: 208.9615\n",
      "Epoch 8/25\n",
      "278/278 [==============================] - 17s 63ms/step - loss: 231.9857 - val_loss: 234.8961\n",
      "Epoch 9/25\n",
      "278/278 [==============================] - 17s 61ms/step - loss: 227.3443 - val_loss: 206.9987\n",
      "Epoch 10/25\n",
      "278/278 [==============================] - 17s 60ms/step - loss: 220.9366 - val_loss: 344.7443\n",
      "Epoch 11/25\n",
      "278/278 [==============================] - 19s 69ms/step - loss: 214.6006 - val_loss: 260.8032\n",
      "Epoch 12/25\n",
      "278/278 [==============================] - 17s 60ms/step - loss: 212.5859 - val_loss: 224.8300\n",
      "Epoch 13/25\n",
      "278/278 [==============================] - 17s 62ms/step - loss: 211.3689 - val_loss: 203.4934\n",
      "Epoch 14/25\n",
      "278/278 [==============================] - 17s 59ms/step - loss: 209.6046 - val_loss: 211.6736\n",
      "Epoch 15/25\n",
      "278/278 [==============================] - 17s 62ms/step - loss: 204.7699 - val_loss: 211.3891\n",
      "Epoch 16/25\n",
      "278/278 [==============================] - 18s 63ms/step - loss: 203.7573 - val_loss: 226.3459\n",
      "Epoch 17/25\n",
      "278/278 [==============================] - 17s 62ms/step - loss: 201.2644 - val_loss: 200.7251\n",
      "Epoch 18/25\n",
      "278/278 [==============================] - 19s 67ms/step - loss: 203.0090 - val_loss: 205.0770\n",
      "Epoch 19/25\n",
      "278/278 [==============================] - 17s 60ms/step - loss: 197.9978 - val_loss: 212.0796\n",
      "Epoch 20/25\n",
      "278/278 [==============================] - 17s 63ms/step - loss: 197.6800 - val_loss: 215.3278\n",
      "Epoch 21/25\n",
      "278/278 [==============================] - 18s 63ms/step - loss: 193.6586 - val_loss: 210.3091\n",
      "Epoch 22/25\n",
      "278/278 [==============================] - 18s 64ms/step - loss: 196.5207 - val_loss: 219.0444\n",
      "Epoch 23/25\n",
      "278/278 [==============================] - 18s 65ms/step - loss: 195.0849 - val_loss: 211.7124\n",
      "Epoch 24/25\n",
      "278/278 [==============================] - 18s 64ms/step - loss: 194.9660 - val_loss: 209.8448\n",
      "Epoch 25/25\n",
      "278/278 [==============================] - 18s 65ms/step - loss: 193.5692 - val_loss: 243.2268\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "CPU times: user 11min 55s, sys: 3min 50s, total: 15min 46s\n",
      "Wall time: 7min 22s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# results = pd.DataFrame(columns=['RMSE', 'std_RMSE', \n",
    "#                                 'S_score','std_S_score',\n",
    "#                                 'MSE', 'std_MSE',\n",
    "#                                 'nodes', 'dropout',\n",
    "#                                 'activation', 'batch_size'])\n",
    "\n",
    "\n",
    "# for i in range(ITERATIONS):\n",
    "#     if ITERATIONS < 10:\n",
    "#         print('iteration ', i+1)\n",
    "#     elif ((i+1) % 10 == 0):\n",
    "#         print('iteration ', i+1)    \n",
    "#     tf.random.set_seed(SEED)\n",
    "#     mse = []\n",
    "#     R2_val = []\n",
    "#     RMSE = []\n",
    "#     score_val = []\n",
    "    \n",
    "    \n",
    "# # parameter's sample\n",
    "#     alpha = 0.3\n",
    "#     sequence_length = 30\n",
    "#     epochs = 25\n",
    "#     nodes_per_layer = random.sample(nodes_list, 1)[0]\n",
    "#     dropout = random.sample(dropouts, 1)[0]\n",
    "#     activation = random.sample(activation_functions, 1)[0]\n",
    "#     batch_size = random.sample(batch_size_list, 1)[0]\n",
    "#     remaining_sensors = remaining_sensors\n",
    "#     drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "#     # create model\n",
    "#     input_shape = (sequence_length, len(remaining_sensors))\n",
    "#     model = create_model2C(input_shape, nodes_per_layer, dropout, activation)\n",
    "    \n",
    "#     # Data prepration\n",
    "#     X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "#     # create sequences train, test\n",
    "#     train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "#     label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "#     test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "#                for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "#     test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "#     test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "#     print(train_array.shape, label_array.shape, test_array.shape)\n",
    "\n",
    "#     with tf.device('/device:GPU:0'):\n",
    "#         history = model.fit(train_array, label_array,\n",
    "#                                 validation_data=(test_array, test_rul),\n",
    "#                                 epochs=epochs,\n",
    "#                                 batch_size=batch_size,\n",
    "#                                 # callbacks=[cb],\n",
    "#                                 verbose=1)\n",
    "#         mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "#         y_hat_val_split = model.predict(test_array)\n",
    "#         R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "#         RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "#         score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "#     #       append results\n",
    "#     d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "#          'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "#          'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "#          'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "#          'activation':activation, 'batch_size':batch_size}\n",
    "\n",
    "# #     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "#     results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f6bb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.517865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>321.525047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.732727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.850535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>372.648272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>191.837311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13.959899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>352.199345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>194.878784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.073073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>370.466091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>198.051361</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.126784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>376.765614</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.566055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.179949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>409.939201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>201.070984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.252792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>408.749642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203.142105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.374160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>390.931189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206.616440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.512681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>445.433419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210.617889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.679458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>438.169429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>215.486450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE  \\\n",
       "3  13.517865       0.0  321.525047          0.0  182.732727      0.0   \n",
       "1  13.850535       0.0  372.648272          0.0  191.837311      0.0   \n",
       "5  13.959899       0.0  352.199345          0.0  194.878784      0.0   \n",
       "8  14.073073       0.0  370.466091          0.0  198.051361      0.0   \n",
       "9  14.126784       0.0  376.765614          0.0  199.566055      0.0   \n",
       "4  14.179949       0.0  409.939201          0.0  201.070984      0.0   \n",
       "7  14.252792       0.0  408.749642          0.0  203.142105      0.0   \n",
       "6  14.374160       0.0  390.931189          0.0  206.616440      0.0   \n",
       "2  14.512681       0.0  445.433419          0.0  210.617889      0.0   \n",
       "0  14.679458       0.0  438.169429          0.0  215.486450      0.0   \n",
       "\n",
       "        nodes  dropout activation batch_size  \n",
       "3   [256, 64]      0.2       tanh        128  \n",
       "1  [256, 128]      0.1       tanh         64  \n",
       "5  [256, 128]      0.1       tanh         64  \n",
       "8   [256, 64]      0.2       tanh        128  \n",
       "9  [256, 128]      0.1       tanh        128  \n",
       "4   [256, 64]      0.1       tanh        128  \n",
       "7  [256, 128]      0.1       tanh         64  \n",
       "6   [256, 64]      0.1       tanh         64  \n",
       "2  [256, 128]      0.1       tanh        128  \n",
       "0  [256, 128]      0.2       tanh        128  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results.sort_values(by='RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3e7d369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 14) (17731, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "278/278 [==============================] - 24s 75ms/step - loss: 2690.4429 - val_loss: 1729.3969\n",
      "Epoch 2/25\n",
      "278/278 [==============================] - 18s 63ms/step - loss: 1903.5499 - val_loss: 1675.2544\n",
      "Epoch 3/25\n",
      "278/278 [==============================] - 19s 67ms/step - loss: 1901.9313 - val_loss: 1721.7600\n",
      "Epoch 4/25\n",
      "278/278 [==============================] - 19s 70ms/step - loss: 1899.9883 - val_loss: 1684.6479\n",
      "Epoch 5/25\n",
      "278/278 [==============================] - 19s 67ms/step - loss: 961.8239 - val_loss: 292.1729\n",
      "Epoch 6/25\n",
      "278/278 [==============================] - 18s 65ms/step - loss: 283.0903 - val_loss: 509.9312\n",
      "Epoch 7/25\n",
      "278/278 [==============================] - 19s 70ms/step - loss: 266.8125 - val_loss: 192.6033\n",
      "Epoch 8/25\n",
      "278/278 [==============================] - 20s 72ms/step - loss: 231.3839 - val_loss: 198.8567\n",
      "Epoch 9/25\n",
      "278/278 [==============================] - 19s 67ms/step - loss: 226.8311 - val_loss: 188.8478\n",
      "Epoch 10/25\n",
      "278/278 [==============================] - 19s 67ms/step - loss: 221.3913 - val_loss: 332.4962\n",
      "Epoch 11/25\n",
      "278/278 [==============================] - 20s 71ms/step - loss: 216.4834 - val_loss: 244.8773\n",
      "Epoch 12/25\n",
      "278/278 [==============================] - 19s 70ms/step - loss: 212.5374 - val_loss: 197.5214\n",
      "Epoch 13/25\n",
      "278/278 [==============================] - 18s 65ms/step - loss: 214.0806 - val_loss: 194.8156\n",
      "Epoch 14/25\n",
      "278/278 [==============================] - 19s 67ms/step - loss: 211.0023 - val_loss: 290.2775\n",
      "Epoch 15/25\n",
      "278/278 [==============================] - 19s 69ms/step - loss: 226.8304 - val_loss: 209.9248\n",
      "Epoch 16/25\n",
      "278/278 [==============================] - 19s 67ms/step - loss: 207.6572 - val_loss: 214.6726\n",
      "Epoch 17/25\n",
      "278/278 [==============================] - 18s 66ms/step - loss: 202.2627 - val_loss: 227.2010\n",
      "Epoch 18/25\n",
      "278/278 [==============================] - 19s 69ms/step - loss: 208.7224 - val_loss: 190.1480\n",
      "Epoch 19/25\n",
      "278/278 [==============================] - 19s 67ms/step - loss: 201.6789 - val_loss: 204.5997\n",
      "Epoch 20/25\n",
      "278/278 [==============================] - 19s 67ms/step - loss: 200.9583 - val_loss: 198.8347\n",
      "Epoch 21/25\n",
      "278/278 [==============================] - 18s 66ms/step - loss: 199.1031 - val_loss: 180.1788\n",
      "Epoch 22/25\n",
      "278/278 [==============================] - 20s 71ms/step - loss: 201.9068 - val_loss: 203.7883\n",
      "Epoch 23/25\n",
      "278/278 [==============================] - 19s 70ms/step - loss: 198.1129 - val_loss: 193.3220\n",
      "Epoch 24/25\n",
      "278/278 [==============================] - 20s 70ms/step - loss: 199.9712 - val_loss: 225.6353\n",
      "Epoch 25/25\n",
      "278/278 [==============================] - 20s 70ms/step - loss: 195.8939 - val_loss: 210.4884\n",
      "4/4 [==============================] - 1s 12ms/step\n",
      "(17731, 30, 14) (17731, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "139/139 [==============================] - 15s 97ms/step - loss: 3601.6851 - val_loss: 1689.9918\n",
      "Epoch 2/25\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 1888.4612 - val_loss: 1702.3966\n",
      "Epoch 3/25\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 1890.7372 - val_loss: 1698.1403\n",
      "Epoch 4/25\n",
      "139/139 [==============================] - 14s 102ms/step - loss: 1890.9674 - val_loss: 1696.5856\n",
      "Epoch 5/25\n",
      "139/139 [==============================] - 14s 103ms/step - loss: 1888.0988 - val_loss: 1740.1177\n",
      "Epoch 6/25\n",
      "139/139 [==============================] - 14s 102ms/step - loss: 1888.8108 - val_loss: 1710.4717\n",
      "Epoch 7/25\n",
      "139/139 [==============================] - 15s 104ms/step - loss: 1889.9868 - val_loss: 1698.1012\n",
      "Epoch 8/25\n",
      "139/139 [==============================] - 16s 114ms/step - loss: 1889.9550 - val_loss: 1700.6584\n",
      "Epoch 9/25\n",
      "139/139 [==============================] - 15s 109ms/step - loss: 1887.4933 - val_loss: 1684.8400\n",
      "Epoch 10/25\n",
      "139/139 [==============================] - 14s 102ms/step - loss: 1116.5924 - val_loss: 277.6795\n",
      "Epoch 11/25\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 249.9355 - val_loss: 220.2587\n",
      "Epoch 12/25\n",
      "139/139 [==============================] - 14s 102ms/step - loss: 230.5617 - val_loss: 214.0982\n",
      "Epoch 13/25\n",
      "139/139 [==============================] - 14s 102ms/step - loss: 216.9230 - val_loss: 204.8592\n",
      "Epoch 14/25\n",
      "139/139 [==============================] - 16s 118ms/step - loss: 214.8647 - val_loss: 214.3024\n",
      "Epoch 15/25\n",
      "139/139 [==============================] - 14s 103ms/step - loss: 206.8662 - val_loss: 205.8671\n",
      "Epoch 16/25\n",
      "139/139 [==============================] - 15s 108ms/step - loss: 209.9567 - val_loss: 269.5760\n",
      "Epoch 17/25\n",
      "139/139 [==============================] - 14s 102ms/step - loss: 207.4892 - val_loss: 214.2120\n",
      "Epoch 18/25\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 199.1300 - val_loss: 236.4749\n",
      "Epoch 19/25\n",
      "139/139 [==============================] - 14s 103ms/step - loss: 204.8593 - val_loss: 189.6143\n",
      "Epoch 20/25\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 201.0523 - val_loss: 234.7952\n",
      "Epoch 21/25\n",
      "139/139 [==============================] - 14s 103ms/step - loss: 195.7657 - val_loss: 201.8968\n",
      "Epoch 22/25\n",
      "139/139 [==============================] - 15s 106ms/step - loss: 196.0074 - val_loss: 201.0137\n",
      "Epoch 23/25\n",
      "139/139 [==============================] - 14s 102ms/step - loss: 197.0721 - val_loss: 210.6167\n",
      "Epoch 24/25\n",
      "139/139 [==============================] - 14s 103ms/step - loss: 197.1031 - val_loss: 199.0923\n",
      "Epoch 25/25\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 188.0682 - val_loss: 217.1066\n",
      "4/4 [==============================] - 1s 10ms/step\n",
      "(17731, 30, 14) (17731, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "139/139 [==============================] - 18s 114ms/step - loss: 3556.9548 - val_loss: 1688.3701\n",
      "Epoch 2/25\n",
      "139/139 [==============================] - 16s 113ms/step - loss: 1902.4716 - val_loss: 1701.6709\n",
      "Epoch 3/25\n",
      "139/139 [==============================] - 16s 115ms/step - loss: 1895.3573 - val_loss: 1698.8988\n",
      "Epoch 4/25\n",
      "139/139 [==============================] - 16s 117ms/step - loss: 1896.5839 - val_loss: 1694.8306\n",
      "Epoch 5/25\n",
      "139/139 [==============================] - 18s 128ms/step - loss: 1897.9097 - val_loss: 1735.8584\n",
      "Epoch 6/25\n",
      "139/139 [==============================] - 17s 123ms/step - loss: 1897.5780 - val_loss: 1708.1919\n",
      "Epoch 7/25\n",
      "139/139 [==============================] - 16s 118ms/step - loss: 1898.6106 - val_loss: 1696.2280\n",
      "Epoch 8/25\n",
      "139/139 [==============================] - 17s 122ms/step - loss: 1893.4910 - val_loss: 1702.8036\n",
      "Epoch 9/25\n",
      "139/139 [==============================] - 17s 123ms/step - loss: 1897.9451 - val_loss: 1688.6184\n",
      "Epoch 10/25\n",
      "139/139 [==============================] - 18s 128ms/step - loss: 1899.9537 - val_loss: 1682.4681\n",
      "Epoch 11/25\n",
      "139/139 [==============================] - 20s 141ms/step - loss: 1897.8772 - val_loss: 1697.6725\n",
      "Epoch 12/25\n",
      "139/139 [==============================] - 21s 150ms/step - loss: 1900.3479 - val_loss: 1683.4178\n",
      "Epoch 13/25\n",
      "139/139 [==============================] - 18s 127ms/step - loss: 1883.6796 - val_loss: 857.0222\n",
      "Epoch 14/25\n",
      "139/139 [==============================] - 16s 113ms/step - loss: 458.2779 - val_loss: 321.0337\n",
      "Epoch 15/25\n",
      "139/139 [==============================] - 16s 114ms/step - loss: 258.6492 - val_loss: 246.3101\n",
      "Epoch 16/25\n",
      "139/139 [==============================] - 16s 112ms/step - loss: 255.0581 - val_loss: 218.8937\n",
      "Epoch 17/25\n",
      "139/139 [==============================] - 15s 110ms/step - loss: 242.4196 - val_loss: 195.7457\n",
      "Epoch 18/25\n",
      "139/139 [==============================] - 17s 124ms/step - loss: 243.5317 - val_loss: 217.2163\n",
      "Epoch 19/25\n",
      "139/139 [==============================] - 16s 115ms/step - loss: 234.3385 - val_loss: 222.0347\n",
      "Epoch 20/25\n",
      "139/139 [==============================] - 16s 115ms/step - loss: 224.2775 - val_loss: 178.3043\n",
      "Epoch 21/25\n",
      "139/139 [==============================] - 16s 112ms/step - loss: 226.6154 - val_loss: 256.9653\n",
      "Epoch 22/25\n",
      "139/139 [==============================] - 16s 113ms/step - loss: 218.1071 - val_loss: 195.2247\n",
      "Epoch 23/25\n",
      "139/139 [==============================] - 15s 109ms/step - loss: 219.5515 - val_loss: 185.6608\n",
      "Epoch 24/25\n",
      "139/139 [==============================] - 16s 112ms/step - loss: 230.0182 - val_loss: 204.8938\n",
      "Epoch 25/25\n",
      "139/139 [==============================] - 15s 111ms/step - loss: 212.1176 - val_loss: 246.8015\n",
      "4/4 [==============================] - 1s 12ms/step\n",
      "(17731, 30, 14) (17731, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "139/139 [==============================] - 17s 112ms/step - loss: 3572.2324 - val_loss: 1687.0431\n",
      "Epoch 2/25\n",
      "139/139 [==============================] - 16s 116ms/step - loss: 1901.7216 - val_loss: 1699.7371\n",
      "Epoch 3/25\n",
      "139/139 [==============================] - 18s 130ms/step - loss: 1900.0146 - val_loss: 1698.7634\n",
      "Epoch 4/25\n",
      "139/139 [==============================] - 16s 115ms/step - loss: 1899.7830 - val_loss: 1696.8910\n",
      "Epoch 5/25\n",
      "139/139 [==============================] - 16s 112ms/step - loss: 1899.0112 - val_loss: 1735.0060\n",
      "Epoch 6/25\n",
      "139/139 [==============================] - 16s 115ms/step - loss: 1897.9478 - val_loss: 1706.7292\n",
      "Epoch 7/25\n",
      "139/139 [==============================] - 16s 116ms/step - loss: 1897.3192 - val_loss: 1694.9653\n",
      "Epoch 8/25\n",
      "139/139 [==============================] - 16s 118ms/step - loss: 1898.2483 - val_loss: 1699.7839\n",
      "Epoch 9/25\n",
      "139/139 [==============================] - 16s 112ms/step - loss: 1892.9863 - val_loss: 1689.9565\n",
      "Epoch 10/25\n",
      "139/139 [==============================] - 16s 112ms/step - loss: 1897.9012 - val_loss: 1683.4385\n",
      "Epoch 11/25\n",
      "139/139 [==============================] - 16s 113ms/step - loss: 1893.2452 - val_loss: 1697.5029\n",
      "Epoch 12/25\n",
      "139/139 [==============================] - 15s 109ms/step - loss: 1903.3336 - val_loss: 1683.9808\n",
      "Epoch 13/25\n",
      "139/139 [==============================] - 15s 109ms/step - loss: 1900.7935 - val_loss: 1678.1589\n",
      "Epoch 14/25\n",
      "139/139 [==============================] - 16s 116ms/step - loss: 1902.7074 - val_loss: 1701.9775\n",
      "Epoch 15/25\n",
      "139/139 [==============================] - 15s 107ms/step - loss: 1896.6509 - val_loss: 1691.5769\n",
      "Epoch 16/25\n",
      "139/139 [==============================] - 16s 116ms/step - loss: 1896.4104 - val_loss: 1688.2856\n",
      "Epoch 17/25\n",
      "139/139 [==============================] - 15s 110ms/step - loss: 1897.6176 - val_loss: 1743.6953\n",
      "Epoch 18/25\n",
      "139/139 [==============================] - 15s 111ms/step - loss: 830.3768 - val_loss: 416.7798\n",
      "Epoch 19/25\n",
      "139/139 [==============================] - 17s 126ms/step - loss: 293.9514 - val_loss: 251.6759\n",
      "Epoch 20/25\n",
      "139/139 [==============================] - 18s 127ms/step - loss: 248.4499 - val_loss: 190.6898\n",
      "Epoch 21/25\n",
      "139/139 [==============================] - 15s 109ms/step - loss: 232.5173 - val_loss: 245.1936\n",
      "Epoch 22/25\n",
      "139/139 [==============================] - 15s 107ms/step - loss: 220.5667 - val_loss: 205.5751\n",
      "Epoch 23/25\n",
      "139/139 [==============================] - 16s 112ms/step - loss: 216.9050 - val_loss: 202.1142\n",
      "Epoch 24/25\n",
      "139/139 [==============================] - 15s 107ms/step - loss: 226.0757 - val_loss: 193.9154\n",
      "Epoch 25/25\n",
      "139/139 [==============================] - 15s 108ms/step - loss: 212.9366 - val_loss: 240.0735\n",
      "4/4 [==============================] - 1s 13ms/step\n",
      "(17731, 30, 14) (17731, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "139/139 [==============================] - 18s 113ms/step - loss: 3437.5105 - val_loss: 1692.4163\n",
      "Epoch 2/25\n",
      "139/139 [==============================] - 15s 109ms/step - loss: 1890.9606 - val_loss: 1702.4893\n",
      "Epoch 3/25\n",
      "139/139 [==============================] - 15s 110ms/step - loss: 1888.1899 - val_loss: 1699.5708\n",
      "Epoch 4/25\n",
      "139/139 [==============================] - 15s 106ms/step - loss: 1886.9701 - val_loss: 1694.8738\n",
      "Epoch 5/25\n",
      "139/139 [==============================] - 14s 102ms/step - loss: 1886.9321 - val_loss: 1741.9268\n",
      "Epoch 6/25\n",
      "139/139 [==============================] - 16s 114ms/step - loss: 1887.7721 - val_loss: 1709.5236\n",
      "Epoch 7/25\n",
      "139/139 [==============================] - 15s 110ms/step - loss: 1886.4266 - val_loss: 1698.3726\n",
      "Epoch 8/25\n",
      "139/139 [==============================] - 17s 122ms/step - loss: 1886.7714 - val_loss: 1704.7720\n",
      "Epoch 9/25\n",
      "139/139 [==============================] - 15s 110ms/step - loss: 1889.3082 - val_loss: 1692.3337\n",
      "Epoch 10/25\n",
      "139/139 [==============================] - 16s 116ms/step - loss: 1888.8480 - val_loss: 1685.7510\n",
      "Epoch 11/25\n",
      "139/139 [==============================] - 16s 112ms/step - loss: 1885.0333 - val_loss: 1706.6266\n",
      "Epoch 12/25\n",
      "139/139 [==============================] - 16s 112ms/step - loss: 1888.5342 - val_loss: 1685.8302\n",
      "Epoch 13/25\n",
      "139/139 [==============================] - 15s 110ms/step - loss: 1891.2960 - val_loss: 1686.5928\n",
      "Epoch 14/25\n",
      "139/139 [==============================] - 16s 112ms/step - loss: 1887.6620 - val_loss: 1713.3488\n",
      "Epoch 15/25\n",
      "139/139 [==============================] - 17s 122ms/step - loss: 1889.1095 - val_loss: 1691.2126\n",
      "Epoch 16/25\n",
      "139/139 [==============================] - 16s 114ms/step - loss: 1885.6196 - val_loss: 1690.5850\n",
      "Epoch 17/25\n",
      "139/139 [==============================] - 15s 111ms/step - loss: 1883.8695 - val_loss: 1605.4720\n",
      "Epoch 18/25\n",
      "139/139 [==============================] - 15s 106ms/step - loss: 660.6120 - val_loss: 321.7648\n",
      "Epoch 19/25\n",
      "139/139 [==============================] - 15s 109ms/step - loss: 337.3957 - val_loss: 297.7147\n",
      "Epoch 20/25\n",
      "139/139 [==============================] - 16s 113ms/step - loss: 275.8853 - val_loss: 281.2716\n",
      "Epoch 21/25\n",
      "139/139 [==============================] - 15s 112ms/step - loss: 252.1397 - val_loss: 303.8395\n",
      "Epoch 22/25\n",
      "139/139 [==============================] - 16s 114ms/step - loss: 236.3470 - val_loss: 260.3456\n",
      "Epoch 23/25\n",
      "139/139 [==============================] - 16s 113ms/step - loss: 225.2024 - val_loss: 208.7315\n",
      "Epoch 24/25\n",
      "139/139 [==============================] - 16s 116ms/step - loss: 228.4519 - val_loss: 200.4440\n",
      "Epoch 25/25\n",
      "139/139 [==============================] - 16s 114ms/step - loss: 215.3980 - val_loss: 278.9226\n",
      "4/4 [==============================] - 1s 24ms/step\n",
      "(17731, 30, 14) (17731, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "278/278 [==============================] - 20s 61ms/step - loss: 2780.5251 - val_loss: 1724.0385\n",
      "Epoch 2/25\n",
      "278/278 [==============================] - 17s 60ms/step - loss: 1903.0487 - val_loss: 1683.0133\n",
      "Epoch 3/25\n",
      "278/278 [==============================] - 17s 60ms/step - loss: 1897.7996 - val_loss: 1719.5571\n",
      "Epoch 4/25\n",
      "278/278 [==============================] - 17s 60ms/step - loss: 1901.9365 - val_loss: 1686.9113\n",
      "Epoch 5/25\n",
      "278/278 [==============================] - 19s 69ms/step - loss: 1904.5299 - val_loss: 1720.5931\n",
      "Epoch 6/25\n",
      "278/278 [==============================] - 17s 61ms/step - loss: 1709.6953 - val_loss: 503.0101\n",
      "Epoch 7/25\n",
      "278/278 [==============================] - 17s 60ms/step - loss: 472.7697 - val_loss: 539.2539\n",
      "Epoch 8/25\n",
      "278/278 [==============================] - 16s 59ms/step - loss: 372.0891 - val_loss: 443.2651\n",
      "Epoch 9/25\n",
      "278/278 [==============================] - 17s 63ms/step - loss: 330.3698 - val_loss: 315.7203\n",
      "Epoch 10/25\n",
      "278/278 [==============================] - 17s 62ms/step - loss: 303.2206 - val_loss: 335.2723\n",
      "Epoch 11/25\n",
      "278/278 [==============================] - 16s 59ms/step - loss: 261.0672 - val_loss: 286.4366\n",
      "Epoch 12/25\n",
      "278/278 [==============================] - 17s 60ms/step - loss: 254.3435 - val_loss: 211.4818\n",
      "Epoch 13/25\n",
      "278/278 [==============================] - 16s 59ms/step - loss: 239.5260 - val_loss: 226.5381\n",
      "Epoch 14/25\n",
      "278/278 [==============================] - 17s 59ms/step - loss: 228.3246 - val_loss: 476.8562\n",
      "Epoch 15/25\n",
      "278/278 [==============================] - 18s 64ms/step - loss: 249.6911 - val_loss: 226.6387\n",
      "Epoch 16/25\n",
      "278/278 [==============================] - 17s 60ms/step - loss: 222.3449 - val_loss: 241.4429\n",
      "Epoch 17/25\n",
      "278/278 [==============================] - 16s 59ms/step - loss: 218.1719 - val_loss: 225.3185\n",
      "Epoch 18/25\n",
      "278/278 [==============================] - 17s 60ms/step - loss: 221.0036 - val_loss: 229.9953\n",
      "Epoch 19/25\n",
      "278/278 [==============================] - 16s 59ms/step - loss: 214.7393 - val_loss: 220.5208\n",
      "Epoch 20/25\n",
      "278/278 [==============================] - 17s 61ms/step - loss: 215.4310 - val_loss: 209.2600\n",
      "Epoch 21/25\n",
      "278/278 [==============================] - 16s 59ms/step - loss: 209.9305 - val_loss: 207.9308\n",
      "Epoch 22/25\n",
      "278/278 [==============================] - 17s 62ms/step - loss: 208.5101 - val_loss: 216.6000\n",
      "Epoch 23/25\n",
      "278/278 [==============================] - 16s 59ms/step - loss: 207.6503 - val_loss: 207.0152\n",
      "Epoch 24/25\n",
      "278/278 [==============================] - 17s 60ms/step - loss: 207.9895 - val_loss: 219.6506\n",
      "Epoch 25/25\n",
      "278/278 [==============================] - 17s 63ms/step - loss: 201.6824 - val_loss: 249.6727\n",
      "4/4 [==============================] - 1s 14ms/step\n",
      "(17731, 30, 14) (17731, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "139/139 [==============================] - 17s 104ms/step - loss: 3520.8635 - val_loss: 1690.7842\n",
      "Epoch 2/25\n",
      "139/139 [==============================] - 16s 113ms/step - loss: 1901.0845 - val_loss: 1699.4604\n",
      "Epoch 3/25\n",
      "139/139 [==============================] - 15s 110ms/step - loss: 1899.5140 - val_loss: 1700.3466\n",
      "Epoch 4/25\n",
      "139/139 [==============================] - 17s 121ms/step - loss: 1901.6998 - val_loss: 1694.6731\n",
      "Epoch 5/25\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 1899.8582 - val_loss: 1736.9354\n",
      "Epoch 6/25\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 1897.2316 - val_loss: 1708.1166\n",
      "Epoch 7/25\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 1897.3146 - val_loss: 1695.7720\n",
      "Epoch 8/25\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 1893.8671 - val_loss: 1703.0398\n",
      "Epoch 9/25\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 1897.8318 - val_loss: 1689.7092\n",
      "Epoch 10/25\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 1901.0364 - val_loss: 1673.9059\n",
      "Epoch 11/25\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 730.4922 - val_loss: 293.3114\n",
      "Epoch 12/25\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 267.0209 - val_loss: 246.1025\n",
      "Epoch 13/25\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 244.4056 - val_loss: 217.2726\n",
      "Epoch 14/25\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 231.2144 - val_loss: 211.1154\n",
      "Epoch 15/25\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 226.1053 - val_loss: 214.0362\n",
      "Epoch 16/25\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 222.7538 - val_loss: 244.5011\n",
      "Epoch 17/25\n",
      "139/139 [==============================] - 14s 104ms/step - loss: 220.3451 - val_loss: 217.0901\n",
      "Epoch 18/25\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 212.2148 - val_loss: 227.8996\n",
      "Epoch 19/25\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 218.0028 - val_loss: 198.0067\n",
      "Epoch 20/25\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 213.3689 - val_loss: 204.9386\n",
      "Epoch 21/25\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 207.8876 - val_loss: 199.7755\n",
      "Epoch 22/25\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 208.1388 - val_loss: 205.5195\n",
      "Epoch 23/25\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 208.4865 - val_loss: 212.0616\n",
      "Epoch 24/25\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 201.6687 - val_loss: 205.9969\n",
      "Epoch 25/25\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 200.6255 - val_loss: 219.4441\n",
      "4/4 [==============================] - 1s 10ms/step\n",
      "(17731, 30, 14) (17731, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "278/278 [==============================] - 24s 79ms/step - loss: 2761.9768 - val_loss: 1726.5463\n",
      "Epoch 2/25\n",
      "278/278 [==============================] - 20s 73ms/step - loss: 1900.7686 - val_loss: 1680.7725\n",
      "Epoch 3/25\n",
      "278/278 [==============================] - 22s 78ms/step - loss: 1897.8982 - val_loss: 1724.3566\n",
      "Epoch 4/25\n",
      "278/278 [==============================] - 23s 84ms/step - loss: 1904.9044 - val_loss: 1694.2896\n",
      "Epoch 5/25\n",
      "278/278 [==============================] - 21s 77ms/step - loss: 1897.4426 - val_loss: 1717.5100\n",
      "Epoch 6/25\n",
      "278/278 [==============================] - 22s 78ms/step - loss: 1901.0190 - val_loss: 1684.2433\n",
      "Epoch 7/25\n",
      "278/278 [==============================] - 23s 83ms/step - loss: 1903.8167 - val_loss: 1707.5709\n",
      "Epoch 8/25\n",
      "278/278 [==============================] - 21s 77ms/step - loss: 838.5738 - val_loss: 292.8845\n",
      "Epoch 9/25\n",
      "278/278 [==============================] - 22s 80ms/step - loss: 281.6032 - val_loss: 213.9162\n",
      "Epoch 10/25\n",
      "278/278 [==============================] - 21s 76ms/step - loss: 248.3526 - val_loss: 315.2832\n",
      "Epoch 11/25\n",
      "278/278 [==============================] - 21s 77ms/step - loss: 234.3014 - val_loss: 245.7758\n",
      "Epoch 12/25\n",
      "278/278 [==============================] - 22s 78ms/step - loss: 223.9800 - val_loss: 218.2029\n",
      "Epoch 13/25\n",
      "278/278 [==============================] - 23s 82ms/step - loss: 220.4536 - val_loss: 203.7051\n",
      "Epoch 14/25\n",
      "278/278 [==============================] - 21s 77ms/step - loss: 217.0343 - val_loss: 234.5181\n",
      "Epoch 15/25\n",
      "278/278 [==============================] - 22s 77ms/step - loss: 214.9709 - val_loss: 199.4505\n",
      "Epoch 16/25\n",
      "278/278 [==============================] - 22s 78ms/step - loss: 213.3062 - val_loss: 227.1934\n",
      "Epoch 17/25\n",
      "278/278 [==============================] - 22s 81ms/step - loss: 206.4890 - val_loss: 208.3196\n",
      "Epoch 18/25\n",
      "278/278 [==============================] - 22s 78ms/step - loss: 206.7055 - val_loss: 208.3480\n",
      "Epoch 19/25\n",
      "278/278 [==============================] - 21s 77ms/step - loss: 203.0393 - val_loss: 209.0699\n",
      "Epoch 20/25\n",
      "278/278 [==============================] - 22s 78ms/step - loss: 206.8233 - val_loss: 183.7852\n",
      "Epoch 21/25\n",
      "278/278 [==============================] - 22s 78ms/step - loss: 200.2490 - val_loss: 192.6158\n",
      "Epoch 22/25\n",
      "278/278 [==============================] - 22s 78ms/step - loss: 201.7123 - val_loss: 197.9610\n",
      "Epoch 23/25\n",
      "278/278 [==============================] - 24s 86ms/step - loss: 201.0449 - val_loss: 194.5195\n",
      "Epoch 24/25\n",
      "278/278 [==============================] - 21s 77ms/step - loss: 201.4758 - val_loss: 217.2913\n",
      "Epoch 25/25\n",
      "278/278 [==============================] - 21s 75ms/step - loss: 196.7392 - val_loss: 217.0220\n",
      "4/4 [==============================] - 1s 13ms/step\n",
      "(17731, 30, 14) (17731, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "139/139 [==============================] - 19s 114ms/step - loss: 3408.7903 - val_loss: 1690.5463\n",
      "Epoch 2/25\n",
      "139/139 [==============================] - 16s 112ms/step - loss: 1887.9912 - val_loss: 1703.3402\n",
      "Epoch 3/25\n",
      "139/139 [==============================] - 15s 111ms/step - loss: 1886.4540 - val_loss: 1700.5328\n",
      "Epoch 4/25\n",
      "139/139 [==============================] - 15s 107ms/step - loss: 1887.4614 - val_loss: 1695.8120\n",
      "Epoch 5/25\n",
      "139/139 [==============================] - 16s 113ms/step - loss: 1887.2827 - val_loss: 1740.3093\n",
      "Epoch 6/25\n",
      "139/139 [==============================] - 15s 108ms/step - loss: 1886.8337 - val_loss: 1711.7546\n",
      "Epoch 7/25\n",
      "139/139 [==============================] - 15s 111ms/step - loss: 1888.3451 - val_loss: 1697.7177\n",
      "Epoch 8/25\n",
      "139/139 [==============================] - 15s 111ms/step - loss: 1886.2804 - val_loss: 1702.6204\n",
      "Epoch 9/25\n",
      "139/139 [==============================] - 15s 111ms/step - loss: 1654.1083 - val_loss: 518.9287\n",
      "Epoch 10/25\n",
      "139/139 [==============================] - 15s 110ms/step - loss: 325.5999 - val_loss: 224.1154\n",
      "Epoch 11/25\n",
      "139/139 [==============================] - 15s 111ms/step - loss: 237.3656 - val_loss: 210.2188\n",
      "Epoch 12/25\n",
      "139/139 [==============================] - 17s 122ms/step - loss: 223.9408 - val_loss: 256.3563\n",
      "Epoch 13/25\n",
      "139/139 [==============================] - 15s 111ms/step - loss: 223.5799 - val_loss: 193.1943\n",
      "Epoch 14/25\n",
      "139/139 [==============================] - 16s 116ms/step - loss: 210.7991 - val_loss: 216.4817\n",
      "Epoch 15/25\n",
      "139/139 [==============================] - 15s 108ms/step - loss: 207.1790 - val_loss: 210.7747\n",
      "Epoch 16/25\n",
      "139/139 [==============================] - 16s 115ms/step - loss: 201.6057 - val_loss: 214.0724\n",
      "Epoch 17/25\n",
      "139/139 [==============================] - 16s 117ms/step - loss: 210.2609 - val_loss: 215.9834\n",
      "Epoch 18/25\n",
      "139/139 [==============================] - 17s 119ms/step - loss: 199.5271 - val_loss: 212.9648\n",
      "Epoch 19/25\n",
      "139/139 [==============================] - 16s 114ms/step - loss: 200.5445 - val_loss: 197.9768\n",
      "Epoch 20/25\n",
      "139/139 [==============================] - 16s 116ms/step - loss: 196.0285 - val_loss: 190.6624\n",
      "Epoch 21/25\n",
      "139/139 [==============================] - 16s 113ms/step - loss: 189.2042 - val_loss: 233.6214\n",
      "Epoch 22/25\n",
      "139/139 [==============================] - 16s 116ms/step - loss: 189.3037 - val_loss: 197.8049\n",
      "Epoch 23/25\n",
      "139/139 [==============================] - 15s 108ms/step - loss: 190.7787 - val_loss: 198.1110\n",
      "Epoch 24/25\n",
      "139/139 [==============================] - 16s 113ms/step - loss: 194.8721 - val_loss: 195.2283\n",
      "Epoch 25/25\n",
      "139/139 [==============================] - 16s 112ms/step - loss: 183.7121 - val_loss: 244.3019\n",
      "4/4 [==============================] - 1s 12ms/step\n",
      "iteration  10\n",
      "(17731, 30, 14) (17731, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "278/278 [==============================] - 23s 75ms/step - loss: 2609.9441 - val_loss: 1735.0396\n",
      "Epoch 2/25\n",
      "278/278 [==============================] - 21s 77ms/step - loss: 1897.3856 - val_loss: 1675.6466\n",
      "Epoch 3/25\n",
      "278/278 [==============================] - 21s 76ms/step - loss: 1895.9709 - val_loss: 1727.0853\n",
      "Epoch 4/25\n",
      "278/278 [==============================] - 23s 83ms/step - loss: 1895.2690 - val_loss: 1692.5082\n",
      "Epoch 5/25\n",
      "278/278 [==============================] - 22s 79ms/step - loss: 1899.1987 - val_loss: 1719.3745\n",
      "Epoch 6/25\n",
      "278/278 [==============================] - 22s 78ms/step - loss: 1377.4027 - val_loss: 678.6583\n",
      "Epoch 7/25\n",
      "278/278 [==============================] - 25s 90ms/step - loss: 297.3250 - val_loss: 236.7496\n",
      "Epoch 8/25\n",
      "278/278 [==============================] - 23s 83ms/step - loss: 236.2816 - val_loss: 206.6786\n",
      "Epoch 9/25\n",
      "278/278 [==============================] - 22s 79ms/step - loss: 221.9722 - val_loss: 227.4727\n",
      "Epoch 10/25\n",
      "278/278 [==============================] - 22s 78ms/step - loss: 218.2705 - val_loss: 302.2665\n",
      "Epoch 11/25\n",
      "278/278 [==============================] - 22s 79ms/step - loss: 209.5662 - val_loss: 240.3548\n",
      "Epoch 12/25\n",
      "278/278 [==============================] - 22s 80ms/step - loss: 208.2961 - val_loss: 193.6341\n",
      "Epoch 13/25\n",
      "278/278 [==============================] - 23s 84ms/step - loss: 204.9837 - val_loss: 191.3545\n",
      "Epoch 14/25\n",
      "278/278 [==============================] - 23s 81ms/step - loss: 205.4433 - val_loss: 206.3142\n",
      "Epoch 15/25\n",
      "278/278 [==============================] - 22s 79ms/step - loss: 201.7779 - val_loss: 197.1071\n",
      "Epoch 16/25\n",
      "278/278 [==============================] - 22s 79ms/step - loss: 200.4969 - val_loss: 246.7916\n",
      "Epoch 17/25\n",
      "278/278 [==============================] - 22s 79ms/step - loss: 198.2028 - val_loss: 217.3869\n",
      "Epoch 18/25\n",
      "278/278 [==============================] - 22s 78ms/step - loss: 202.7002 - val_loss: 193.2982\n",
      "Epoch 19/25\n",
      "278/278 [==============================] - 22s 79ms/step - loss: 196.0443 - val_loss: 208.0638\n",
      "Epoch 20/25\n",
      "278/278 [==============================] - 22s 78ms/step - loss: 196.5484 - val_loss: 179.8118\n",
      "Epoch 21/25\n",
      "278/278 [==============================] - 22s 78ms/step - loss: 192.3078 - val_loss: 192.1570\n",
      "Epoch 22/25\n",
      "278/278 [==============================] - 21s 76ms/step - loss: 196.8681 - val_loss: 208.7489\n",
      "Epoch 23/25\n",
      "278/278 [==============================] - 21s 76ms/step - loss: 194.8003 - val_loss: 196.2278\n",
      "Epoch 24/25\n",
      "278/278 [==============================] - 22s 79ms/step - loss: 196.4562 - val_loss: 225.1941\n",
      "Epoch 25/25\n",
      "278/278 [==============================] - 25s 91ms/step - loss: 191.0006 - val_loss: 211.3163\n",
      "4/4 [==============================] - 1s 13ms/step\n",
      "CPU times: user 2h 2min 24s, sys: 34min 47s, total: 2h 37min 12s\n",
      "Wall time: 1h 12min 11s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# results = pd.DataFrame(columns=['RMSE', 'std_RMSE', \n",
    "#                                 'S_score','std_S_score',\n",
    "#                                 'MSE', 'std_MSE',\n",
    "#                                 'nodes', 'dropout',\n",
    "#                                 'activation', 'batch_size'])\n",
    "\n",
    "\n",
    "# for i in range(ITERATIONS):\n",
    "#     if ITERATIONS < 10:\n",
    "#         print('iteration ', i+1)\n",
    "#     elif ((i+1) % 10 == 0):\n",
    "#         print('iteration ', i+1)    \n",
    "#     tf.random.set_seed(SEED)\n",
    "#     mse = []\n",
    "#     R2_val = []\n",
    "#     RMSE = []\n",
    "#     score_val = []\n",
    "    \n",
    "    \n",
    "# # parameter's sample\n",
    "#     alpha = 0.3\n",
    "#     sequence_length = 30\n",
    "#     epochs = 25\n",
    "#     nodes_per_layer = random.sample(nodes_list, 1)[0]\n",
    "#     dropout = random.sample(dropouts, 1)[0]\n",
    "#     activation = random.sample(activation_functions, 1)[0]\n",
    "#     batch_size = random.sample(batch_size_list, 1)[0]\n",
    "#     remaining_sensors = remaining_sensors\n",
    "#     drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "#     # create model\n",
    "#     input_shape = (sequence_length, len(remaining_sensors))\n",
    "#     model = create_model3C(input_shape, nodes_per_layer, dropout, activation)\n",
    "    \n",
    "#     # Data prepration\n",
    "#     X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "#     # create sequences train, test\n",
    "#     train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "#     label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "#     test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "#                for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "#     test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "#     test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "#     print(train_array.shape, label_array.shape, test_array.shape)\n",
    "\n",
    "#     with tf.device('/device:GPU:0'):\n",
    "#         history = model.fit(train_array, label_array,\n",
    "#                                 validation_data=(test_array, test_rul),\n",
    "#                                 epochs=epochs,\n",
    "#                                 batch_size=batch_size,\n",
    "#                                 # callbacks=[cb],\n",
    "#                                 verbose=1)\n",
    "#         mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "#         y_hat_val_split = model.predict(test_array)\n",
    "#         R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "#         RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "#         score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "#     #       append results\n",
    "#     d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "#          'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "#          'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "#          'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "#          'activation':activation, 'batch_size':batch_size}\n",
    "\n",
    "# #     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "#     results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3f51ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.508221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>449.657836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210.488434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.536720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>452.679087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211.316254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.731667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>481.297785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>217.022034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.734535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>446.855999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>217.106567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.813646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>457.662222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>219.444122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.494307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>543.409197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>240.073517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15.630160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>561.202318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>244.301941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.709916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>616.340811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>246.801468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.801034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>575.720540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249.672653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.700977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>716.102582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>278.922607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE  \\\n",
       "0  14.508221       0.0  449.657836          0.0  210.488434      0.0   \n",
       "9  14.536720       0.0  452.679087          0.0  211.316254      0.0   \n",
       "7  14.731667       0.0  481.297785          0.0  217.022034      0.0   \n",
       "1  14.734535       0.0  446.855999          0.0  217.106567      0.0   \n",
       "6  14.813646       0.0  457.662222          0.0  219.444122      0.0   \n",
       "3  15.494307       0.0  543.409197          0.0  240.073517      0.0   \n",
       "8  15.630160       0.0  561.202318          0.0  244.301941      0.0   \n",
       "2  15.709916       0.0  616.340811          0.0  246.801468      0.0   \n",
       "5  15.801034       0.0  575.720540          0.0  249.672653      0.0   \n",
       "4  16.700977       0.0  716.102582          0.0  278.922607      0.0   \n",
       "\n",
       "       nodes  dropout activation batch_size  \n",
       "0  [256, 32]      0.2       tanh         64  \n",
       "9  [256, 64]      0.2       tanh         64  \n",
       "7  [256, 64]      0.2       tanh         64  \n",
       "1  [256, 32]      0.1       tanh        128  \n",
       "6  [256, 32]      0.2       tanh        128  \n",
       "3  [256, 64]      0.2       tanh        128  \n",
       "8  [256, 64]      0.1       tanh        128  \n",
       "2  [256, 64]      0.2       tanh        128  \n",
       "5  [256, 32]      0.2       tanh         64  \n",
       "4  [256, 64]      0.1       tanh        128  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results.sort_values(by='RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fac9455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1\n",
      "(17731, 30, 14) (17731, 1) (100, 30, 14)\n",
      "Epoch 1/25\n",
      "278/278 [==============================] - 24s 74ms/step - loss: 2546.8135 - val_loss: 1746.3311\n",
      "Epoch 2/25\n",
      "278/278 [==============================] - 20s 72ms/step - loss: 1896.3131 - val_loss: 1655.9519\n",
      "Epoch 3/25\n",
      "278/278 [==============================] - 20s 72ms/step - loss: 1894.5208 - val_loss: 1721.5046\n",
      "Epoch 4/25\n",
      "278/278 [==============================] - 20s 71ms/step - loss: 1894.0259 - val_loss: 1691.7137\n",
      "Epoch 5/25\n",
      "278/278 [==============================] - 20s 71ms/step - loss: 1891.7537 - val_loss: 1689.7113\n",
      "Epoch 6/25\n",
      "278/278 [==============================] - 20s 71ms/step - loss: 1900.5850 - val_loss: 1728.5280\n",
      "Epoch 7/25\n",
      "278/278 [==============================] - 20s 72ms/step - loss: 1897.0848 - val_loss: 1728.1847\n",
      "Epoch 8/25\n",
      "278/278 [==============================] - 21s 75ms/step - loss: 1896.4448 - val_loss: 1672.9656\n",
      "Epoch 9/25\n",
      "278/278 [==============================] - 20s 73ms/step - loss: 1898.2627 - val_loss: 1842.1548\n",
      "Epoch 10/25\n",
      "278/278 [==============================] - 20s 71ms/step - loss: 1638.3572 - val_loss: 520.6451\n",
      "Epoch 11/25\n",
      "278/278 [==============================] - 19s 70ms/step - loss: 338.3697 - val_loss: 245.5044\n",
      "Epoch 12/25\n",
      "278/278 [==============================] - 20s 73ms/step - loss: 240.5029 - val_loss: 196.1694\n",
      "Epoch 13/25\n",
      "278/278 [==============================] - 21s 74ms/step - loss: 214.4235 - val_loss: 224.1985\n",
      "Epoch 14/25\n",
      "278/278 [==============================] - 19s 70ms/step - loss: 209.2420 - val_loss: 469.7841\n",
      "Epoch 15/25\n",
      "278/278 [==============================] - 20s 70ms/step - loss: 232.1940 - val_loss: 193.5816\n",
      "Epoch 16/25\n",
      "278/278 [==============================] - 19s 69ms/step - loss: 196.5072 - val_loss: 307.7284\n",
      "Epoch 17/25\n",
      "278/278 [==============================] - 20s 70ms/step - loss: 202.2515 - val_loss: 215.4870\n",
      "Epoch 18/25\n",
      "278/278 [==============================] - 22s 78ms/step - loss: 203.8967 - val_loss: 263.7532\n",
      "Epoch 19/25\n",
      "278/278 [==============================] - 20s 73ms/step - loss: 202.7266 - val_loss: 189.0265\n",
      "Epoch 20/25\n",
      "278/278 [==============================] - 22s 80ms/step - loss: 196.6507 - val_loss: 202.3183\n",
      "Epoch 21/25\n",
      "278/278 [==============================] - 20s 72ms/step - loss: 196.4412 - val_loss: 186.7952\n",
      "Epoch 22/25\n",
      "278/278 [==============================] - 20s 72ms/step - loss: 196.9104 - val_loss: 205.2325\n",
      "Epoch 23/25\n",
      "278/278 [==============================] - 20s 72ms/step - loss: 194.8800 - val_loss: 189.6691\n",
      "Epoch 24/25\n",
      "278/278 [==============================] - 20s 71ms/step - loss: 191.8551 - val_loss: 193.6998\n",
      "Epoch 25/25\n",
      "278/278 [==============================] - 20s 73ms/step - loss: 189.6472 - val_loss: 193.0023\n",
      "4/4 [==============================] - 3s 32ms/step\n",
      "CPU times: user 13min 31s, sys: 2min 46s, total: 16min 18s\n",
      "Wall time: 8min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = pd.DataFrame(columns=['RMSE', 'std_RMSE', \n",
    "                                'S_score','std_S_score',\n",
    "                                'MSE', 'std_MSE',\n",
    "                                'nodes', 'dropout',\n",
    "                                'activation', 'batch_size'])\n",
    "\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    if ITERATIONS < 10:\n",
    "        print('iteration ', i+1)\n",
    "    elif ((i+1) % 10 == 0):\n",
    "        print('iteration ', i+1)    \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    \n",
    "# parameter's sample\n",
    "    alpha = 0.3\n",
    "    sequence_length = 30\n",
    "    epochs = 25\n",
    "    nodes_per_layer = random.sample(nodes_list, 1)[0]\n",
    "    dropout = random.sample(dropouts, 1)[0]\n",
    "    activation = random.sample(activation_functions, 1)[0]\n",
    "    batch_size = 64\n",
    "    remaining_sensors = remaining_sensors\n",
    "    drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = create_model4C(input_shape, nodes_per_layer, dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #       append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe0ef42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.892528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>373.299821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193.00235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score        MSE  std_MSE  \\\n",
       "0  13.892528       0.0  373.299821          0.0  193.00235      0.0   \n",
       "\n",
       "       nodes  dropout activation batch_size  \n",
       "0  [256, 32]      0.1       tanh         64  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "030240c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\env_test\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\env_test\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "iteration  10\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 947us/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "iteration  20\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "iteration  30\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "iteration  40\n",
      "4/4 [==============================] - 1s 7ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "iteration  50\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "CPU times: total: 19min 26s\n",
      "Wall time: 1h 29min 59s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# results = pd.DataFrame(columns=['RMSE', 'std_RMSE', \n",
    "#                                 'S_score','std_S_score',\n",
    "#                                 'MSE', 'std_MSE',\n",
    "#                                 'nodes', 'dropout',\n",
    "#                                 'activation', 'batch_size'])\n",
    "\n",
    "\n",
    "# for i in range(ITERATIONS):\n",
    "#     if ITERATIONS < 10:\n",
    "#         print('iteration ', i+1)\n",
    "#     elif ((i+1) % 10 == 0):\n",
    "#         print('iteration ', i+1)\n",
    "    \n",
    "#     tf.random.set_seed(SEED)\n",
    "#     mse = []\n",
    "#     R2_val = []\n",
    "#     RMSE = []\n",
    "#     score_val = []\n",
    "    \n",
    "    \n",
    "#     # init parameters\n",
    "#     # Random search\n",
    "#     # Number of layers = 1\n",
    "#     # alpha = random.sample(alpha_list, 1)[0]\n",
    "#     alpha = 0.3\n",
    "#     # sequence_length = random.sample(sequence_list, 1)[0]\n",
    "#     sequence_length = 30\n",
    "#     epochs = 25\n",
    "# #     epochs = random.sample(epoch_list, 1)[0]\n",
    "#     nodes_per_layer = random.sample(nodes_list, 1)[0]\n",
    "#     dropout = random.sample(dropouts, 1)[0]\n",
    "#     activation = random.sample(activation_functions, 1)[0]\n",
    "#     batch_size = random.sample(batch_size_list, 1)[0]\n",
    "#     remaining_sensors = remaining_sensors\n",
    "#     drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "    \n",
    "#     # create model\n",
    "#     input_shape = (sequence_length, len(remaining_sensors))\n",
    "#     model = create_model(input_shape, nodes_per_layer, dropout, activation)\n",
    "    \n",
    "#     # create train-val split\n",
    "#     X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "#     test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "#            for unit_nr in X_test_interim['Unit'].unique())\n",
    "#     test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "        \n",
    "#     gss = GroupShuffleSplit(n_splits=3, train_size=0.80, random_state=0)\n",
    "#     for train_unit, val_unit in gss.split(X_train_interim['Unit'].unique(), groups=X_train_interim['Unit'].unique()):\n",
    "#         train_unit = X_train_interim['Unit'].unique()[train_unit]  # gss returns indexes and index starts at 1\n",
    "#         train_split_array = gen_data_wrapper(X_train_interim, sequence_length, remaining_sensors, train_unit)\n",
    "#         train_split_label = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'], train_unit)\n",
    "        \n",
    "#         val_unit = X_train_interim['Unit'].unique()[val_unit]\n",
    "#         val_split_array = gen_data_wrapper(X_train_interim, sequence_length, remaining_sensors, val_unit)\n",
    "#         val_split_label = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'], val_unit)\n",
    "        \n",
    "#         # train and evaluate model\n",
    "#         # model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# #         model.load_weights(weights_file)  # reset optimizer and node weights before every training iteration \n",
    "#         history = model.fit(train_split_array, train_split_label,\n",
    "#                             validation_data=(val_split_array, val_split_label),\n",
    "#                             epochs=epochs,\n",
    "#                             batch_size=batch_size,\n",
    "#                             callbacks=[cb],\n",
    "#                             verbose=0)\n",
    "#         mse.append(history.history['val_loss'][-1])\n",
    "        \n",
    "#         test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "#         y_hat_val_split = model.predict(test_array)\n",
    "#         R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "#         RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "#         score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "    \n",
    "#     # append results\n",
    "#     d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "#          'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "#          'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "#          'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "#          'activation':activation, 'batch_size':batch_size}\n",
    "\n",
    "# #     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "#     results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc729f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>13.964884</td>\n",
       "      <td>0.775777</td>\n",
       "      <td>343.915667</td>\n",
       "      <td>74.926650</td>\n",
       "      <td>190.121938</td>\n",
       "      <td>5.957180</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.151649</td>\n",
       "      <td>0.318230</td>\n",
       "      <td>360.923998</td>\n",
       "      <td>25.834467</td>\n",
       "      <td>200.052210</td>\n",
       "      <td>7.959946</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>14.062142</td>\n",
       "      <td>0.358300</td>\n",
       "      <td>364.308378</td>\n",
       "      <td>63.508436</td>\n",
       "      <td>203.565669</td>\n",
       "      <td>6.704374</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>16.368388</td>\n",
       "      <td>3.104848</td>\n",
       "      <td>395.652719</td>\n",
       "      <td>110.278752</td>\n",
       "      <td>349.801570</td>\n",
       "      <td>194.723073</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14.488488</td>\n",
       "      <td>0.613202</td>\n",
       "      <td>395.661363</td>\n",
       "      <td>74.371661</td>\n",
       "      <td>200.746068</td>\n",
       "      <td>7.535411</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>16.245971</td>\n",
       "      <td>3.407711</td>\n",
       "      <td>398.048203</td>\n",
       "      <td>105.688379</td>\n",
       "      <td>357.772507</td>\n",
       "      <td>213.303134</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>16.533802</td>\n",
       "      <td>4.439844</td>\n",
       "      <td>414.011227</td>\n",
       "      <td>168.087064</td>\n",
       "      <td>396.946386</td>\n",
       "      <td>250.424198</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.539668</td>\n",
       "      <td>1.396782</td>\n",
       "      <td>438.421432</td>\n",
       "      <td>156.077811</td>\n",
       "      <td>188.438594</td>\n",
       "      <td>10.261512</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>17.224459</td>\n",
       "      <td>3.994632</td>\n",
       "      <td>447.701035</td>\n",
       "      <td>165.744409</td>\n",
       "      <td>398.483220</td>\n",
       "      <td>265.983996</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>14.722426</td>\n",
       "      <td>0.820359</td>\n",
       "      <td>449.178445</td>\n",
       "      <td>109.075064</td>\n",
       "      <td>214.336370</td>\n",
       "      <td>21.456571</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.676321</td>\n",
       "      <td>1.117965</td>\n",
       "      <td>499.745079</td>\n",
       "      <td>159.485144</td>\n",
       "      <td>197.608337</td>\n",
       "      <td>7.942686</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.175423</td>\n",
       "      <td>0.780014</td>\n",
       "      <td>544.613880</td>\n",
       "      <td>76.024880</td>\n",
       "      <td>278.366587</td>\n",
       "      <td>47.045653</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17.618346</td>\n",
       "      <td>4.905179</td>\n",
       "      <td>599.846193</td>\n",
       "      <td>349.365316</td>\n",
       "      <td>416.155553</td>\n",
       "      <td>276.260724</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17.672026</td>\n",
       "      <td>3.727296</td>\n",
       "      <td>616.973416</td>\n",
       "      <td>170.683161</td>\n",
       "      <td>369.286026</td>\n",
       "      <td>219.192960</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>27.548274</td>\n",
       "      <td>12.531744</td>\n",
       "      <td>2410.504064</td>\n",
       "      <td>2676.794819</td>\n",
       "      <td>1153.298381</td>\n",
       "      <td>921.429930</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26.903175</td>\n",
       "      <td>13.410996</td>\n",
       "      <td>2549.311440</td>\n",
       "      <td>2995.408831</td>\n",
       "      <td>1134.166372</td>\n",
       "      <td>1012.210694</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>29.954528</td>\n",
       "      <td>12.026686</td>\n",
       "      <td>2901.631196</td>\n",
       "      <td>2355.880453</td>\n",
       "      <td>1254.584727</td>\n",
       "      <td>858.316123</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>23.272207</td>\n",
       "      <td>11.899907</td>\n",
       "      <td>3711.053476</td>\n",
       "      <td>4675.242927</td>\n",
       "      <td>770.286418</td>\n",
       "      <td>729.700917</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.463595</td>\n",
       "      <td>11.753211</td>\n",
       "      <td>4013.882560</td>\n",
       "      <td>5059.505449</td>\n",
       "      <td>751.167496</td>\n",
       "      <td>733.250166</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.042641</td>\n",
       "      <td>12.066432</td>\n",
       "      <td>4222.743249</td>\n",
       "      <td>5412.611631</td>\n",
       "      <td>741.798950</td>\n",
       "      <td>733.429809</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23.973898</td>\n",
       "      <td>11.540261</td>\n",
       "      <td>4295.189679</td>\n",
       "      <td>5469.773078</td>\n",
       "      <td>813.306763</td>\n",
       "      <td>685.864788</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>33.155420</td>\n",
       "      <td>12.836313</td>\n",
       "      <td>5663.589742</td>\n",
       "      <td>4131.132461</td>\n",
       "      <td>1496.156952</td>\n",
       "      <td>865.523209</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22.574655</td>\n",
       "      <td>12.429659</td>\n",
       "      <td>5690.038087</td>\n",
       "      <td>7631.307992</td>\n",
       "      <td>713.756877</td>\n",
       "      <td>710.862096</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>22.789453</td>\n",
       "      <td>12.230790</td>\n",
       "      <td>5787.210285</td>\n",
       "      <td>7639.486794</td>\n",
       "      <td>720.521357</td>\n",
       "      <td>703.620355</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>33.359378</td>\n",
       "      <td>13.042178</td>\n",
       "      <td>5818.940659</td>\n",
       "      <td>4209.489997</td>\n",
       "      <td>1523.689382</td>\n",
       "      <td>890.438266</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>23.601174</td>\n",
       "      <td>11.937848</td>\n",
       "      <td>6235.391911</td>\n",
       "      <td>8116.969626</td>\n",
       "      <td>730.670720</td>\n",
       "      <td>718.108963</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>33.746142</td>\n",
       "      <td>12.074410</td>\n",
       "      <td>6293.657255</td>\n",
       "      <td>4882.013260</td>\n",
       "      <td>1515.048767</td>\n",
       "      <td>827.526498</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>23.197392</td>\n",
       "      <td>12.263305</td>\n",
       "      <td>6297.094143</td>\n",
       "      <td>8335.788078</td>\n",
       "      <td>728.291168</td>\n",
       "      <td>721.523772</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>22.942276</td>\n",
       "      <td>12.438819</td>\n",
       "      <td>6298.279961</td>\n",
       "      <td>8316.559874</td>\n",
       "      <td>717.511454</td>\n",
       "      <td>728.860536</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23.097550</td>\n",
       "      <td>12.348420</td>\n",
       "      <td>6300.672172</td>\n",
       "      <td>8343.497480</td>\n",
       "      <td>727.268428</td>\n",
       "      <td>722.066594</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>33.316135</td>\n",
       "      <td>12.635933</td>\n",
       "      <td>6445.002019</td>\n",
       "      <td>5141.330798</td>\n",
       "      <td>1505.011790</td>\n",
       "      <td>833.122003</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31.131362</td>\n",
       "      <td>11.931512</td>\n",
       "      <td>8755.773971</td>\n",
       "      <td>6000.385532</td>\n",
       "      <td>1229.928752</td>\n",
       "      <td>646.529468</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40.689847</td>\n",
       "      <td>11.913919</td>\n",
       "      <td>9707.635859</td>\n",
       "      <td>10067.866800</td>\n",
       "      <td>2228.037720</td>\n",
       "      <td>1201.866928</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32.525273</td>\n",
       "      <td>11.019061</td>\n",
       "      <td>10139.222214</td>\n",
       "      <td>7339.227642</td>\n",
       "      <td>1305.066315</td>\n",
       "      <td>634.984090</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>41.973644</td>\n",
       "      <td>12.086333</td>\n",
       "      <td>10581.174666</td>\n",
       "      <td>10635.908214</td>\n",
       "      <td>2349.458415</td>\n",
       "      <td>1229.835159</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>40.856957</td>\n",
       "      <td>1.954005</td>\n",
       "      <td>10749.501986</td>\n",
       "      <td>3545.722925</td>\n",
       "      <td>1892.595256</td>\n",
       "      <td>305.114504</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>43.761171</td>\n",
       "      <td>11.072355</td>\n",
       "      <td>10818.492566</td>\n",
       "      <td>9999.191046</td>\n",
       "      <td>2483.956380</td>\n",
       "      <td>1130.829801</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>41.422480</td>\n",
       "      <td>1.677593</td>\n",
       "      <td>11495.414688</td>\n",
       "      <td>4217.187746</td>\n",
       "      <td>1942.328776</td>\n",
       "      <td>292.533988</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>41.417617</td>\n",
       "      <td>1.625327</td>\n",
       "      <td>11806.953786</td>\n",
       "      <td>4461.870649</td>\n",
       "      <td>1938.522135</td>\n",
       "      <td>288.313933</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>31.681772</td>\n",
       "      <td>12.505839</td>\n",
       "      <td>12151.502964</td>\n",
       "      <td>8424.260704</td>\n",
       "      <td>1246.538401</td>\n",
       "      <td>696.842968</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31.584254</td>\n",
       "      <td>12.716343</td>\n",
       "      <td>12566.644646</td>\n",
       "      <td>8660.835397</td>\n",
       "      <td>1237.246684</td>\n",
       "      <td>711.048439</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>47.389653</td>\n",
       "      <td>6.927122</td>\n",
       "      <td>12637.019276</td>\n",
       "      <td>8624.194444</td>\n",
       "      <td>2712.086100</td>\n",
       "      <td>912.441305</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>45.796555</td>\n",
       "      <td>10.240744</td>\n",
       "      <td>12811.901347</td>\n",
       "      <td>11831.904201</td>\n",
       "      <td>2652.699544</td>\n",
       "      <td>1149.782527</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>40.454135</td>\n",
       "      <td>0.208311</td>\n",
       "      <td>17082.488901</td>\n",
       "      <td>2631.142548</td>\n",
       "      <td>1750.403320</td>\n",
       "      <td>13.800249</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.456036</td>\n",
       "      <td>0.206611</td>\n",
       "      <td>17112.835450</td>\n",
       "      <td>2599.312642</td>\n",
       "      <td>1750.241577</td>\n",
       "      <td>13.584606</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>40.554306</td>\n",
       "      <td>0.057660</td>\n",
       "      <td>18428.690435</td>\n",
       "      <td>647.608506</td>\n",
       "      <td>1744.360799</td>\n",
       "      <td>10.553977</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56.523299</td>\n",
       "      <td>6.703037</td>\n",
       "      <td>29739.384274</td>\n",
       "      <td>20355.147061</td>\n",
       "      <td>3841.285970</td>\n",
       "      <td>946.795819</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>57.197527</td>\n",
       "      <td>6.790727</td>\n",
       "      <td>32031.111111</td>\n",
       "      <td>21990.280410</td>\n",
       "      <td>3929.890625</td>\n",
       "      <td>966.654813</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57.495636</td>\n",
       "      <td>6.822756</td>\n",
       "      <td>33081.456371</td>\n",
       "      <td>22725.785778</td>\n",
       "      <td>3969.248210</td>\n",
       "      <td>974.682811</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>57.738596</td>\n",
       "      <td>6.857621</td>\n",
       "      <td>33977.750742</td>\n",
       "      <td>23373.864229</td>\n",
       "      <td>4001.520833</td>\n",
       "      <td>982.345875</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         RMSE   std_RMSE       S_score   std_S_score          MSE  \\\n",
       "42  13.964884   0.775777    343.915667     74.926650   190.121938   \n",
       "9   14.151649   0.318230    360.923998     25.834467   200.052210   \n",
       "41  14.062142   0.358300    364.308378     63.508436   203.565669   \n",
       "46  16.368388   3.104848    395.652719    110.278752   349.801570   \n",
       "22  14.488488   0.613202    395.661363     74.371661   200.746068   \n",
       "26  16.245971   3.407711    398.048203    105.688379   357.772507   \n",
       "43  16.533802   4.439844    414.011227    168.087064   396.946386   \n",
       "4   14.539668   1.396782    438.421432    156.077811   188.438594   \n",
       "33  17.224459   3.994632    447.701035    165.744409   398.483220   \n",
       "48  14.722426   0.820359    449.178445    109.075064   214.336370   \n",
       "11  14.676321   1.117965    499.745079    159.485144   197.608337   \n",
       "15  16.175423   0.780014    544.613880     76.024880   278.366587   \n",
       "14  17.618346   4.905179    599.846193    349.365316   416.155553   \n",
       "24  17.672026   3.727296    616.973416    170.683161   369.286026   \n",
       "39  27.548274  12.531744   2410.504064   2676.794819  1153.298381   \n",
       "18  26.903175  13.410996   2549.311440   2995.408831  1134.166372   \n",
       "35  29.954528  12.026686   2901.631196   2355.880453  1254.584727   \n",
       "34  23.272207  11.899907   3711.053476   4675.242927   770.286418   \n",
       "3   23.463595  11.753211   4013.882560   5059.505449   751.167496   \n",
       "2   23.042641  12.066432   4222.743249   5412.611631   741.798950   \n",
       "10  23.973898  11.540261   4295.189679   5469.773078   813.306763   \n",
       "45  33.155420  12.836313   5663.589742   4131.132461  1496.156952   \n",
       "5   22.574655  12.429659   5690.038087   7631.307992   713.756877   \n",
       "38  22.789453  12.230790   5787.210285   7639.486794   720.521357   \n",
       "21  33.359378  13.042178   5818.940659   4209.489997  1523.689382   \n",
       "28  23.601174  11.937848   6235.391911   8116.969626   730.670720   \n",
       "37  33.746142  12.074410   6293.657255   4882.013260  1515.048767   \n",
       "16  23.197392  12.263305   6297.094143   8335.788078   728.291168   \n",
       "47  22.942276  12.438819   6298.279961   8316.559874   717.511454   \n",
       "27  23.097550  12.348420   6300.672172   8343.497480   727.268428   \n",
       "13  33.316135  12.635933   6445.002019   5141.330798  1505.011790   \n",
       "8   31.131362  11.931512   8755.773971   6000.385532  1229.928752   \n",
       "7   40.689847  11.913919   9707.635859  10067.866800  2228.037720   \n",
       "12  32.525273  11.019061  10139.222214   7339.227642  1305.066315   \n",
       "29  41.973644  12.086333  10581.174666  10635.908214  2349.458415   \n",
       "36  40.856957   1.954005  10749.501986   3545.722925  1892.595256   \n",
       "40  43.761171  11.072355  10818.492566   9999.191046  2483.956380   \n",
       "32  41.422480   1.677593  11495.414688   4217.187746  1942.328776   \n",
       "49  41.417617   1.625327  11806.953786   4461.870649  1938.522135   \n",
       "17  31.681772  12.505839  12151.502964   8424.260704  1246.538401   \n",
       "20  31.584254  12.716343  12566.644646   8660.835397  1237.246684   \n",
       "25  47.389653   6.927122  12637.019276   8624.194444  2712.086100   \n",
       "19  45.796555  10.240744  12811.901347  11831.904201  2652.699544   \n",
       "23  40.454135   0.208311  17082.488901   2631.142548  1750.403320   \n",
       "0   40.456036   0.206611  17112.835450   2599.312642  1750.241577   \n",
       "31  40.554306   0.057660  18428.690435    647.608506  1744.360799   \n",
       "6   56.523299   6.703037  29739.384274  20355.147061  3841.285970   \n",
       "30  57.197527   6.790727  32031.111111  21990.280410  3929.890625   \n",
       "1   57.495636   6.822756  33081.456371  22725.785778  3969.248210   \n",
       "44  57.738596   6.857621  33977.750742  23373.864229  4001.520833   \n",
       "\n",
       "        std_MSE  nodes  dropout activation batch_size  \n",
       "42     5.957180  [256]      0.3       tanh         64  \n",
       "9      7.959946   [64]      0.3       tanh         64  \n",
       "41     6.704374   [64]      0.2       tanh         64  \n",
       "46   194.723073   [32]      0.4       tanh         64  \n",
       "22     7.535411   [64]      0.2       tanh         64  \n",
       "26   213.303134   [32]      0.3       tanh         64  \n",
       "43   250.424198   [32]      0.1       tanh         64  \n",
       "4     10.261512  [256]      0.4       tanh         64  \n",
       "33   265.983996   [32]      0.3       tanh         64  \n",
       "48    21.456571   [64]      0.1       tanh         64  \n",
       "11     7.942686  [256]      0.1       tanh         64  \n",
       "15    47.045653  [128]      0.1       tanh        128  \n",
       "14   276.260724   [32]      0.1       tanh         64  \n",
       "24   219.192960  [256]      0.2       tanh        128  \n",
       "39   921.429930   [64]      0.1       tanh        256  \n",
       "18  1012.210694   [32]      0.3       tanh        128  \n",
       "35   858.316123   [64]      0.2       tanh        256  \n",
       "34   729.700917   [64]      0.4       tanh        128  \n",
       "3    733.250166   [64]      0.4       tanh        128  \n",
       "2    733.429809   [64]      0.1       tanh        128  \n",
       "10   685.864788  [128]      0.4       tanh        256  \n",
       "45   865.523209   [64]      0.1       tanh        256  \n",
       "5    710.862096  [128]      0.1       tanh        128  \n",
       "38   703.620355  [128]      0.3       tanh        128  \n",
       "21   890.438266   [64]      0.2       tanh        256  \n",
       "28   718.108963  [256]      0.2       tanh        128  \n",
       "37   827.526498   [64]      0.4       tanh        256  \n",
       "16   721.523772  [256]      0.1       tanh        128  \n",
       "47   728.860536  [256]      0.1       tanh        128  \n",
       "27   722.066594  [128]      0.1       tanh         64  \n",
       "13   833.122003   [64]      0.1       tanh        256  \n",
       "8    646.529468  [128]      0.1       tanh        256  \n",
       "7   1201.866928   [32]      0.2       tanh        256  \n",
       "12   634.984090  [128]      0.4       tanh        256  \n",
       "29  1229.835159   [32]      0.3       tanh        256  \n",
       "36   305.114504  [128]      0.2       tanh        512  \n",
       "40  1130.829801   [32]      0.4       tanh        256  \n",
       "32   292.533988  [128]      0.4       tanh        512  \n",
       "49   288.313933  [128]      0.1       tanh        512  \n",
       "17   696.842968  [128]      0.4       tanh        128  \n",
       "20   711.048439  [128]      0.1       tanh        128  \n",
       "25   912.441305   [32]      0.4       tanh        256  \n",
       "19  1149.782527   [32]      0.3       tanh        256  \n",
       "23    13.800249  [256]      0.1       tanh        512  \n",
       "0     13.584606  [256]      0.1       tanh        512  \n",
       "31    10.553977  [256]      0.4       tanh        256  \n",
       "6    946.795819   [32]      0.1       tanh        512  \n",
       "30   966.654813   [32]      0.3       tanh        512  \n",
       "1    974.682811   [32]      0.4       tanh        512  \n",
       "44   982.345875   [32]      0.4       tanh        512  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results.to_csv(\"results/results_lstm_fd001\")\n",
    "# results.sort_values(by=['S_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4968c720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1\n",
      "4/4 [==============================] - 0s 25ms/step\n",
      "4/4 [==============================] - 0s 26ms/step\n",
      "4/4 [==============================] - 0s 19ms/step\n",
      "CPU times: user 54min 1s, sys: 13min 49s, total: 1h 7min 50s\n",
      "Wall time: 21min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = pd.DataFrame(columns=['RMSE', 'std_RMSE', \n",
    "                                'S_score','std_S_score',\n",
    "                                'MSE', 'std_MSE',\n",
    "                                'nodes', 'dropout',\n",
    "                                'activation', 'batch_size'])\n",
    "\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    if ITERATIONS < 10:\n",
    "        print('iteration ', i+1)\n",
    "    elif ((i+1) % 10 == 0):\n",
    "        print('iteration ', i+1)\n",
    "    \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    \n",
    "# parameter's sample\n",
    "    alpha = 0.3\n",
    "    sequence_length = 30\n",
    "    epochs = 20\n",
    "    nodes_per_layer = random.sample(nodes_list, 1)[0]\n",
    "    dropout = random.sample(dropouts, 1)[0]\n",
    "    activation = random.sample(activation_functions, 1)[0]\n",
    "    batch_size = random.sample(batch_size_list, 1)[0]\n",
    "    remaining_sensors = remaining_sensors\n",
    "    drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = create_model(input_shape, nodes_per_layer, dropout, activation)\n",
    "    \n",
    "    # create train-val split\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "           for unit_nr in X_test_interim['Unit'].unique())\n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "        \n",
    "    gss = GroupShuffleSplit(n_splits=3, train_size=0.80, random_state=0)\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        for train_unit, val_unit in gss.split(X_train_interim['Unit'].unique(), groups=X_train_interim['Unit'].unique()):\n",
    "            train_unit = X_train_interim['Unit'].unique()[train_unit]  # gss returns indexes and index starts at 1\n",
    "            train_split_array = gen_data_wrapper(X_train_interim, sequence_length, remaining_sensors, train_unit)\n",
    "            train_split_label = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'], train_unit)\n",
    "            \n",
    "            val_unit = X_train_interim['Unit'].unique()[val_unit]\n",
    "            val_split_array = gen_data_wrapper(X_train_interim, sequence_length, remaining_sensors, val_unit)\n",
    "            val_split_label = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'], val_unit)\n",
    "            \n",
    "            # train and evaluate model\n",
    "            # model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    #         model.load_weights(weights_file)  # reset optimizer and node weights before every training iteration \n",
    "            history = model.fit(train_split_array, train_split_label,\n",
    "                                validation_data=(val_split_array, val_split_label),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[cb],\n",
    "                                verbose=0)\n",
    "            mse.append(history.history['val_loss'][-1])\n",
    "            \n",
    "            test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "            y_hat_val_split = model.predict(test_array)\n",
    "            R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "            RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "            score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "        \n",
    "    #       append results\n",
    "        d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "            'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "            'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "            'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "            'activation':activation, 'batch_size':batch_size}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ed9f70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.671751</td>\n",
       "      <td>0.274223</td>\n",
       "      <td>25234.254876</td>\n",
       "      <td>3095.685427</td>\n",
       "      <td>1880.872396</td>\n",
       "      <td>10.667098</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE       S_score  std_S_score          MSE    std_MSE  \\\n",
       "0  41.671751  0.274223  25234.254876  3095.685427  1880.872396  10.667098   \n",
       "\n",
       "        nodes  dropout activation batch_size  \n",
       "0  [256, 256]      0.1    sigmoid         64  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "378b9c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.589615</td>\n",
       "      <td>10.340965</td>\n",
       "      <td>14485.962438</td>\n",
       "      <td>9981.027034</td>\n",
       "      <td>982.682170</td>\n",
       "      <td>676.837430</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.147799</td>\n",
       "      <td>0.066788</td>\n",
       "      <td>19242.143650</td>\n",
       "      <td>797.596495</td>\n",
       "      <td>1872.618693</td>\n",
       "      <td>25.281601</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.213384</td>\n",
       "      <td>0.162569</td>\n",
       "      <td>19988.531815</td>\n",
       "      <td>1921.223753</td>\n",
       "      <td>1875.515910</td>\n",
       "      <td>21.979551</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.261822</td>\n",
       "      <td>0.085623</td>\n",
       "      <td>20589.415551</td>\n",
       "      <td>991.718543</td>\n",
       "      <td>1873.111938</td>\n",
       "      <td>21.211562</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE   std_RMSE       S_score  std_S_score          MSE     std_MSE  \\\n",
       "0  28.589615  10.340965  14485.962438  9981.027034   982.682170  676.837430   \n",
       "1  41.147799   0.066788  19242.143650   797.596495  1872.618693   25.281601   \n",
       "3  41.213384   0.162569  19988.531815  1921.223753  1875.515910   21.979551   \n",
       "2  41.261822   0.085623  20589.415551   991.718543  1873.111938   21.211562   \n",
       "\n",
       "        nodes  dropout activation batch_size  \n",
       "0  [256, 128]      0.4       tanh         64  \n",
       "1   [256, 32]      0.3       tanh        512  \n",
       "3   [256, 32]      0.2       tanh         64  \n",
       "2   [256, 64]      0.2       tanh        512  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59f6b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6a4c2",
   "metadata": {},
   "source": [
    "## Sommaire: <a class=\"anchor\" id=\"sommaire\"></a>\n",
    "* [Sommaire](#sommaire)\n",
    "* [Preambule](#prem)\n",
    "     * [Package Loading](#package)\n",
    "     * [Functions](#function)\n",
    "* [LSTM](#lstm)\n",
    "    * [1.FD001](#fd001)\n",
    "        * [1.1 Data loading](#fd001dataload)\n",
    "        * [1.2 Model selection](#fd001modelselect)\n",
    "    * [2.FD002](#fd002)\n",
    "         * [2.1 Data loading](#fd002dataload)\n",
    "         * [2.2 Model selection](#fd002modelselect)\n",
    "    * [3.FD003](#fd003)\n",
    "         * [3.1 Data loading](#fd003dataload)\n",
    "         * [3.2 Model selection](#fd003modelselect)\n",
    "    * [4.FD004](#fd004)\n",
    "         * [4.1 Data loading](#fd004dataload)\n",
    "         * [4.2 Model selection](#fd004modelselect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cad48d",
   "metadata": {},
   "source": [
    "### FD002  <a class=\"anchor\" id=\"fd002\">  </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f69d236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [64]\t0.2\ttanh\t128\n",
    "\n",
    "def create_model2C(input_shape, nodes_per_layer, dropout, activation):\n",
    "\n",
    "    # weights_file = 'weights_file.h5'\n",
    "    \n",
    "    model = Sequential([LSTM(64, activation='tanh', return_sequences=True),\n",
    "                        # Dropout(dropout),\n",
    "                        LSTM(64, activation=activation),\n",
    "                        Dense(64, activation = 'relu'),\n",
    "                        Dropout(0.2),\n",
    "                        Dense(1)\n",
    "                        ])\n",
    "    # model.save_weights(weights_file)\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model3C(input_shape, nodes_per_layer, dropout, activation):\n",
    "    \n",
    "    # weights_file = 'weights_file.h5'\n",
    "    bs = 64\n",
    "    \n",
    "    model = Sequential([LSTM(64, activation='tanh', return_sequences=True),\n",
    "                        # Dropout(dropout),\n",
    "                        LSTM(32, activation=activation, return_sequences=True),\n",
    "                        LSTM(64, activation=activation),\n",
    "                        Dense(64, activation = 'relu'),\n",
    "                        Dropout(dropout),\n",
    "                        Dense(1)\n",
    "                        ])\n",
    "    # model.save_weights(weights_file)\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model4C(input_shape, nodes_per_layer, dropout, activation):\n",
    "    \n",
    "    # weights_file = 'weights_file.h5'\n",
    "    \n",
    "    model = Sequential([LSTM(64, activation='tanh', return_sequences=True),\n",
    "                        # Dropout(dropout),\n",
    "                        LSTM(64, activation=activation, return_sequences=True),\n",
    "                        LSTM(64, activation=activation, return_sequences=True),\n",
    "                        LSTM(32, activation=activation),\n",
    "                        Dense(32, activation = 'relu'),\n",
    "                        Dense(64, activation = 'relu'),\n",
    "                        Dropout(dropout),\n",
    "                        Dense(1)\n",
    "                        ])\n",
    "    # model.save_weights(weights_file)\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def model001_2C(input_shape, nodes_per_layer, dropout, activation):\n",
    "    '''\n",
    "    node = 256, activation = tanh, dropout = 0.3, bs = 64\n",
    "    '''\n",
    "    weights_file = \"weights_file.h5\"\n",
    "\n",
    "    cb = keras.callbacks.EarlyStopping(monitor='loss', patience=20)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=nodes_per_layer[0], activation='sigmoid', return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(nodes_per_layer[1], activation=activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=Adam(learning_rate=0.01))\n",
    "    model.save_weights(weights_file)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function for the rectified RUL\n",
    "def rul_piecewise_fct(X_train, rul):\n",
    "    \n",
    "    X_train['RUL'].clip(upper=rul, inplace=True)\n",
    "    \n",
    "    return X_train\n",
    "\n",
    "# Function for data preprocessing\n",
    "def prep_data(train, test, drop_sensors, remaining_sensors, alpha):\n",
    "    \n",
    "    X_train_interim = add_operating_condition(train.drop(drop_sensors, axis=1))\n",
    "    X_test_interim = add_operating_condition(test.drop(drop_sensors, axis=1))\n",
    "\n",
    "    X_train_interim, X_test_interim = condition_scaler(X_train_interim, X_test_interim, remaining_sensors)\n",
    "\n",
    "    X_train_interim = exponential_smoothing(X_train_interim, remaining_sensors, 0, alpha)\n",
    "    X_test_interim = exponential_smoothing(X_test_interim, remaining_sensors, 0, alpha)\n",
    "    \n",
    "    return X_train_interim, X_test_interim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be82591b",
   "metadata": {},
   "source": [
    "#### Data loading <a id = \"fd002dataload\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "048a0c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53759, 27) (33991, 26) (259, 1)\n"
     ]
    }
   ],
   "source": [
    "train, test, y_test = prepare_data('FD002.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50','P30','Nf','Nc','Ps30','phi',\n",
    "                'NRf','NRc','BPR','htBleed','W31','W32'] # selection based on main_notebook\n",
    "\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "train = rul_piecewise_fct(train, 130)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc65dc6",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e6ce5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3584"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lower alpha's perform better, so we can ditch a few high ones to reduce the search space\n",
    "alpha_list = [0.01, 0.05] + list(np.arange(10,60+1,10)/100)\n",
    "\n",
    "sequence_list = list(np.arange(10,40+1,5))\n",
    "epoch_list = list(np.arange(5,20+1,5))\n",
    "nodes_list = [[64, 64], [64, 32]]\n",
    "\n",
    "# lowest dropout=0.1, because I know zero dropout will yield better training results but worse generalization\n",
    "dropouts = list(np.arange(1,3)/10)  \n",
    "\n",
    "# again, earlier testing revealed relu performed significantly worse, so I removed it from the options\n",
    "activation_functions = ['tanh', 'tanh']\n",
    "batch_size_list = [128, 64]\n",
    "sensor_list = [sensor_names]\n",
    "\n",
    "tuning_options = np.prod([len(alpha_list),\n",
    "                          len(sequence_list),\n",
    "                          len(epoch_list),\n",
    "                          len(nodes_list),\n",
    "                          len(dropouts),\n",
    "                          len(activation_functions),\n",
    "                          len(batch_size_list),\n",
    "                          len(sensor_list)])\n",
    "tuning_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f02f2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 5\n",
    "SEED = 0\n",
    "rul_piecewise = 130"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830fab7e",
   "metadata": {},
   "source": [
    "#### Model selection <a id = \"fd002modelselect\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbf25096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1\n",
      "(44139, 38, 14) (44139, 1) (259, 38, 14)\n",
      "Epoch 1/25\n",
      "690/690 [==============================] - 26s 31ms/step - loss: 2417.7385 - val_loss: 1983.9803\n",
      "Epoch 2/25\n",
      "690/690 [==============================] - 21s 30ms/step - loss: 1939.5264 - val_loss: 1982.6530\n",
      "Epoch 3/25\n",
      "690/690 [==============================] - 21s 30ms/step - loss: 1935.9885 - val_loss: 1952.3306\n",
      "Epoch 4/25\n",
      "690/690 [==============================] - 20s 29ms/step - loss: 694.8010 - val_loss: 313.0717\n",
      "Epoch 5/25\n",
      "690/690 [==============================] - 20s 29ms/step - loss: 356.1960 - val_loss: 277.3177\n",
      "Epoch 6/25\n",
      "690/690 [==============================] - 20s 30ms/step - loss: 340.5667 - val_loss: 268.3872\n",
      "Epoch 7/25\n",
      "690/690 [==============================] - 26s 38ms/step - loss: 324.3188 - val_loss: 254.2124\n",
      "Epoch 8/25\n",
      "690/690 [==============================] - 31s 45ms/step - loss: 325.3454 - val_loss: 220.1299\n",
      "Epoch 9/25\n",
      "690/690 [==============================] - 16s 23ms/step - loss: 320.1729 - val_loss: 284.7556\n",
      "Epoch 10/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 310.7556 - val_loss: 241.5399\n",
      "Epoch 11/25\n",
      "690/690 [==============================] - 17s 24ms/step - loss: 309.0677 - val_loss: 202.0634\n",
      "Epoch 12/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 306.7275 - val_loss: 199.0766\n",
      "Epoch 13/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 298.4961 - val_loss: 224.5814\n",
      "Epoch 14/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 293.5686 - val_loss: 189.1500\n",
      "Epoch 15/25\n",
      "690/690 [==============================] - 17s 24ms/step - loss: 293.2063 - val_loss: 220.8885\n",
      "Epoch 16/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 287.5833 - val_loss: 186.9860\n",
      "Epoch 17/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 285.8242 - val_loss: 186.6932\n",
      "Epoch 18/25\n",
      "690/690 [==============================] - 17s 24ms/step - loss: 284.9641 - val_loss: 236.0077\n",
      "Epoch 19/25\n",
      "690/690 [==============================] - 17s 24ms/step - loss: 280.4678 - val_loss: 190.5733\n",
      "Epoch 20/25\n",
      "690/690 [==============================] - 17s 24ms/step - loss: 281.8740 - val_loss: 189.2485\n",
      "Epoch 21/25\n",
      "690/690 [==============================] - 17s 24ms/step - loss: 277.0211 - val_loss: 193.9365\n",
      "Epoch 22/25\n",
      "690/690 [==============================] - 17s 24ms/step - loss: 281.6445 - val_loss: 189.8103\n",
      "Epoch 23/25\n",
      "690/690 [==============================] - 17s 24ms/step - loss: 278.7919 - val_loss: 204.1532\n",
      "Epoch 24/25\n",
      "690/690 [==============================] - 17s 24ms/step - loss: 273.1423 - val_loss: 187.4494\n",
      "Epoch 25/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 273.1205 - val_loss: 209.0566\n",
      "9/9 [==============================] - 1s 9ms/step\n",
      "iteration  2\n",
      "(44139, 38, 14) (44139, 1) (259, 38, 14)\n",
      "Epoch 1/25\n",
      "690/690 [==============================] - 21s 26ms/step - loss: 2353.4771 - val_loss: 1997.6698\n",
      "Epoch 2/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 1920.7029 - val_loss: 1988.0074\n",
      "Epoch 3/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 1922.7592 - val_loss: 1971.0363\n",
      "Epoch 4/25\n",
      "690/690 [==============================] - 18s 26ms/step - loss: 1371.3610 - val_loss: 297.9238\n",
      "Epoch 5/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 354.5514 - val_loss: 223.6075\n",
      "Epoch 6/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 325.0103 - val_loss: 224.0179\n",
      "Epoch 7/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 311.6777 - val_loss: 286.2892\n",
      "Epoch 8/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 307.0264 - val_loss: 208.5377\n",
      "Epoch 9/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 302.3308 - val_loss: 240.6334\n",
      "Epoch 10/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 297.9059 - val_loss: 234.3619\n",
      "Epoch 11/25\n",
      "690/690 [==============================] - 17s 24ms/step - loss: 294.3834 - val_loss: 204.6415\n",
      "Epoch 12/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 294.8528 - val_loss: 204.9670\n",
      "Epoch 13/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 289.8077 - val_loss: 199.9807\n",
      "Epoch 14/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 288.7163 - val_loss: 191.1599\n",
      "Epoch 15/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 290.6963 - val_loss: 212.6445\n",
      "Epoch 16/25\n",
      "690/690 [==============================] - 18s 26ms/step - loss: 285.9692 - val_loss: 186.7359\n",
      "Epoch 17/25\n",
      "690/690 [==============================] - 18s 26ms/step - loss: 284.5371 - val_loss: 183.7008\n",
      "Epoch 18/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 281.1418 - val_loss: 233.1851\n",
      "Epoch 19/25\n",
      "690/690 [==============================] - 18s 26ms/step - loss: 280.5963 - val_loss: 189.1990\n",
      "Epoch 20/25\n",
      "690/690 [==============================] - 19s 27ms/step - loss: 277.2080 - val_loss: 189.6666\n",
      "Epoch 21/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 274.7097 - val_loss: 203.1820\n",
      "Epoch 22/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 276.6135 - val_loss: 198.3539\n",
      "Epoch 23/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 274.6048 - val_loss: 200.8785\n",
      "Epoch 24/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 271.0153 - val_loss: 188.2280\n",
      "Epoch 25/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 269.6125 - val_loss: 182.8558\n",
      "9/9 [==============================] - 1s 8ms/step\n",
      "iteration  3\n",
      "(44139, 38, 14) (44139, 1) (259, 38, 14)\n",
      "Epoch 1/25\n",
      "345/345 [==============================] - 25s 63ms/step - loss: 3015.6162 - val_loss: 1968.0897\n",
      "Epoch 2/25\n",
      "345/345 [==============================] - 22s 63ms/step - loss: 1942.9651 - val_loss: 1983.6534\n",
      "Epoch 3/25\n",
      "345/345 [==============================] - 21s 62ms/step - loss: 1933.0018 - val_loss: 1986.5118\n",
      "Epoch 4/25\n",
      "345/345 [==============================] - 21s 62ms/step - loss: 1940.1284 - val_loss: 1972.3506\n",
      "Epoch 5/25\n",
      "345/345 [==============================] - 22s 63ms/step - loss: 1932.4374 - val_loss: 1964.7612\n",
      "Epoch 6/25\n",
      "345/345 [==============================] - 22s 63ms/step - loss: 1873.9143 - val_loss: 1543.3049\n",
      "Epoch 7/25\n",
      "345/345 [==============================] - 22s 64ms/step - loss: 696.5518 - val_loss: 347.2129\n",
      "Epoch 8/25\n",
      "345/345 [==============================] - 22s 64ms/step - loss: 377.3489 - val_loss: 260.4464\n",
      "Epoch 9/25\n",
      "345/345 [==============================] - 22s 63ms/step - loss: 338.9710 - val_loss: 221.1621\n",
      "Epoch 10/25\n",
      "345/345 [==============================] - 22s 63ms/step - loss: 333.1104 - val_loss: 205.2864\n",
      "Epoch 11/25\n",
      "345/345 [==============================] - 22s 63ms/step - loss: 325.0089 - val_loss: 221.5767\n",
      "Epoch 12/25\n",
      "345/345 [==============================] - 22s 63ms/step - loss: 317.6783 - val_loss: 202.6054\n",
      "Epoch 13/25\n",
      "345/345 [==============================] - 22s 63ms/step - loss: 314.2974 - val_loss: 204.1243\n",
      "Epoch 14/25\n",
      "345/345 [==============================] - 22s 64ms/step - loss: 309.9909 - val_loss: 197.0600\n",
      "Epoch 15/25\n",
      "345/345 [==============================] - 22s 63ms/step - loss: 307.0076 - val_loss: 200.1438\n",
      "Epoch 16/25\n",
      "345/345 [==============================] - 22s 63ms/step - loss: 302.7560 - val_loss: 197.6448\n",
      "Epoch 17/25\n",
      "345/345 [==============================] - 22s 64ms/step - loss: 304.8449 - val_loss: 209.0047\n",
      "Epoch 18/25\n",
      "345/345 [==============================] - 22s 64ms/step - loss: 305.3488 - val_loss: 210.3732\n",
      "Epoch 19/25\n",
      "345/345 [==============================] - 22s 63ms/step - loss: 299.5210 - val_loss: 222.3753\n",
      "Epoch 20/25\n",
      "345/345 [==============================] - 22s 65ms/step - loss: 297.1450 - val_loss: 203.5451\n",
      "Epoch 21/25\n",
      "345/345 [==============================] - 22s 63ms/step - loss: 295.0120 - val_loss: 195.7613\n",
      "Epoch 22/25\n",
      "345/345 [==============================] - 22s 63ms/step - loss: 293.7683 - val_loss: 200.2100\n",
      "Epoch 23/25\n",
      "345/345 [==============================] - 22s 63ms/step - loss: 291.0909 - val_loss: 206.0997\n",
      "Epoch 24/25\n",
      "345/345 [==============================] - 22s 63ms/step - loss: 291.1485 - val_loss: 194.8973\n",
      "Epoch 25/25\n",
      "345/345 [==============================] - 21s 62ms/step - loss: 285.7336 - val_loss: 232.7168\n",
      "9/9 [==============================] - 1s 8ms/step\n",
      "iteration  4\n",
      "(44139, 38, 14) (44139, 1) (259, 38, 14)\n",
      "Epoch 1/25\n",
      "690/690 [==============================] - 21s 26ms/step - loss: 2453.4351 - val_loss: 1995.7133\n",
      "Epoch 2/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 1929.4637 - val_loss: 1987.5304\n",
      "Epoch 3/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 1924.0603 - val_loss: 1968.6422\n",
      "Epoch 4/25\n",
      "690/690 [==============================] - 18s 25ms/step - loss: 1087.4923 - val_loss: 364.0287\n",
      "Epoch 5/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 350.7612 - val_loss: 233.0604\n",
      "Epoch 6/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 327.6168 - val_loss: 279.4241\n",
      "Epoch 7/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 317.2883 - val_loss: 308.6337\n",
      "Epoch 8/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 314.0904 - val_loss: 219.5097\n",
      "Epoch 9/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 305.6213 - val_loss: 292.5857\n",
      "Epoch 10/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 296.9542 - val_loss: 226.9090\n",
      "Epoch 11/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 290.2669 - val_loss: 208.5488\n",
      "Epoch 12/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 289.6328 - val_loss: 199.2966\n",
      "Epoch 13/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 283.3273 - val_loss: 202.3606\n",
      "Epoch 14/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 282.5814 - val_loss: 188.6227\n",
      "Epoch 15/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 282.3782 - val_loss: 236.5154\n",
      "Epoch 16/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 277.4180 - val_loss: 191.1771\n",
      "Epoch 17/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 272.8032 - val_loss: 183.1496\n",
      "Epoch 18/25\n",
      "690/690 [==============================] - 18s 25ms/step - loss: 273.9973 - val_loss: 225.5282\n",
      "Epoch 19/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 268.9560 - val_loss: 192.6153\n",
      "Epoch 20/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 267.4664 - val_loss: 187.6631\n",
      "Epoch 21/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 262.2413 - val_loss: 186.0468\n",
      "Epoch 22/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 267.2219 - val_loss: 199.6282\n",
      "Epoch 23/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 261.6344 - val_loss: 189.7506\n",
      "Epoch 24/25\n",
      "690/690 [==============================] - 17s 25ms/step - loss: 259.3583 - val_loss: 210.3421\n",
      "Epoch 25/25\n",
      "690/690 [==============================] - 18s 25ms/step - loss: 260.2590 - val_loss: 207.7652\n",
      "9/9 [==============================] - 1s 8ms/step\n",
      "iteration  5\n",
      "(44139, 38, 14) (44139, 1) (259, 38, 14)\n",
      "Epoch 1/25\n",
      "345/345 [==============================] - 31s 80ms/step - loss: 2491.6377 - val_loss: 1976.6393\n",
      "Epoch 2/25\n",
      "345/345 [==============================] - 27s 79ms/step - loss: 1949.0142 - val_loss: 1977.0555\n",
      "Epoch 3/25\n",
      "345/345 [==============================] - 27s 79ms/step - loss: 1942.9218 - val_loss: 2000.1708\n",
      "Epoch 4/25\n",
      "345/345 [==============================] - 27s 80ms/step - loss: 1938.8405 - val_loss: 1990.9519\n",
      "Epoch 5/25\n",
      "345/345 [==============================] - 28s 80ms/step - loss: 1247.9854 - val_loss: 406.6970\n",
      "Epoch 6/25\n",
      "345/345 [==============================] - 27s 79ms/step - loss: 495.7671 - val_loss: 316.6272\n",
      "Epoch 7/25\n",
      "345/345 [==============================] - 28s 81ms/step - loss: 353.3298 - val_loss: 246.0691\n",
      "Epoch 8/25\n",
      "345/345 [==============================] - 27s 79ms/step - loss: 334.4908 - val_loss: 230.1833\n",
      "Epoch 9/25\n",
      "345/345 [==============================] - 28s 80ms/step - loss: 322.0287 - val_loss: 231.3781\n",
      "Epoch 10/25\n",
      "345/345 [==============================] - 27s 77ms/step - loss: 319.6014 - val_loss: 208.2811\n",
      "Epoch 11/25\n",
      "345/345 [==============================] - 27s 79ms/step - loss: 312.2532 - val_loss: 220.5303\n",
      "Epoch 12/25\n",
      "345/345 [==============================] - 27s 79ms/step - loss: 311.3379 - val_loss: 209.6995\n",
      "Epoch 13/25\n",
      "345/345 [==============================] - 28s 81ms/step - loss: 308.0605 - val_loss: 206.0284\n",
      "Epoch 14/25\n",
      "345/345 [==============================] - 27s 77ms/step - loss: 304.8653 - val_loss: 208.8607\n",
      "Epoch 15/25\n",
      "345/345 [==============================] - 27s 78ms/step - loss: 304.7829 - val_loss: 216.1308\n",
      "Epoch 16/25\n",
      "345/345 [==============================] - 28s 81ms/step - loss: 298.7483 - val_loss: 225.5113\n",
      "Epoch 17/25\n",
      "345/345 [==============================] - 27s 78ms/step - loss: 304.2995 - val_loss: 208.5184\n",
      "Epoch 18/25\n",
      "345/345 [==============================] - 28s 81ms/step - loss: 303.3310 - val_loss: 247.9914\n",
      "Epoch 19/25\n",
      "345/345 [==============================] - 27s 79ms/step - loss: 301.8596 - val_loss: 228.4896\n",
      "Epoch 20/25\n",
      "345/345 [==============================] - 27s 79ms/step - loss: 301.7162 - val_loss: 212.8958\n",
      "Epoch 21/25\n",
      "345/345 [==============================] - 28s 80ms/step - loss: 297.0811 - val_loss: 232.6021\n",
      "Epoch 22/25\n",
      "345/345 [==============================] - 27s 77ms/step - loss: 299.4402 - val_loss: 197.5289\n",
      "Epoch 23/25\n",
      "345/345 [==============================] - 28s 80ms/step - loss: 297.9026 - val_loss: 213.3273\n",
      "Epoch 24/25\n",
      "345/345 [==============================] - 27s 79ms/step - loss: 292.5273 - val_loss: 195.9630\n",
      "Epoch 25/25\n",
      "345/345 [==============================] - 27s 78ms/step - loss: 293.2018 - val_loss: 228.8633\n",
      "9/9 [==============================] - 1s 10ms/step\n",
      "CPU times: total: 9min 16s\n",
      "Wall time: 43min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = pd.DataFrame(columns=['RMSE', 'std_RMSE', \n",
    "                                'S_score','std_S_score',\n",
    "                                'MSE', 'std_MSE',\n",
    "                                'nodes', 'dropout',\n",
    "                                'activation', 'batch_size'])\n",
    "\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    if ITERATIONS < 10:\n",
    "        print('iteration ', i+1)\n",
    "    elif ((i+1) % 10 == 0):\n",
    "        print('iteration ', i+1)    \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    \n",
    "# parameter's sample\n",
    "    alpha = 0.3\n",
    "    sequence_length = 38\n",
    "    epochs = 25\n",
    "    nodes_per_layer = random.sample(nodes_list, 1)[0]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = random.sample(batch_size_list, 1)[0]\n",
    "    remaining_sensors = remaining_sensors\n",
    "    drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = create_model4C(input_shape, nodes_per_layer, dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #       append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23928136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.877344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56882.125904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>523.372864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.491750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1682.817681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>271.977844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.304586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>814.459646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>204.621170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.454804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>818.358242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>208.941376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.894001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>828.601908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193.043274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE       S_score  std_S_score         MSE  std_MSE  \\\n",
       "0  22.877344       0.0  56882.125904          0.0  523.372864      0.0   \n",
       "1  16.491750       0.0   1682.817681          0.0  271.977844      0.0   \n",
       "2  14.304586       0.0    814.459646          0.0  204.621170      0.0   \n",
       "3  14.454804       0.0    818.358242          0.0  208.941376      0.0   \n",
       "4  13.894001       0.0    828.601908          0.0  193.043274      0.0   \n",
       "\n",
       "      nodes  dropout activation batch_size  \n",
       "0  [64, 32]      0.2       tanh        128  \n",
       "1  [64, 64]      0.2       tanh        128  \n",
       "2  [64, 32]      0.2       tanh        128  \n",
       "3  [64, 32]      0.2       tanh        128  \n",
       "4  [64, 64]      0.2       tanh        128  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 Layers\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0d270d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.511870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>801.168680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210.594376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.864087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3375.886937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>284.397430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.112611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3691.585384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>328.066650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.377577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>760.836050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206.714706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.911973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>785.093423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193.542999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE  \\\n",
       "0  14.511870       0.0   801.168680          0.0  210.594376      0.0   \n",
       "1  16.864087       0.0  3375.886937          0.0  284.397430      0.0   \n",
       "2  18.112611       0.0  3691.585384          0.0  328.066650      0.0   \n",
       "3  14.377577       0.0   760.836050          0.0  206.714706      0.0   \n",
       "4  13.911973       0.0   785.093423          0.0  193.542999      0.0   \n",
       "\n",
       "      nodes  dropout activation batch_size  \n",
       "0  [64, 64]      0.2       tanh        128  \n",
       "1  [64, 32]      0.2       tanh         64  \n",
       "2  [64, 64]      0.2       tanh        128  \n",
       "3  [64, 32]      0.2       tanh        128  \n",
       "4  [64, 32]      0.2       tanh         64  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 layers \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56200ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.458791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>806.669349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>209.056641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.522419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>779.026240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.855804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.255057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>818.839181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>232.716782</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.414061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>853.118127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>207.765152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.128228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>836.288811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.863281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE  \\\n",
       "0  14.458791       0.0  806.669349          0.0  209.056641      0.0   \n",
       "1  13.522419       0.0  779.026240          0.0  182.855804      0.0   \n",
       "2  15.255057       0.0  818.839181          0.0  232.716782      0.0   \n",
       "3  14.414061       0.0  853.118127          0.0  207.765152      0.0   \n",
       "4  15.128228       0.0  836.288811          0.0  228.863281      0.0   \n",
       "\n",
       "      nodes  dropout activation batch_size  \n",
       "0  [64, 32]      0.2       tanh         64  \n",
       "1  [64, 32]      0.2       tanh         64  \n",
       "2  [64, 32]      0.2       tanh        128  \n",
       "3  [64, 32]      0.2       tanh         64  \n",
       "4  [64, 64]      0.2       tanh        128  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 layers \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5485ffdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\env_test\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\env_test\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 7ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 7ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 1s 6ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 8ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "iteration  10\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "9/9 [==============================] - 0s 10ms/step\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 9ms/step\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "9/9 [==============================] - 0s 10ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 10ms/step\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "9/9 [==============================] - 0s 10ms/step\n",
      "iteration  20\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 10ms/step\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 7ms/step\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 9ms/step\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "iteration  30\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 8ms/step\n",
      "9/9 [==============================] - 0s 7ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 7ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 1s 15ms/step\n",
      "9/9 [==============================] - 0s 10ms/step\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "iteration  40\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 7ms/step\n",
      "9/9 [==============================] - 0s 7ms/step\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 7ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "9/9 [==============================] - 0s 7ms/step\n",
      "9/9 [==============================] - 0s 7ms/step\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "iteration  50\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "9/9 [==============================] - 0s 15ms/step\n",
      "9/9 [==============================] - 0s 15ms/step\n",
      "CPU times: total: 50min 30s\n",
      "Wall time: 3h 53min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = pd.DataFrame(columns=['RMSE', 'std_RMSE', \n",
    "                                'S_score','std_S_score',\n",
    "                                'MSE', 'std_MSE',\n",
    "                                'nodes', 'dropout',\n",
    "                                'activation', 'batch_size'])\n",
    "\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    if ITERATIONS < 10:\n",
    "        print('iteration ', i+1)\n",
    "    elif ((i+1) % 10 == 0):\n",
    "        print('iteration ', i+1)\n",
    "    \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "\n",
    "    # parameter's sample\n",
    "    alpha = 0.3\n",
    "    sequence_length = 30\n",
    "    epochs = 15\n",
    "    nodes_per_layer = random.sample(nodes_list, 1)[0]\n",
    "    dropout = random.sample(dropouts, 1)[0]\n",
    "    activation = random.sample(activation_functions, 1)[0]\n",
    "    batch_size = random.sample(batch_size_list, 1)[0]\n",
    "    remaining_sensors = remaining_sensors\n",
    "    drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "    \n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = create_model(input_shape, nodes_per_layer, dropout, activation)\n",
    "    \n",
    "    # create train-val split\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "           for unit_nr in X_test_interim['Unit'].unique())\n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "        \n",
    "    gss = GroupShuffleSplit(n_splits=3, train_size=0.80, random_state=0)\n",
    "    for train_unit, val_unit in gss.split(X_train_interim['Unit'].unique(), groups=X_train_interim['Unit'].unique()):\n",
    "        train_unit = X_train_interim['Unit'].unique()[train_unit]  # gss returns indexes and index starts at 1\n",
    "        train_split_array = gen_data_wrapper(X_train_interim, sequence_length, remaining_sensors, train_unit)\n",
    "        train_split_label = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'], train_unit)\n",
    "        \n",
    "        val_unit = X_train_interim['Unit'].unique()[val_unit]\n",
    "        val_split_array = gen_data_wrapper(X_train_interim, sequence_length, remaining_sensors, val_unit)\n",
    "        val_split_label = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'], val_unit)\n",
    "        \n",
    "        # train and evaluate model\n",
    "        history = model.fit(train_split_array, train_split_label,\n",
    "                            validation_data=(val_split_array, val_split_label),\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            callbacks=[cb],\n",
    "                            verbose=0)\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "        \n",
    "        test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "    \n",
    "    # append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bccf5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>14.831516</td>\n",
       "      <td>0.287748</td>\n",
       "      <td>964.328597</td>\n",
       "      <td>84.554237</td>\n",
       "      <td>252.536174</td>\n",
       "      <td>17.892908</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.805402</td>\n",
       "      <td>0.739714</td>\n",
       "      <td>1128.832308</td>\n",
       "      <td>376.061607</td>\n",
       "      <td>254.512899</td>\n",
       "      <td>24.815123</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14.963188</td>\n",
       "      <td>0.369970</td>\n",
       "      <td>1142.584571</td>\n",
       "      <td>168.177174</td>\n",
       "      <td>232.057353</td>\n",
       "      <td>18.358876</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14.293991</td>\n",
       "      <td>0.894635</td>\n",
       "      <td>1151.887167</td>\n",
       "      <td>383.437056</td>\n",
       "      <td>245.601725</td>\n",
       "      <td>27.634230</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14.882330</td>\n",
       "      <td>0.451319</td>\n",
       "      <td>1181.642982</td>\n",
       "      <td>438.739383</td>\n",
       "      <td>266.680257</td>\n",
       "      <td>13.604762</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>14.490891</td>\n",
       "      <td>0.082729</td>\n",
       "      <td>1214.240597</td>\n",
       "      <td>183.147265</td>\n",
       "      <td>240.145737</td>\n",
       "      <td>24.178127</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15.302502</td>\n",
       "      <td>1.085625</td>\n",
       "      <td>1325.333676</td>\n",
       "      <td>648.526013</td>\n",
       "      <td>265.947968</td>\n",
       "      <td>21.246164</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>15.165437</td>\n",
       "      <td>1.208680</td>\n",
       "      <td>1361.339447</td>\n",
       "      <td>754.663156</td>\n",
       "      <td>257.504644</td>\n",
       "      <td>36.099646</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17.410164</td>\n",
       "      <td>2.663108</td>\n",
       "      <td>1754.234416</td>\n",
       "      <td>520.296523</td>\n",
       "      <td>359.644979</td>\n",
       "      <td>143.454438</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>17.695608</td>\n",
       "      <td>1.817548</td>\n",
       "      <td>3034.631001</td>\n",
       "      <td>348.437438</td>\n",
       "      <td>299.014175</td>\n",
       "      <td>68.932800</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>17.056624</td>\n",
       "      <td>0.644706</td>\n",
       "      <td>3248.102605</td>\n",
       "      <td>1692.089717</td>\n",
       "      <td>260.148295</td>\n",
       "      <td>30.050463</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16.102721</td>\n",
       "      <td>1.160014</td>\n",
       "      <td>5989.847764</td>\n",
       "      <td>6242.446808</td>\n",
       "      <td>260.996933</td>\n",
       "      <td>34.899484</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25.182919</td>\n",
       "      <td>12.735443</td>\n",
       "      <td>6136.375514</td>\n",
       "      <td>6903.477859</td>\n",
       "      <td>885.938833</td>\n",
       "      <td>773.003374</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>18.009804</td>\n",
       "      <td>1.354985</td>\n",
       "      <td>6700.667312</td>\n",
       "      <td>5041.997954</td>\n",
       "      <td>257.962962</td>\n",
       "      <td>19.854733</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.800684</td>\n",
       "      <td>13.221121</td>\n",
       "      <td>7133.141226</td>\n",
       "      <td>8412.627918</td>\n",
       "      <td>934.904897</td>\n",
       "      <td>801.865954</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.125693</td>\n",
       "      <td>1.399498</td>\n",
       "      <td>7450.780451</td>\n",
       "      <td>4573.073905</td>\n",
       "      <td>244.218470</td>\n",
       "      <td>22.457198</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>26.322294</td>\n",
       "      <td>12.878629</td>\n",
       "      <td>7503.382125</td>\n",
       "      <td>8184.356263</td>\n",
       "      <td>922.994456</td>\n",
       "      <td>806.482737</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>27.260557</td>\n",
       "      <td>12.294798</td>\n",
       "      <td>7604.520783</td>\n",
       "      <td>8064.907248</td>\n",
       "      <td>948.778870</td>\n",
       "      <td>794.527515</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>27.698138</td>\n",
       "      <td>11.925267</td>\n",
       "      <td>7983.045156</td>\n",
       "      <td>7788.807151</td>\n",
       "      <td>946.547394</td>\n",
       "      <td>797.082241</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>27.141009</td>\n",
       "      <td>12.355170</td>\n",
       "      <td>8403.000478</td>\n",
       "      <td>7827.425663</td>\n",
       "      <td>961.558818</td>\n",
       "      <td>765.944271</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>22.933568</td>\n",
       "      <td>7.441319</td>\n",
       "      <td>9327.012932</td>\n",
       "      <td>7987.469386</td>\n",
       "      <td>584.395274</td>\n",
       "      <td>455.114493</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.668515</td>\n",
       "      <td>11.813934</td>\n",
       "      <td>10323.173465</td>\n",
       "      <td>6934.308503</td>\n",
       "      <td>938.071859</td>\n",
       "      <td>804.170340</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>17.669244</td>\n",
       "      <td>2.558634</td>\n",
       "      <td>12266.521099</td>\n",
       "      <td>10412.441358</td>\n",
       "      <td>240.653325</td>\n",
       "      <td>23.519455</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>25.024451</td>\n",
       "      <td>11.744739</td>\n",
       "      <td>14150.204464</td>\n",
       "      <td>15111.103550</td>\n",
       "      <td>726.764435</td>\n",
       "      <td>641.439052</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>33.214553</td>\n",
       "      <td>12.013278</td>\n",
       "      <td>14785.374987</td>\n",
       "      <td>9529.124495</td>\n",
       "      <td>1250.234070</td>\n",
       "      <td>688.489828</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33.625055</td>\n",
       "      <td>12.963316</td>\n",
       "      <td>17732.972524</td>\n",
       "      <td>13037.481370</td>\n",
       "      <td>1333.772624</td>\n",
       "      <td>720.289982</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>24.449855</td>\n",
       "      <td>13.256525</td>\n",
       "      <td>18729.950768</td>\n",
       "      <td>24703.705734</td>\n",
       "      <td>756.113612</td>\n",
       "      <td>696.676135</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>27.436617</td>\n",
       "      <td>10.858038</td>\n",
       "      <td>20051.885806</td>\n",
       "      <td>19513.951358</td>\n",
       "      <td>778.510345</td>\n",
       "      <td>664.467022</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>26.332617</td>\n",
       "      <td>11.816566</td>\n",
       "      <td>20206.472589</td>\n",
       "      <td>22067.056642</td>\n",
       "      <td>784.235067</td>\n",
       "      <td>652.162917</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>34.364636</td>\n",
       "      <td>12.815080</td>\n",
       "      <td>20412.138087</td>\n",
       "      <td>16331.182896</td>\n",
       "      <td>1383.250682</td>\n",
       "      <td>739.910116</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.877057</td>\n",
       "      <td>1.664825</td>\n",
       "      <td>21030.157962</td>\n",
       "      <td>15575.377141</td>\n",
       "      <td>259.764420</td>\n",
       "      <td>33.234554</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>25.741253</td>\n",
       "      <td>12.285957</td>\n",
       "      <td>21087.444789</td>\n",
       "      <td>27107.620217</td>\n",
       "      <td>770.441620</td>\n",
       "      <td>667.299447</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.704578</td>\n",
       "      <td>12.563770</td>\n",
       "      <td>21820.531953</td>\n",
       "      <td>28530.648287</td>\n",
       "      <td>770.120229</td>\n",
       "      <td>689.320825</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24.381129</td>\n",
       "      <td>13.540606</td>\n",
       "      <td>22044.528834</td>\n",
       "      <td>29839.542587</td>\n",
       "      <td>761.600311</td>\n",
       "      <td>695.649355</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>27.148671</td>\n",
       "      <td>11.438259</td>\n",
       "      <td>22563.786716</td>\n",
       "      <td>27328.941866</td>\n",
       "      <td>781.501292</td>\n",
       "      <td>668.955055</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>26.056138</td>\n",
       "      <td>12.326501</td>\n",
       "      <td>22889.355512</td>\n",
       "      <td>27924.230853</td>\n",
       "      <td>766.829305</td>\n",
       "      <td>691.622469</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34.347901</td>\n",
       "      <td>13.191707</td>\n",
       "      <td>25116.914633</td>\n",
       "      <td>22321.195848</td>\n",
       "      <td>1380.657461</td>\n",
       "      <td>730.096996</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>26.943528</td>\n",
       "      <td>11.825388</td>\n",
       "      <td>26161.682029</td>\n",
       "      <td>27866.675311</td>\n",
       "      <td>755.911102</td>\n",
       "      <td>699.239244</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20.880357</td>\n",
       "      <td>2.131613</td>\n",
       "      <td>40966.205428</td>\n",
       "      <td>47633.974369</td>\n",
       "      <td>331.851181</td>\n",
       "      <td>110.752481</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>43.541843</td>\n",
       "      <td>0.342554</td>\n",
       "      <td>43566.485065</td>\n",
       "      <td>17028.908719</td>\n",
       "      <td>1842.030924</td>\n",
       "      <td>126.510484</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>19.557553</td>\n",
       "      <td>3.690347</td>\n",
       "      <td>49378.603923</td>\n",
       "      <td>66178.888439</td>\n",
       "      <td>269.366201</td>\n",
       "      <td>16.928481</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.522832</td>\n",
       "      <td>11.207314</td>\n",
       "      <td>49852.825730</td>\n",
       "      <td>16120.782097</td>\n",
       "      <td>1260.410909</td>\n",
       "      <td>689.676596</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>43.446918</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>61194.899979</td>\n",
       "      <td>2558.466145</td>\n",
       "      <td>1748.422933</td>\n",
       "      <td>2.371037</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>43.478982</td>\n",
       "      <td>0.006503</td>\n",
       "      <td>62425.891634</td>\n",
       "      <td>243.819584</td>\n",
       "      <td>1748.145671</td>\n",
       "      <td>2.801699</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.126661</td>\n",
       "      <td>10.868560</td>\n",
       "      <td>73685.014823</td>\n",
       "      <td>32740.899751</td>\n",
       "      <td>806.171326</td>\n",
       "      <td>662.394781</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>21.663151</td>\n",
       "      <td>3.321160</td>\n",
       "      <td>110886.899723</td>\n",
       "      <td>117326.862890</td>\n",
       "      <td>249.970474</td>\n",
       "      <td>15.417556</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>37.446542</td>\n",
       "      <td>8.552798</td>\n",
       "      <td>113059.250726</td>\n",
       "      <td>70178.509483</td>\n",
       "      <td>1259.945465</td>\n",
       "      <td>687.986348</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21.901515</td>\n",
       "      <td>3.457008</td>\n",
       "      <td>116785.888534</td>\n",
       "      <td>91557.511520</td>\n",
       "      <td>266.083872</td>\n",
       "      <td>16.758816</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>22.578949</td>\n",
       "      <td>4.224520</td>\n",
       "      <td>211151.354279</td>\n",
       "      <td>148117.476652</td>\n",
       "      <td>259.680979</td>\n",
       "      <td>29.144504</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23.517156</td>\n",
       "      <td>4.103458</td>\n",
       "      <td>273676.716384</td>\n",
       "      <td>195746.162788</td>\n",
       "      <td>259.820587</td>\n",
       "      <td>27.107727</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         RMSE   std_RMSE        S_score    std_S_score          MSE  \\\n",
       "47  14.831516   0.287748     964.328597      84.554237   252.536174   \n",
       "26  14.805402   0.739714    1128.832308     376.061607   254.512899   \n",
       "17  14.963188   0.369970    1142.584571     168.177174   232.057353   \n",
       "27  14.293991   0.894635    1151.887167     383.437056   245.601725   \n",
       "10  14.882330   0.451319    1181.642982     438.739383   266.680257   \n",
       "36  14.490891   0.082729    1214.240597     183.147265   240.145737   \n",
       "29  15.302502   1.085625    1325.333676     648.526013   265.947968   \n",
       "33  15.165437   1.208680    1361.339447     754.663156   257.504644   \n",
       "8   17.410164   2.663108    1754.234416     520.296523   359.644979   \n",
       "42  17.695608   1.817548    3034.631001     348.437438   299.014175   \n",
       "49  17.056624   0.644706    3248.102605    1692.089717   260.148295   \n",
       "19  16.102721   1.160014    5989.847764    6242.446808   260.996933   \n",
       "23  25.182919  12.735443    6136.375514    6903.477859   885.938833   \n",
       "31  18.009804   1.354985    6700.667312    5041.997954   257.962962   \n",
       "24  25.800684  13.221121    7133.141226    8412.627918   934.904897   \n",
       "1   17.125693   1.399498    7450.780451    4573.073905   244.218470   \n",
       "37  26.322294  12.878629    7503.382125    8184.356263   922.994456   \n",
       "21  27.260557  12.294798    7604.520783    8064.907248   948.778870   \n",
       "14  27.698138  11.925267    7983.045156    7788.807151   946.547394   \n",
       "16  27.141009  12.355170    8403.000478    7827.425663   961.558818   \n",
       "41  22.933568   7.441319    9327.012932    7987.469386   584.395274   \n",
       "3   27.668515  11.813934   10323.173465    6934.308503   938.071859   \n",
       "30  17.669244   2.558634   12266.521099   10412.441358   240.653325   \n",
       "45  25.024451  11.744739   14150.204464   15111.103550   726.764435   \n",
       "12  33.214553  12.013278   14785.374987    9529.124495  1250.234070   \n",
       "5   33.625055  12.963316   17732.972524   13037.481370  1333.772624   \n",
       "38  24.449855  13.256525   18729.950768   24703.705734   756.113612   \n",
       "39  27.436617  10.858038   20051.885806   19513.951358   778.510345   \n",
       "46  26.332617  11.816566   20206.472589   22067.056642   784.235067   \n",
       "7   34.364636  12.815080   20412.138087   16331.182896  1383.250682   \n",
       "4   19.877057   1.664825   21030.157962   15575.377141   259.764420   \n",
       "43  25.741253  12.285957   21087.444789   27107.620217   770.441620   \n",
       "0   25.704578  12.563770   21820.531953   28530.648287   770.120229   \n",
       "25  24.381129  13.540606   22044.528834   29839.542587   761.600311   \n",
       "15  27.148671  11.438259   22563.786716   27328.941866   781.501292   \n",
       "44  26.056138  12.326501   22889.355512   27924.230853   766.829305   \n",
       "34  34.347901  13.191707   25116.914633   22321.195848  1380.657461   \n",
       "32  26.943528  11.825388   26161.682029   27866.675311   755.911102   \n",
       "20  20.880357   2.131613   40966.205428   47633.974369   331.851181   \n",
       "11  43.541843   0.342554   43566.485065   17028.908719  1842.030924   \n",
       "22  19.557553   3.690347   49378.603923   66178.888439   269.366201   \n",
       "2   35.522832  11.207314   49852.825730   16120.782097  1260.410909   \n",
       "48  43.446918   0.067416   61194.899979    2558.466145  1748.422933   \n",
       "18  43.478982   0.006503   62425.891634     243.819584  1748.145671   \n",
       "28  29.126661  10.868560   73685.014823   32740.899751   806.171326   \n",
       "35  21.663151   3.321160  110886.899723  117326.862890   249.970474   \n",
       "40  37.446542   8.552798  113059.250726   70178.509483  1259.945465   \n",
       "6   21.901515   3.457008  116785.888534   91557.511520   266.083872   \n",
       "13  22.578949   4.224520  211151.354279  148117.476652   259.680979   \n",
       "9   23.517156   4.103458  273676.716384  195746.162788   259.820587   \n",
       "\n",
       "       std_MSE  nodes  dropout activation batch_size  \n",
       "47   17.892908   [64]      0.2       tanh        128  \n",
       "26   24.815123  [256]      0.2       tanh        128  \n",
       "17   18.358876  [128]      0.2       tanh         64  \n",
       "27   27.634230   [64]      0.1       tanh         64  \n",
       "10   13.604762  [128]      0.3       tanh        128  \n",
       "36   24.178127   [32]      0.2       tanh         64  \n",
       "29   21.246164  [256]      0.2       tanh        128  \n",
       "33   36.099646  [256]      0.3       tanh        128  \n",
       "8   143.454438   [64]      0.3       tanh        256  \n",
       "42   68.932800   [32]      0.4       tanh        128  \n",
       "49   30.050463  [256]      0.4       tanh        128  \n",
       "19   34.899484   [64]      0.2       tanh        128  \n",
       "23  773.003374   [32]      0.4       tanh        256  \n",
       "31   19.854733  [128]      0.3       tanh        128  \n",
       "24  801.865954   [32]      0.3       tanh        256  \n",
       "1    22.457198  [128]      0.4       tanh         64  \n",
       "37  806.482737   [32]      0.1       tanh        256  \n",
       "21  794.527515   [32]      0.2       tanh        256  \n",
       "14  797.082241   [32]      0.2       tanh        256  \n",
       "16  765.944271   [32]      0.1       tanh        256  \n",
       "41  455.114493   [64]      0.4       tanh        256  \n",
       "3   804.170340   [32]      0.3       tanh        256  \n",
       "30   23.519455   [32]      0.3       tanh         64  \n",
       "45  641.439052   [64]      0.3       tanh        256  \n",
       "12  688.489828   [64]      0.1       tanh        512  \n",
       "5   720.289982   [64]      0.4       tanh        512  \n",
       "38  696.676135   [64]      0.1       tanh        256  \n",
       "39  664.467022  [128]      0.1       tanh        512  \n",
       "46  652.162917  [128]      0.1       tanh        512  \n",
       "7   739.910116   [64]      0.4       tanh        512  \n",
       "4    33.234554   [64]      0.3       tanh         64  \n",
       "43  667.299447  [256]      0.2       tanh        512  \n",
       "0   689.320825  [128]      0.3       tanh        256  \n",
       "25  695.649355  [128]      0.2       tanh        128  \n",
       "15  668.955055  [256]      0.1       tanh        512  \n",
       "44  691.622469  [128]      0.3       tanh        256  \n",
       "34  730.096996   [64]      0.3       tanh        512  \n",
       "32  699.239244  [128]      0.1       tanh        128  \n",
       "20  110.752481   [64]      0.4       tanh        256  \n",
       "11  126.510484   [64]      0.4       tanh        512  \n",
       "22   16.928481  [256]      0.3       tanh        128  \n",
       "2   689.676596  [128]      0.4       tanh        256  \n",
       "48    2.371037  [128]      0.1       tanh        512  \n",
       "18    2.801699  [256]      0.4       tanh        512  \n",
       "28  662.394781  [256]      0.2       tanh        512  \n",
       "35   15.417556  [256]      0.1       tanh        128  \n",
       "40  687.986348  [256]      0.3       tanh        256  \n",
       "6    16.758816  [128]      0.3       tanh        128  \n",
       "13   29.144504  [256]      0.4       tanh        128  \n",
       "9    27.107727  [256]      0.2       tanh        128  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.to_csv(\"results/results_lstm_fd002\")\n",
    "results.sort_values(by=['S_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b24cc4",
   "metadata": {},
   "source": [
    "## Sommaire: <a class=\"anchor\" id=\"sommaire\"></a>\n",
    "* [Sommaire](#sommaire)\n",
    "* [Preambule](#prem)\n",
    "     * [Package Loading](#package)\n",
    "     * [Functions](#function)\n",
    "* [LSTM](#lstm)\n",
    "    * [1.FD001](#fd001)\n",
    "        * [1.1 Data loading](#fd001dataload)\n",
    "        * [1.2 Model selection](#fd001modelselect)\n",
    "    * [2.FD002](#fd002)\n",
    "         * [2.1 Data loading](#fd002dataload)\n",
    "         * [2.2 Model selection](#fd002modelselect)\n",
    "    * [3.FD003](#fd003)\n",
    "         * [3.1 Data loading](#fd003dataload)\n",
    "         * [3.2 Model selection](#fd003modelselect)\n",
    "    * [4.FD004](#fd004)\n",
    "         * [4.1 Data loading](#fd004dataload)\n",
    "         * [4.2 Model selection](#fd004modelselect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ff4ca7",
   "metadata": {},
   "source": [
    "### FD003  <a class=\"anchor\" id=\"fd003\">  </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31819f2c",
   "metadata": {},
   "source": [
    "#### Data loading <a id = \"fd003dataload\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87a04591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model2C(input_shape, nodes_per_layer, dropout, activation):\n",
    "\n",
    "    # weights_file = 'weights_file.h5'\n",
    "    \n",
    "    model = Sequential([LSTM(nodes_per_layer[0], activation='tanh', return_sequences=True),\n",
    "                        # Dropout(dropout),\n",
    "                        LSTM(64, activation=activation),\n",
    "                        Dense(nodes_per_layer[0], activation = 'relu'),\n",
    "                        Dropout(dropout),\n",
    "                        Dense(1)\n",
    "                        ])\n",
    "    # model.save_weights(weights_file)\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model3C(input_shape, nodes_per_layer, dropout, activation):\n",
    "    \n",
    "    # weights_file = 'weights_file.h5'\n",
    "    # bs = 64\n",
    "    \n",
    "    model = Sequential([LSTM(nodes_per_layer[0], activation='tanh', return_sequences=True),\n",
    "                        # Dropout(dropout),\n",
    "                        LSTM(64, activation=activation, return_sequences=True),\n",
    "                        LSTM(64, activation=activation),\n",
    "                        Dense(32, activation = 'relu'),\n",
    "                        Dropout(dropout),\n",
    "                        Dense(1)\n",
    "                        ])\n",
    "    # model.save_weights(weights_file)\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model4C(input_shape, nodes_per_layer, dropout, activation):\n",
    "    \n",
    "    # weights_file = 'weights_file.h5'\n",
    "    \n",
    "    model = Sequential([LSTM(nodes_per_layer[0], activation='tanh', return_sequences=True),\n",
    "                        # Dropout(dropout),\n",
    "                        LSTM(64, activation=activation, return_sequences=True),\n",
    "                        LSTM(64, activation=activation, return_sequences=True),\n",
    "                        LSTM(64, activation=activation),\n",
    "                        Dense(32, activation = 'relu'),\n",
    "                        Dense(64, activation = 'relu'),\n",
    "                        Dropout(dropout),\n",
    "                        Dense(1)\n",
    "                        ])\n",
    "    # model.save_weights(weights_file)\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def model001_2C(input_shape, nodes_per_layer, dropout, activation):\n",
    "    '''\n",
    "    node = 256, activation = tanh, dropout = 0.3, bs = 64\n",
    "    '''\n",
    "    weights_file = \"weights_file.h5\"\n",
    "\n",
    "    cb = keras.callbacks.EarlyStopping(monitor='loss', patience=20)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=nodes_per_layer[0], activation='sigmoid', return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(nodes_per_layer[1], activation=activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=Adam(learning_rate=0.01))\n",
    "    model.save_weights(weights_file)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function for the rectified RUL\n",
    "def rul_piecewise_fct(X_train, rul):\n",
    "    \n",
    "    X_train['RUL'].clip(upper=rul, inplace=True)\n",
    "    \n",
    "    return X_train\n",
    "\n",
    "# Function for data preprocessing\n",
    "def prep_data(train, test, drop_sensors, remaining_sensors, alpha):\n",
    "    \n",
    "    X_train_interim = add_operating_condition(train.drop(drop_sensors, axis=1))\n",
    "    X_test_interim = add_operating_condition(test.drop(drop_sensors, axis=1))\n",
    "\n",
    "    X_train_interim, X_test_interim = condition_scaler(X_train_interim, X_test_interim, remaining_sensors)\n",
    "\n",
    "    X_train_interim = exponential_smoothing(X_train_interim, remaining_sensors, 0, alpha)\n",
    "    X_test_interim = exponential_smoothing(X_test_interim, remaining_sensors, 0, alpha)\n",
    "    \n",
    "    return X_train_interim, X_test_interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8510b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24720, 27) (16596, 26) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "train, test, y_test = prepare_data('FD003.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50','P30','Nf','Nc','Ps30','phi',\n",
    "                'NRf','NRc','BPR','htBleed','W31','W32'] # selection based on main_notebook\n",
    "\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "train = rul_piecewise_fct(train, 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "641f3298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [32]\t0.4\ttanh\t64\n",
    "# 0.3 125 40\n",
    "\n",
    "# Lower alpha's perform better, so we can ditch a few high ones to reduce the search space\n",
    "alpha_list = [0.01, 0.05] + list(np.arange(10,60+1,10)/100)\n",
    "\n",
    "sequence_list = list(np.arange(10,40+1,5))\n",
    "epoch_list = list(np.arange(5,20+1,5))\n",
    "nodes_list = [[32, 64], [32, 32]]\n",
    "\n",
    "# lowest dropout=0.1, because I know zero dropout will yield better training results but worse generalization\n",
    "dropouts = list(np.arange(1,3)/10)  \n",
    "\n",
    "# again, earlier testing revealed relu performed significantly worse, so I removed it from the options\n",
    "activation_functions = ['tanh', 'tanh']\n",
    "batch_size_list = [64]\n",
    "sensor_list = [sensor_names]\n",
    "\n",
    "tuning_options = np.prod([len(alpha_list),\n",
    "                          len(sequence_list),\n",
    "                          len(epoch_list),\n",
    "                          len(nodes_list),\n",
    "                          len(dropouts),\n",
    "                          len(activation_functions),\n",
    "                          len(batch_size_list),\n",
    "                          len(sensor_list)])\n",
    "tuning_options\n",
    "ITERATIONS = 10\n",
    "SEED = 0\n",
    "rul_piecewise = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76543ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 5s 11ms/step - loss: 4320.3857 - val_loss: 1603.5320\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1935.7366 - val_loss: 1651.4017\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1958.1536 - val_loss: 1681.0681\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1934.7556 - val_loss: 1674.3405\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1962.9840 - val_loss: 1644.9933\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1949.6104 - val_loss: 1669.2524\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1841.5752 - val_loss: 522.1207\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 584.3326 - val_loss: 282.4592\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 457.8391 - val_loss: 313.3980\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 432.6646 - val_loss: 233.8096\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 409.9556 - val_loss: 202.6940\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 397.7727 - val_loss: 194.2620\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 399.9516 - val_loss: 216.3073\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 398.2460 - val_loss: 214.0400\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 388.2622 - val_loss: 186.4006\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 383.9758 - val_loss: 204.2377\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 388.7852 - val_loss: 194.9149\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 389.6788 - val_loss: 181.7086\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 374.0822 - val_loss: 179.6743\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 379.5532 - val_loss: 188.2350\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 6s 14ms/step - loss: 4388.1792 - val_loss: 1607.7091\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1927.1199 - val_loss: 1563.2540\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1041.4926 - val_loss: 372.0458\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 503.9901 - val_loss: 330.9858\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 462.4536 - val_loss: 292.8080\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 427.4804 - val_loss: 293.2505\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 403.8519 - val_loss: 243.7886\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 399.7328 - val_loss: 224.5740\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 395.3336 - val_loss: 237.3156\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 384.6238 - val_loss: 204.8575\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 375.4496 - val_loss: 203.5688\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 375.4240 - val_loss: 201.9009\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 368.4622 - val_loss: 219.0802\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 356.2698 - val_loss: 241.3143\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 349.2662 - val_loss: 180.9525\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 341.9724 - val_loss: 178.2939\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 346.0081 - val_loss: 176.3538\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 346.2667 - val_loss: 167.6979\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 333.5434 - val_loss: 166.1623\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 336.8078 - val_loss: 179.9411\n",
      "4/4 [==============================] - 1s 4ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 6s 12ms/step - loss: 4261.6929 - val_loss: 1626.2585\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1930.1239 - val_loss: 1656.7251\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1927.4489 - val_loss: 1687.2794\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1922.3770 - val_loss: 1674.1178\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1411.3524 - val_loss: 443.7037\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 535.5559 - val_loss: 328.6052\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 450.0964 - val_loss: 274.4807\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 420.0580 - val_loss: 239.9967\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 408.4239 - val_loss: 248.1137\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 391.3945 - val_loss: 261.4843\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 395.2149 - val_loss: 225.2082\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 381.0325 - val_loss: 211.1795\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 380.3911 - val_loss: 193.0906\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 381.3152 - val_loss: 238.6265\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 377.6072 - val_loss: 183.0237\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 370.2252 - val_loss: 167.3655\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 374.5976 - val_loss: 196.9314\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 373.9914 - val_loss: 222.3621\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 359.7028 - val_loss: 179.4441\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 361.6096 - val_loss: 178.8331\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 6s 12ms/step - loss: 4194.8232 - val_loss: 1624.0898\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1935.4161 - val_loss: 1654.4784\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1927.3839 - val_loss: 1691.8666\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1924.0228 - val_loss: 1670.4912\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1928.7830 - val_loss: 1650.0884\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 903.2986 - val_loss: 367.3845\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 558.6417 - val_loss: 289.2798\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 512.6374 - val_loss: 246.0250\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 481.9187 - val_loss: 279.0737\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 447.7963 - val_loss: 234.9852\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 426.4953 - val_loss: 208.0483\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 400.8452 - val_loss: 233.2786\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 394.7445 - val_loss: 221.4072\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 380.7162 - val_loss: 313.3601\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 375.7410 - val_loss: 179.5484\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 372.1801 - val_loss: 167.8192\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 370.7813 - val_loss: 182.5359\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 374.4652 - val_loss: 241.3570\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 358.4996 - val_loss: 159.3961\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 352.1903 - val_loss: 171.8798\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 7s 14ms/step - loss: 3663.4966 - val_loss: 1638.8285\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2008.4108 - val_loss: 1628.7373\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2006.6224 - val_loss: 1670.1304\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1995.9514 - val_loss: 1647.9912\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2005.8400 - val_loss: 1637.7937\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1269.0255 - val_loss: 386.2832\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 564.2634 - val_loss: 275.6000\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 513.6091 - val_loss: 232.3111\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 501.5824 - val_loss: 259.7563\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 478.1079 - val_loss: 233.8975\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 482.6546 - val_loss: 238.0804\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 471.3431 - val_loss: 231.4141\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 455.6451 - val_loss: 212.8912\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 467.6369 - val_loss: 227.2643\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 455.0154 - val_loss: 242.2081\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 447.4611 - val_loss: 216.2075\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 451.6309 - val_loss: 214.1704\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 445.4212 - val_loss: 174.6517\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 432.5970 - val_loss: 157.7864\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 427.4081 - val_loss: 161.8308\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 6s 12ms/step - loss: 4089.0464 - val_loss: 1614.7887\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1954.3007 - val_loss: 1650.6084\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1944.3843 - val_loss: 1645.5641\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 798.7851 - val_loss: 324.5036\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 515.8591 - val_loss: 254.6147\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 468.3103 - val_loss: 261.1399\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 462.8862 - val_loss: 212.8776\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 433.1547 - val_loss: 203.7037\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 414.6969 - val_loss: 228.2511\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 411.4655 - val_loss: 184.2349\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 406.6591 - val_loss: 185.5789\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 395.7490 - val_loss: 179.7549\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 379.2222 - val_loss: 183.7959\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 377.6820 - val_loss: 199.4058\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 372.8715 - val_loss: 189.4407\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 358.4155 - val_loss: 180.5726\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 349.4010 - val_loss: 172.5483\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 348.9226 - val_loss: 159.2380\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 328.8517 - val_loss: 163.5486\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 331.9432 - val_loss: 173.8920\n",
      "4/4 [==============================] - 1s 4ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 7s 14ms/step - loss: 3902.9568 - val_loss: 1616.0212\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2066.1223 - val_loss: 1623.4116\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1692.8624 - val_loss: 411.1628\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 630.2055 - val_loss: 314.9036\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 575.7077 - val_loss: 383.0514\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 524.3254 - val_loss: 308.6027\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 528.8949 - val_loss: 255.7627\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 501.0234 - val_loss: 238.6130\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 504.4347 - val_loss: 255.5448\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 473.0366 - val_loss: 205.8764\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 465.8406 - val_loss: 216.3384\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 461.3294 - val_loss: 209.7301\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 448.3893 - val_loss: 196.5684\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 445.6951 - val_loss: 228.5060\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 451.3162 - val_loss: 220.1648\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 444.2309 - val_loss: 209.4775\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 438.5404 - val_loss: 190.3972\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 444.9061 - val_loss: 191.5877\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 430.3612 - val_loss: 182.0630\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 431.0364 - val_loss: 182.1088\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 7s 14ms/step - loss: 3679.3826 - val_loss: 1632.7616\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2001.2207 - val_loss: 1629.1436\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1805.7485 - val_loss: 415.8300\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 585.2484 - val_loss: 280.0146\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 523.7377 - val_loss: 270.1257\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 509.3197 - val_loss: 266.4965\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 492.7559 - val_loss: 253.3907\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 476.4068 - val_loss: 238.3237\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 478.2017 - val_loss: 236.4944\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 467.5830 - val_loss: 236.0472\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 473.0340 - val_loss: 227.6614\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 453.6840 - val_loss: 226.6964\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 454.2134 - val_loss: 209.8983\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 447.1226 - val_loss: 204.7836\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 443.5076 - val_loss: 199.1886\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 431.2164 - val_loss: 188.0043\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 444.0455 - val_loss: 231.8824\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 439.0331 - val_loss: 185.1865\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 423.7754 - val_loss: 177.2738\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 416.1537 - val_loss: 171.5666\n",
      "4/4 [==============================] - 1s 9ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 6s 12ms/step - loss: 5491.0889 - val_loss: 1604.6698\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2158.7844 - val_loss: 1620.2153\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2129.7834 - val_loss: 1629.9365\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2128.5144 - val_loss: 1630.4530\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2117.3176 - val_loss: 1585.2887\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2085.3818 - val_loss: 1031.9832\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 891.4056 - val_loss: 316.5473\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 634.9866 - val_loss: 258.6566\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 586.1781 - val_loss: 240.7979\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 574.6237 - val_loss: 232.8844\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 544.4902 - val_loss: 224.7888\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 501.0105 - val_loss: 213.4905\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 475.5019 - val_loss: 206.1191\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 473.0445 - val_loss: 233.3808\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 470.3342 - val_loss: 190.9618\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 469.1514 - val_loss: 190.3215\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 466.3286 - val_loss: 232.2862\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 462.2141 - val_loss: 181.7990\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 448.8329 - val_loss: 170.4339\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 457.3945 - val_loss: 174.2969\n",
      "4/4 [==============================] - 1s 4ms/step\n",
      "iteration  10\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 7s 16ms/step - loss: 3620.8545 - val_loss: 1636.5887\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1984.2371 - val_loss: 1630.6644\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1938.8573 - val_loss: 872.2028\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 633.7051 - val_loss: 293.9836\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 512.0084 - val_loss: 334.7192\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 474.9788 - val_loss: 256.3676\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 469.9812 - val_loss: 230.4587\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 436.1325 - val_loss: 216.9306\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 428.0957 - val_loss: 234.1261\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 422.5873 - val_loss: 213.6363\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 410.7797 - val_loss: 216.4514\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 401.0740 - val_loss: 218.4138\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 393.6200 - val_loss: 215.1966\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 395.8707 - val_loss: 226.8995\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 394.6661 - val_loss: 200.0010\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 390.1848 - val_loss: 198.1177\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 383.0710 - val_loss: 193.2558\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 394.5556 - val_loss: 202.7201\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 372.0289 - val_loss: 184.2236\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 380.4792 - val_loss: 194.0747\n",
      "4/4 [==============================] - 1s 6ms/step\n",
      "CPU times: total: 2min 15s\n",
      "Wall time: 13min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = pd.DataFrame(columns=['RMSE', 'std_RMSE', \n",
    "                                'S_score','std_S_score',\n",
    "                                'MSE', 'std_MSE',\n",
    "                                'nodes', 'dropout',\n",
    "                                'activation', 'batch_size'])\n",
    "\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    if ITERATIONS < 10:\n",
    "        print('iteration ', i+1)\n",
    "    elif ((i+1) % 10 == 0):\n",
    "        print('iteration ', i+1)    \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    \n",
    "# parameter's sample\n",
    "    alpha = 0.3\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = random.sample(nodes_list, 1)[0]\n",
    "    dropout = 0.3\n",
    "    activation = 'tanh'\n",
    "    batch_size = random.sample(batch_size_list, 1)[0]\n",
    "    remaining_sensors = remaining_sensors\n",
    "    drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = create_model2C(input_shape, nodes_per_layer, dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #       append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10246abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.721274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.834971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.830795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.098344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310.562840</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.566635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.110292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>288.555443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.879791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13.186811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>315.361035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.891953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.202157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>309.432383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174.296921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.372850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310.274731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.833084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.414212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>571.424493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179.941071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13.494769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>392.801972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.108765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.719877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>420.625730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>188.235016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13.931069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.297174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>194.074692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE  \\\n",
       "4  12.721274       0.0  252.834971          0.0  161.830795      0.0   \n",
       "7  13.098344       0.0  310.562840          0.0  171.566635      0.0   \n",
       "3  13.110292       0.0  288.555443          0.0  171.879791      0.0   \n",
       "5  13.186811       0.0  315.361035          0.0  173.891953      0.0   \n",
       "8  13.202157       0.0  309.432383          0.0  174.296921      0.0   \n",
       "2  13.372850       0.0  310.274731          0.0  178.833084      0.0   \n",
       "1  13.414212       0.0  571.424493          0.0  179.941071      0.0   \n",
       "6  13.494769       0.0  392.801972          0.0  182.108765      0.0   \n",
       "0  13.719877       0.0  420.625730          0.0  188.235016      0.0   \n",
       "9  13.931069       0.0  468.297174          0.0  194.074692      0.0   \n",
       "\n",
       "      nodes  dropout activation batch_size  \n",
       "4  [32, 64]      0.3       tanh         64  \n",
       "7  [32, 64]      0.3       tanh         64  \n",
       "3  [32, 32]      0.3       tanh         64  \n",
       "5  [32, 32]      0.3       tanh         64  \n",
       "8  [32, 32]      0.3       tanh         64  \n",
       "2  [32, 32]      0.3       tanh         64  \n",
       "1  [32, 32]      0.3       tanh         64  \n",
       "6  [32, 64]      0.3       tanh         64  \n",
       "0  [32, 32]      0.3       tanh         64  \n",
       "9  [32, 64]      0.3       tanh         64  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91b84233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 9s 20ms/step - loss: 5428.1226 - val_loss: 1622.2159\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2183.5127 - val_loss: 1622.1790\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2120.4612 - val_loss: 1625.1847\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2127.3665 - val_loss: 1628.2081\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2137.5244 - val_loss: 1554.7216\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2089.0591 - val_loss: 1544.2635\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 938.3187 - val_loss: 363.0216\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 622.8356 - val_loss: 263.5007\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 548.8660 - val_loss: 245.5668\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 522.1755 - val_loss: 217.8311\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 510.2955 - val_loss: 266.4294\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 495.6889 - val_loss: 226.6484\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 490.6336 - val_loss: 192.6598\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 488.6383 - val_loss: 238.6745\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 479.2209 - val_loss: 190.4737\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 448.5058 - val_loss: 186.0103\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 438.8813 - val_loss: 173.3739\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 445.6042 - val_loss: 180.7240\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 428.4898 - val_loss: 163.6122\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 432.2810 - val_loss: 155.8955\n",
      "4/4 [==============================] - 1s 4ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 10s 22ms/step - loss: 3187.5881 - val_loss: 1649.0027\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1934.4336 - val_loss: 1640.5389\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 1926.3109 - val_loss: 1700.4797\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 1097.1895 - val_loss: 320.0757\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 481.2273 - val_loss: 245.9184\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 425.5578 - val_loss: 272.1808\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 417.5139 - val_loss: 225.9369\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 387.0486 - val_loss: 169.1501\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 391.7025 - val_loss: 215.5595\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 374.2829 - val_loss: 191.0441\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 373.6335 - val_loss: 192.1153\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 360.3074 - val_loss: 211.8300\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 363.9770 - val_loss: 177.8470\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 363.7010 - val_loss: 190.6021\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 358.7829 - val_loss: 180.4181\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 353.8191 - val_loss: 168.6187\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 354.0457 - val_loss: 210.6188\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 351.9683 - val_loss: 167.5756\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 338.6133 - val_loss: 165.9814\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 345.3651 - val_loss: 171.7411\n",
      "4/4 [==============================] - 1s 5ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 10s 21ms/step - loss: 3264.4045 - val_loss: 1656.1340\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1940.2745 - val_loss: 1635.7623\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 1936.9437 - val_loss: 1693.2504\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1935.0298 - val_loss: 1672.5024\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1944.7947 - val_loss: 1656.1908\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1863.1915 - val_loss: 684.9352\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 608.2208 - val_loss: 290.1082\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 489.0024 - val_loss: 245.5714\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 432.7128 - val_loss: 256.6585\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 397.9424 - val_loss: 198.9734\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 389.0587 - val_loss: 213.3285\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 380.5957 - val_loss: 222.3899\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 382.7641 - val_loss: 205.9892\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 366.8279 - val_loss: 264.3799\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 367.6219 - val_loss: 175.5773\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 363.9660 - val_loss: 209.1619\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 362.1083 - val_loss: 170.0746\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 364.8910 - val_loss: 186.9464\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 346.4745 - val_loss: 158.6782\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 343.2450 - val_loss: 163.2909\n",
      "4/4 [==============================] - 1s 7ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 9s 19ms/step - loss: 5502.1450 - val_loss: 1560.2797\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2072.2014 - val_loss: 1634.3126\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2045.0000 - val_loss: 1642.2299\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2055.1843 - val_loss: 1644.5938\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2053.4377 - val_loss: 1629.6012\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1576.9238 - val_loss: 485.6302\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 651.6284 - val_loss: 252.1452\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 585.6522 - val_loss: 236.5660\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 529.2136 - val_loss: 260.0625\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 518.4119 - val_loss: 228.5668\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 506.0867 - val_loss: 230.4413\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 488.1976 - val_loss: 238.1102\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 465.0477 - val_loss: 212.2474\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 456.2965 - val_loss: 217.4640\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 460.9211 - val_loss: 198.9058\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 450.6388 - val_loss: 159.7985\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 439.7634 - val_loss: 200.4164\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 431.1199 - val_loss: 202.3487\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 421.5458 - val_loss: 154.7528\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 418.5139 - val_loss: 182.7572\n",
      "4/4 [==============================] - 1s 5ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 9s 19ms/step - loss: 4573.3213 - val_loss: 1588.1735\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1929.5408 - val_loss: 1658.8538\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1933.5272 - val_loss: 1681.2175\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1925.7443 - val_loss: 1673.2152\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1951.6387 - val_loss: 1648.1965\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1932.7410 - val_loss: 1671.6494\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1918.5024 - val_loss: 1640.9006\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1940.8857 - val_loss: 1634.5123\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1929.2096 - val_loss: 1664.6481\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1931.2008 - val_loss: 1670.2428\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1578.5137 - val_loss: 664.4667\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 580.9086 - val_loss: 307.8849\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 471.4017 - val_loss: 294.8012\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 428.8257 - val_loss: 244.0612\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 411.0076 - val_loss: 211.4734\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 403.9212 - val_loss: 204.6118\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 404.2400 - val_loss: 232.4200\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 405.5889 - val_loss: 229.5137\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 390.2228 - val_loss: 181.4462\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 394.4650 - val_loss: 187.9909\n",
      "4/4 [==============================] - 1s 4ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 9s 21ms/step - loss: 4231.2578 - val_loss: 1597.7936\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 2083.4946 - val_loss: 1628.1672\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2078.8708 - val_loss: 1651.0750\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2078.9277 - val_loss: 1624.7638\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 2087.2666 - val_loss: 1614.9027\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 2085.5920 - val_loss: 1635.3833\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 2081.3728 - val_loss: 1612.9900\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2092.1870 - val_loss: 1610.7238\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1656.5391 - val_loss: 440.0321\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 621.6660 - val_loss: 265.0539\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 550.9598 - val_loss: 221.4065\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 496.1724 - val_loss: 209.3081\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 478.1660 - val_loss: 197.2525\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 467.7963 - val_loss: 225.8170\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 456.7200 - val_loss: 199.1395\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 445.9591 - val_loss: 188.9548\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 439.0584 - val_loss: 174.4507\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 436.2457 - val_loss: 161.3578\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 410.9353 - val_loss: 138.6929\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 419.3289 - val_loss: 144.7906\n",
      "4/4 [==============================] - 1s 6ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 10s 21ms/step - loss: 3500.1072 - val_loss: 1648.7947\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1955.8456 - val_loss: 1649.8931\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1975.0088 - val_loss: 1682.4487\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1799.0175 - val_loss: 422.0493\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 567.1324 - val_loss: 261.3731\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 489.6955 - val_loss: 287.8336\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 469.2276 - val_loss: 238.5224\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 430.7061 - val_loss: 226.7135\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 403.7464 - val_loss: 249.4906\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 392.2466 - val_loss: 195.3773\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 379.7713 - val_loss: 198.7001\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 364.9521 - val_loss: 189.5712\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 362.0889 - val_loss: 180.5342\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 365.4028 - val_loss: 188.3628\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 361.8061 - val_loss: 190.6854\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 351.1418 - val_loss: 157.4473\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 348.5188 - val_loss: 142.6199\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 355.6289 - val_loss: 165.0698\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 337.5339 - val_loss: 154.2004\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 331.2101 - val_loss: 167.7711\n",
      "4/4 [==============================] - 1s 5ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 9s 19ms/step - loss: 4626.8550 - val_loss: 1567.3153\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1958.4443 - val_loss: 1651.1986\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1965.4586 - val_loss: 1675.9525\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1950.1256 - val_loss: 1668.5670\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1973.9030 - val_loss: 1644.6517\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1965.8511 - val_loss: 1667.9159\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1956.5996 - val_loss: 1629.1150\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1967.0107 - val_loss: 1632.3654\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1947.8785 - val_loss: 1650.7290\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 933.2377 - val_loss: 252.0487\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 455.3217 - val_loss: 339.6790\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 435.6120 - val_loss: 218.9432\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 431.4936 - val_loss: 197.7405\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 408.4448 - val_loss: 258.3282\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 398.7447 - val_loss: 199.9878\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 390.8427 - val_loss: 203.2439\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 393.8933 - val_loss: 239.4052\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 392.2873 - val_loss: 258.4066\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 376.3412 - val_loss: 176.8251\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 385.5266 - val_loss: 179.9023\n",
      "4/4 [==============================] - 1s 5ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 9s 19ms/step - loss: 4528.8652 - val_loss: 1574.8313\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1972.1031 - val_loss: 1652.1819\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1971.0223 - val_loss: 1679.2314\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1947.6052 - val_loss: 1660.9438\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1961.4727 - val_loss: 1638.2544\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1952.7704 - val_loss: 1654.6837\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1953.0968 - val_loss: 1646.5972\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1964.1882 - val_loss: 1635.7959\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1863.8932 - val_loss: 696.3658\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 563.4742 - val_loss: 266.5698\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 468.9793 - val_loss: 298.3089\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 439.2822 - val_loss: 253.4910\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 421.5839 - val_loss: 245.2339\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 412.9085 - val_loss: 263.6548\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 399.0568 - val_loss: 188.8400\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 392.9320 - val_loss: 194.5374\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 387.3953 - val_loss: 193.6682\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 386.2233 - val_loss: 192.7866\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 376.7440 - val_loss: 178.1929\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 378.7990 - val_loss: 182.1141\n",
      "4/4 [==============================] - 1s 4ms/step\n",
      "iteration  10\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 10s 22ms/step - loss: 3750.4053 - val_loss: 1607.2272\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2093.6208 - val_loss: 1611.0564\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2085.0508 - val_loss: 1643.7247\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 2080.2664 - val_loss: 1622.4503\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2088.1294 - val_loss: 1617.8716\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2064.3203 - val_loss: 1570.9003\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 779.5885 - val_loss: 298.2168\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 525.9620 - val_loss: 238.4469\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 472.8507 - val_loss: 291.7263\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 459.5854 - val_loss: 223.4536\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 459.0704 - val_loss: 209.8460\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 445.7043 - val_loss: 192.0067\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 448.7753 - val_loss: 231.0122\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 439.5487 - val_loss: 207.5111\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 439.1847 - val_loss: 192.2019\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 433.0021 - val_loss: 193.3180\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 422.9373 - val_loss: 187.3670\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 432.8922 - val_loss: 178.3224\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 403.3324 - val_loss: 153.6845\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 390.8552 - val_loss: 155.5583\n",
      "4/4 [==============================] - 1s 5ms/step\n",
      "CPU times: total: 5min 53s\n",
      "Wall time: 21min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = pd.DataFrame(columns=['RMSE', 'std_RMSE', \n",
    "                                'S_score','std_S_score',\n",
    "                                'MSE', 'std_MSE',\n",
    "                                'nodes', 'dropout',\n",
    "                                'activation', 'batch_size'])\n",
    "\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    if ITERATIONS < 10:\n",
    "        print('iteration ', i+1)\n",
    "    elif ((i+1) % 10 == 0):\n",
    "        print('iteration ', i+1)    \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    \n",
    "# parameter's sample\n",
    "    alpha = 0.3\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = random.sample(nodes_list, 1)[0]\n",
    "    dropout = 0.3\n",
    "    activation = 'tanh'\n",
    "    batch_size = random.sample(batch_size_list, 1)[0]\n",
    "    remaining_sensors = remaining_sensors\n",
    "    drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = create_model3C(input_shape, nodes_per_layer, dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #       append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ba3b3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.032895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>235.421067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.790588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.472302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>279.885393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155.558319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.485813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>299.417289</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155.895523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.778531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>307.900036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.290878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12.952646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>397.209433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>167.771057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.105003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>353.504548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.741119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.412764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>254.972358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179.902252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.494967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>426.002984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.114136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.518773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>518.512123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.757233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.710979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>333.747896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.990936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE  \\\n",
       "5  12.032895       0.0  235.421067          0.0  144.790588      0.0   \n",
       "9  12.472302       0.0  279.885393          0.0  155.558319      0.0   \n",
       "0  12.485813       0.0  299.417289          0.0  155.895523      0.0   \n",
       "2  12.778531       0.0  307.900036          0.0  163.290878      0.0   \n",
       "6  12.952646       0.0  397.209433          0.0  167.771057      0.0   \n",
       "1  13.105003       0.0  353.504548          0.0  171.741119      0.0   \n",
       "7  13.412764       0.0  254.972358          0.0  179.902252      0.0   \n",
       "8  13.494967       0.0  426.002984          0.0  182.114136      0.0   \n",
       "3  13.518773       0.0  518.512123          0.0  182.757233      0.0   \n",
       "4  13.710979       0.0  333.747896          0.0  187.990936      0.0   \n",
       "\n",
       "      nodes  dropout activation batch_size  \n",
       "5  [32, 64]      0.3       tanh         64  \n",
       "9  [32, 64]      0.3       tanh         64  \n",
       "0  [32, 32]      0.3       tanh         64  \n",
       "2  [32, 64]      0.3       tanh         64  \n",
       "6  [32, 64]      0.3       tanh         64  \n",
       "1  [32, 64]      0.3       tanh         64  \n",
       "7  [32, 32]      0.3       tanh         64  \n",
       "8  [32, 32]      0.3       tanh         64  \n",
       "3  [32, 32]      0.3       tanh         64  \n",
       "4  [32, 32]      0.3       tanh         64  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6473c494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 12s 26ms/step - loss: 3145.7966 - val_loss: 1656.7677\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 7s 23ms/step - loss: 1897.3990 - val_loss: 1616.4978\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 7s 23ms/step - loss: 1899.6655 - val_loss: 1700.0643\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 7s 23ms/step - loss: 1874.0341 - val_loss: 1661.0127\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 7s 23ms/step - loss: 1883.7297 - val_loss: 1693.8285\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1888.5500 - val_loss: 1652.5763\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 1519.4968 - val_loss: 558.3528\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 514.8961 - val_loss: 356.6377\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 398.9040 - val_loss: 340.5733\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 373.7055 - val_loss: 223.9701\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 353.1175 - val_loss: 219.2968\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 346.3751 - val_loss: 202.8939\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 336.4723 - val_loss: 178.0299\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 326.3746 - val_loss: 222.0858\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 320.8804 - val_loss: 190.8582\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 322.5943 - val_loss: 211.2305\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 321.6345 - val_loss: 202.3131\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 310.5478 - val_loss: 212.0570\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 308.0055 - val_loss: 187.6609\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 314.0660 - val_loss: 180.5516\n",
      "4/4 [==============================] - 1s 6ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 13s 27ms/step - loss: 3546.2766 - val_loss: 1658.6725\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1893.3462 - val_loss: 1622.0570\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 1907.2347 - val_loss: 1686.4703\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 1885.2738 - val_loss: 1671.3276\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1893.7111 - val_loss: 1677.4703\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1899.6719 - val_loss: 1654.5569\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1601.8807 - val_loss: 523.5393\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 499.6178 - val_loss: 266.1075\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 405.3626 - val_loss: 246.4770\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 388.5509 - val_loss: 201.7580\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 376.6397 - val_loss: 197.9793\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 370.0694 - val_loss: 215.6906\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 353.9302 - val_loss: 181.4529\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 350.4882 - val_loss: 176.9704\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 355.1069 - val_loss: 165.6442\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 342.1610 - val_loss: 178.8598\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 339.4270 - val_loss: 219.2377\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 344.8941 - val_loss: 183.9648\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 323.7102 - val_loss: 159.7393\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 327.5825 - val_loss: 161.4853\n",
      "4/4 [==============================] - 1s 7ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 13s 28ms/step - loss: 3106.4211 - val_loss: 1645.8210\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 1875.2123 - val_loss: 1610.7668\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1865.0995 - val_loss: 1703.1798\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1848.7415 - val_loss: 1676.0647\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1503.2014 - val_loss: 529.7527\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 409.8414 - val_loss: 253.9640\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 345.1230 - val_loss: 269.4869\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 320.7059 - val_loss: 194.0126\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 310.7178 - val_loss: 232.2937\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 313.1681 - val_loss: 175.6450\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 301.6914 - val_loss: 172.8881\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 307.7925 - val_loss: 209.2333\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 294.8838 - val_loss: 166.2670\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 285.5280 - val_loss: 160.9107\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 288.7240 - val_loss: 158.4627\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 288.2545 - val_loss: 152.7783\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 275.9986 - val_loss: 173.4903\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 288.1811 - val_loss: 273.8860\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 276.7641 - val_loss: 150.7356\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 281.7551 - val_loss: 145.5659\n",
      "4/4 [==============================] - 1s 6ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 12s 27ms/step - loss: 3274.8127 - val_loss: 1649.0437\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1910.1936 - val_loss: 1613.1458\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1897.7252 - val_loss: 1721.0121\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1884.0543 - val_loss: 1693.2816\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1887.5239 - val_loss: 1687.2234\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1880.5758 - val_loss: 1661.3728\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1105.7960 - val_loss: 393.6003\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 449.4514 - val_loss: 350.8771\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 400.9722 - val_loss: 306.3844\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 370.0747 - val_loss: 213.8140\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 356.5793 - val_loss: 245.2369\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 347.3502 - val_loss: 207.4154\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 332.7973 - val_loss: 195.7476\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 333.0074 - val_loss: 206.3141\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 327.9747 - val_loss: 171.7006\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 321.1220 - val_loss: 162.7855\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 309.6513 - val_loss: 170.0535\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 311.2140 - val_loss: 155.8610\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 303.7906 - val_loss: 150.3411\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 297.3315 - val_loss: 147.5639\n",
      "4/4 [==============================] - 1s 6ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 13s 28ms/step - loss: 3192.2166 - val_loss: 1667.6338\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1863.0338 - val_loss: 1613.6887\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 1799.9097 - val_loss: 1879.3916\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1756.6522 - val_loss: 1566.4949\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 695.5692 - val_loss: 447.5283\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 380.8112 - val_loss: 266.8580\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 339.6544 - val_loss: 313.4918\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 318.9930 - val_loss: 234.1075\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 309.8767 - val_loss: 382.8555\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 307.8699 - val_loss: 203.2208\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 290.4903 - val_loss: 180.2926\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 291.3924 - val_loss: 214.8510\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 282.3680 - val_loss: 160.4516\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 278.1371 - val_loss: 216.6760\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 273.4102 - val_loss: 172.0111\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 274.9204 - val_loss: 175.0868\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 260.1303 - val_loss: 188.2478\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 268.5768 - val_loss: 199.6773\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 251.9048 - val_loss: 146.0733\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 252.7995 - val_loss: 148.1565\n",
      "4/4 [==============================] - 1s 8ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 13s 27ms/step - loss: 3230.7192 - val_loss: 1642.5693\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1897.6555 - val_loss: 1607.9279\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 1890.1044 - val_loss: 1723.3215\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1477.5195 - val_loss: 440.6593\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 427.0853 - val_loss: 308.0821\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 358.0055 - val_loss: 240.8150\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 346.7887 - val_loss: 208.5028\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 315.1639 - val_loss: 186.2066\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 311.2315 - val_loss: 251.0503\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 310.4072 - val_loss: 173.2005\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 295.7196 - val_loss: 188.3660\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 290.1336 - val_loss: 242.6301\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 291.5669 - val_loss: 154.6222\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 281.7565 - val_loss: 169.2165\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 280.4743 - val_loss: 147.8780\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 280.9809 - val_loss: 151.6209\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 270.6048 - val_loss: 145.6631\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 278.8292 - val_loss: 154.6045\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 269.1133 - val_loss: 121.7461\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 271.0100 - val_loss: 155.8261\n",
      "4/4 [==============================] - 1s 7ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 13s 29ms/step - loss: 3564.4451 - val_loss: 1638.5853\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 1936.8282 - val_loss: 1617.2069\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 1926.5618 - val_loss: 1691.9984\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 1919.8386 - val_loss: 1681.8634\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1934.4583 - val_loss: 1668.6340\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 1917.5712 - val_loss: 1648.9878\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 1259.1554 - val_loss: 368.6234\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 438.4098 - val_loss: 228.4772\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 409.8346 - val_loss: 343.4692\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 395.4232 - val_loss: 190.3988\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 380.2546 - val_loss: 235.1391\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 380.9492 - val_loss: 205.1910\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 361.5976 - val_loss: 171.6267\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 345.5686 - val_loss: 211.5993\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 342.5930 - val_loss: 175.2971\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 338.4197 - val_loss: 156.3208\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 331.1963 - val_loss: 185.5568\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 333.3142 - val_loss: 151.5670\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 313.5723 - val_loss: 160.0983\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 313.6623 - val_loss: 147.4415\n",
      "4/4 [==============================] - 1s 10ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 12s 28ms/step - loss: 2983.5161 - val_loss: 1651.0787\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 1866.6277 - val_loss: 1611.0747\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 1855.2454 - val_loss: 1703.6072\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 1249.1727 - val_loss: 310.5629\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 390.8566 - val_loss: 359.3413\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 342.4123 - val_loss: 296.2298\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 337.0644 - val_loss: 222.5744\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 321.2999 - val_loss: 201.5412\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 315.0492 - val_loss: 316.9352\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 311.0050 - val_loss: 188.1186\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 294.9536 - val_loss: 184.0744\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 290.8273 - val_loss: 193.7877\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 283.0636 - val_loss: 158.0977\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 273.4030 - val_loss: 190.8047\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 268.2484 - val_loss: 165.9317\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 267.8839 - val_loss: 141.4692\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 263.5607 - val_loss: 198.6465\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 264.7947 - val_loss: 231.8361\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 254.7757 - val_loss: 160.1935\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 246.9267 - val_loss: 146.6753\n",
      "4/4 [==============================] - 2s 11ms/step\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 12s 28ms/step - loss: 3035.7639 - val_loss: 1658.3085\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 1854.1792 - val_loss: 1604.7969\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1854.2029 - val_loss: 1706.8365\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 1657.7852 - val_loss: 434.2251\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 425.8950 - val_loss: 281.7486\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 355.0206 - val_loss: 262.5145\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 331.4206 - val_loss: 259.5923\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 312.0768 - val_loss: 231.3144\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 303.1017 - val_loss: 317.1917\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 303.2363 - val_loss: 221.3603\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 297.0713 - val_loss: 244.0852\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 285.1484 - val_loss: 214.3187\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 276.7050 - val_loss: 173.8143\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 273.4552 - val_loss: 213.4471\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 272.2316 - val_loss: 183.2685\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 261.2669 - val_loss: 137.2950\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 252.1930 - val_loss: 137.1574\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 254.6840 - val_loss: 163.7516\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 247.7512 - val_loss: 135.3758\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 245.4132 - val_loss: 141.3164\n",
      "4/4 [==============================] - 1s 8ms/step\n",
      "iteration  10\n",
      "(20820, 40, 14) (20820, 1) (100, 40, 14)\n",
      "Epoch 1/20\n",
      "326/326 [==============================] - 13s 30ms/step - loss: 3209.2778 - val_loss: 1658.1882\n",
      "Epoch 2/20\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 1892.9933 - val_loss: 1617.9720\n",
      "Epoch 3/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1879.5587 - val_loss: 1719.2556\n",
      "Epoch 4/20\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 1860.3320 - val_loss: 1674.5072\n",
      "Epoch 5/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1877.7883 - val_loss: 1681.9481\n",
      "Epoch 6/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1883.4884 - val_loss: 1656.4418\n",
      "Epoch 7/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1870.5764 - val_loss: 1617.7594\n",
      "Epoch 8/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 937.3971 - val_loss: 317.5911\n",
      "Epoch 9/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 385.7858 - val_loss: 320.0327\n",
      "Epoch 10/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 346.9103 - val_loss: 214.0153\n",
      "Epoch 11/20\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 333.1737 - val_loss: 221.4580\n",
      "Epoch 12/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 327.8941 - val_loss: 228.0757\n",
      "Epoch 13/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 316.2196 - val_loss: 213.2479\n",
      "Epoch 14/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 315.4431 - val_loss: 240.1137\n",
      "Epoch 15/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 305.3390 - val_loss: 204.8106\n",
      "Epoch 16/20\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 302.6025 - val_loss: 224.5632\n",
      "Epoch 17/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 294.3785 - val_loss: 230.6005\n",
      "Epoch 18/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 296.8898 - val_loss: 178.3512\n",
      "Epoch 19/20\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 286.1193 - val_loss: 148.1529\n",
      "Epoch 20/20\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 278.4137 - val_loss: 166.0425\n",
      "4/4 [==============================] - 1s 8ms/step\n",
      "CPU times: total: 5min 44s\n",
      "Wall time: 29min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = pd.DataFrame(columns=['RMSE', 'std_RMSE', \n",
    "                                'S_score','std_S_score',\n",
    "                                'MSE', 'std_MSE',\n",
    "                                'nodes', 'dropout',\n",
    "                                'activation', 'batch_size'])\n",
    "\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    if ITERATIONS < 10:\n",
    "        print('iteration ', i+1)\n",
    "    elif ((i+1) % 10 == 0):\n",
    "        print('iteration ', i+1)    \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    \n",
    "# parameter's sample\n",
    "    alpha = 0.3\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = random.sample(nodes_list, 1)[0]\n",
    "    dropout = 0.3\n",
    "    activation = 'tanh'\n",
    "    batch_size = random.sample(batch_size_list, 1)[0]\n",
    "    remaining_sensors = remaining_sensors\n",
    "    drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = create_model4C(input_shape, nodes_per_layer, dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #       append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72509e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11.887657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>246.921235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.316360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.065069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>209.999264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>145.565903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.110960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.332510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>146.675339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12.142549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>257.206619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>147.441483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.147590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>264.618308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>147.563919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.171956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>273.558709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>148.156540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.483033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>308.818673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155.826126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.707685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>230.847180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.485260</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.885747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>307.268455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.042465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.436948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>267.266505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.551605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE  \\\n",
       "8  11.887657       0.0  246.921235          0.0  141.316360      0.0   \n",
       "2  12.065069       0.0  209.999264          0.0  145.565903      0.0   \n",
       "7  12.110960       0.0  205.332510          0.0  146.675339      0.0   \n",
       "6  12.142549       0.0  257.206619          0.0  147.441483      0.0   \n",
       "3  12.147590       0.0  264.618308          0.0  147.563919      0.0   \n",
       "4  12.171956       0.0  273.558709          0.0  148.156540      0.0   \n",
       "5  12.483033       0.0  308.818673          0.0  155.826126      0.0   \n",
       "1  12.707685       0.0  230.847180          0.0  161.485260      0.0   \n",
       "9  12.885747       0.0  307.268455          0.0  166.042465      0.0   \n",
       "0  13.436948       0.0  267.266505          0.0  180.551605      0.0   \n",
       "\n",
       "      nodes  dropout activation batch_size  \n",
       "8  [32, 32]      0.3       tanh         64  \n",
       "2  [32, 64]      0.3       tanh         64  \n",
       "7  [32, 32]      0.3       tanh         64  \n",
       "6  [32, 32]      0.3       tanh         64  \n",
       "3  [32, 32]      0.3       tanh         64  \n",
       "4  [32, 32]      0.3       tanh         64  \n",
       "5  [32, 32]      0.3       tanh         64  \n",
       "1  [32, 32]      0.3       tanh         64  \n",
       "9  [32, 32]      0.3       tanh         64  \n",
       "0  [32, 64]      0.3       tanh         64  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba36a16",
   "metadata": {},
   "source": [
    "#### Model selection <a id = \"fd003modelselect\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9404c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 8ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "iteration  10\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 1s 15ms/step\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "4/4 [==============================] - 1s 8ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "iteration  20\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 1s 8ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "4/4 [==============================] - 1s 7ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "iteration  30\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "iteration  40\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "iteration  50\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "CPU times: total: 25min 11s\n",
      "Wall time: 2h 28min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = pd.DataFrame(columns=['RMSE', 'std_RMSE', \n",
    "                                'S_score','std_S_score',\n",
    "                                'MSE', 'std_MSE',\n",
    "                                'nodes', 'dropout',\n",
    "                                'activation', 'batch_size'])\n",
    "\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    if ITERATIONS < 10:\n",
    "        print('iteration ', i+1)\n",
    "    elif ((i+1) % 10 == 0):\n",
    "        print('iteration ', i+1)\n",
    "    \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "\n",
    "    # parameter's sample\n",
    "    alpha = 0.3\n",
    "    sequence_length = 40\n",
    "    epochs = 15\n",
    "    nodes_per_layer = random.sample(nodes_list, 1)[0]\n",
    "    dropout = random.sample(dropouts, 1)[0]\n",
    "    activation = random.sample(activation_functions, 1)[0]\n",
    "    batch_size = random.sample(batch_size_list, 1)[0]\n",
    "    remaining_sensors = remaining_sensors\n",
    "    drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "    \n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = create_model(input_shape, nodes_per_layer, dropout, activation)\n",
    "    \n",
    "    # create train-val split\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "           for unit_nr in X_test_interim['Unit'].unique())\n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "        \n",
    "    gss = GroupShuffleSplit(n_splits=3, train_size=0.80, random_state=0)\n",
    "    for train_unit, val_unit in gss.split(X_train_interim['Unit'].unique(), groups=X_train_interim['Unit'].unique()):\n",
    "        train_unit = X_train_interim['Unit'].unique()[train_unit]  # gss returns indexes and index starts at 1\n",
    "        train_split_array = gen_data_wrapper(X_train_interim, sequence_length, remaining_sensors, train_unit)\n",
    "        train_split_label = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'], train_unit)\n",
    "        \n",
    "        val_unit = X_train_interim['Unit'].unique()[val_unit]\n",
    "        val_split_array = gen_data_wrapper(X_train_interim, sequence_length, remaining_sensors, val_unit)\n",
    "        val_split_label = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'], val_unit)\n",
    "        \n",
    "        # train and evaluate model\n",
    "        history = model.fit(train_split_array, train_split_label,\n",
    "                            validation_data=(val_split_array, val_split_label),\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            callbacks=[cb],\n",
    "                            verbose=0)\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "        \n",
    "        test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "    \n",
    "    # append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c8a8639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17.497979</td>\n",
       "      <td>3.562215</td>\n",
       "      <td>825.774870</td>\n",
       "      <td>401.682215</td>\n",
       "      <td>400.173126</td>\n",
       "      <td>298.613398</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>16.745512</td>\n",
       "      <td>0.604739</td>\n",
       "      <td>965.182981</td>\n",
       "      <td>367.017037</td>\n",
       "      <td>193.470510</td>\n",
       "      <td>66.504243</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16.503820</td>\n",
       "      <td>2.025543</td>\n",
       "      <td>1006.692206</td>\n",
       "      <td>529.569084</td>\n",
       "      <td>190.939529</td>\n",
       "      <td>69.731883</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>17.605326</td>\n",
       "      <td>0.619697</td>\n",
       "      <td>1112.880867</td>\n",
       "      <td>39.172785</td>\n",
       "      <td>326.713832</td>\n",
       "      <td>226.434678</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18.371744</td>\n",
       "      <td>1.790792</td>\n",
       "      <td>1142.275679</td>\n",
       "      <td>218.889242</td>\n",
       "      <td>364.033732</td>\n",
       "      <td>287.240205</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>18.700393</td>\n",
       "      <td>2.609535</td>\n",
       "      <td>1161.005507</td>\n",
       "      <td>636.561197</td>\n",
       "      <td>370.646434</td>\n",
       "      <td>205.968875</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.011934</td>\n",
       "      <td>2.814851</td>\n",
       "      <td>1263.985011</td>\n",
       "      <td>916.279482</td>\n",
       "      <td>215.209880</td>\n",
       "      <td>101.440674</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>17.625022</td>\n",
       "      <td>1.263987</td>\n",
       "      <td>1487.148969</td>\n",
       "      <td>48.503773</td>\n",
       "      <td>228.362208</td>\n",
       "      <td>120.898029</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19.126311</td>\n",
       "      <td>2.110756</td>\n",
       "      <td>1665.596473</td>\n",
       "      <td>449.880854</td>\n",
       "      <td>232.322617</td>\n",
       "      <td>91.951862</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>29.325745</td>\n",
       "      <td>9.705228</td>\n",
       "      <td>2807.163994</td>\n",
       "      <td>2002.520789</td>\n",
       "      <td>1359.117264</td>\n",
       "      <td>1006.017335</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>23.095542</td>\n",
       "      <td>9.421141</td>\n",
       "      <td>4379.919678</td>\n",
       "      <td>5407.203321</td>\n",
       "      <td>622.112859</td>\n",
       "      <td>552.379638</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26.392097</td>\n",
       "      <td>9.130650</td>\n",
       "      <td>7546.435694</td>\n",
       "      <td>7709.062278</td>\n",
       "      <td>720.886286</td>\n",
       "      <td>673.245429</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24.769750</td>\n",
       "      <td>11.028122</td>\n",
       "      <td>8120.346761</td>\n",
       "      <td>10031.896243</td>\n",
       "      <td>740.888097</td>\n",
       "      <td>710.912814</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>24.398759</td>\n",
       "      <td>10.564490</td>\n",
       "      <td>8286.917443</td>\n",
       "      <td>9814.929095</td>\n",
       "      <td>614.265757</td>\n",
       "      <td>628.087601</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>24.441491</td>\n",
       "      <td>11.277916</td>\n",
       "      <td>8361.951758</td>\n",
       "      <td>10133.429452</td>\n",
       "      <td>702.232931</td>\n",
       "      <td>735.094165</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>24.292283</td>\n",
       "      <td>10.925982</td>\n",
       "      <td>8533.229048</td>\n",
       "      <td>11136.027635</td>\n",
       "      <td>623.655436</td>\n",
       "      <td>651.057081</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.496920</td>\n",
       "      <td>10.015116</td>\n",
       "      <td>9815.157938</td>\n",
       "      <td>8726.546643</td>\n",
       "      <td>1543.873779</td>\n",
       "      <td>940.089034</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>24.219685</td>\n",
       "      <td>12.090146</td>\n",
       "      <td>10898.724662</td>\n",
       "      <td>14318.843285</td>\n",
       "      <td>670.551412</td>\n",
       "      <td>722.170198</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>43.701816</td>\n",
       "      <td>5.195296</td>\n",
       "      <td>11330.866388</td>\n",
       "      <td>4283.397028</td>\n",
       "      <td>2937.274577</td>\n",
       "      <td>1018.155713</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>37.354193</td>\n",
       "      <td>4.081427</td>\n",
       "      <td>11360.469851</td>\n",
       "      <td>8188.081295</td>\n",
       "      <td>1723.544189</td>\n",
       "      <td>556.301155</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>24.227292</td>\n",
       "      <td>12.241211</td>\n",
       "      <td>11688.374155</td>\n",
       "      <td>15281.609160</td>\n",
       "      <td>690.193242</td>\n",
       "      <td>704.961146</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>25.646023</td>\n",
       "      <td>11.132737</td>\n",
       "      <td>11704.781574</td>\n",
       "      <td>14895.488752</td>\n",
       "      <td>733.367716</td>\n",
       "      <td>656.771684</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.594619</td>\n",
       "      <td>11.942162</td>\n",
       "      <td>11795.221873</td>\n",
       "      <td>15182.471734</td>\n",
       "      <td>679.291280</td>\n",
       "      <td>699.559097</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24.864661</td>\n",
       "      <td>11.826517</td>\n",
       "      <td>11807.551516</td>\n",
       "      <td>15377.039205</td>\n",
       "      <td>693.166280</td>\n",
       "      <td>700.378040</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>24.937354</td>\n",
       "      <td>11.748832</td>\n",
       "      <td>11807.782297</td>\n",
       "      <td>15334.189330</td>\n",
       "      <td>694.724798</td>\n",
       "      <td>701.520493</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>26.464920</td>\n",
       "      <td>10.795586</td>\n",
       "      <td>11860.579275</td>\n",
       "      <td>15207.862708</td>\n",
       "      <td>808.737396</td>\n",
       "      <td>635.561485</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25.471285</td>\n",
       "      <td>11.416411</td>\n",
       "      <td>12005.732445</td>\n",
       "      <td>15456.828492</td>\n",
       "      <td>668.215520</td>\n",
       "      <td>719.926001</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>25.293153</td>\n",
       "      <td>11.516187</td>\n",
       "      <td>12101.026358</td>\n",
       "      <td>15248.684488</td>\n",
       "      <td>718.063202</td>\n",
       "      <td>681.933139</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>25.099941</td>\n",
       "      <td>11.695238</td>\n",
       "      <td>12108.065325</td>\n",
       "      <td>15580.112495</td>\n",
       "      <td>713.100601</td>\n",
       "      <td>687.626129</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.148993</td>\n",
       "      <td>5.650175</td>\n",
       "      <td>12549.591689</td>\n",
       "      <td>5181.409746</td>\n",
       "      <td>2994.281250</td>\n",
       "      <td>1104.139331</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>44.226517</td>\n",
       "      <td>5.726926</td>\n",
       "      <td>12700.091277</td>\n",
       "      <td>5322.964460</td>\n",
       "      <td>3006.863118</td>\n",
       "      <td>1116.986012</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.650993</td>\n",
       "      <td>6.144368</td>\n",
       "      <td>12940.459855</td>\n",
       "      <td>6082.571791</td>\n",
       "      <td>3099.312703</td>\n",
       "      <td>1169.493138</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.413085</td>\n",
       "      <td>10.533023</td>\n",
       "      <td>18486.133544</td>\n",
       "      <td>12226.266101</td>\n",
       "      <td>1247.956136</td>\n",
       "      <td>652.101355</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>40.667191</td>\n",
       "      <td>0.495222</td>\n",
       "      <td>19747.666920</td>\n",
       "      <td>10710.124251</td>\n",
       "      <td>1977.917562</td>\n",
       "      <td>345.778755</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>40.616117</td>\n",
       "      <td>0.548840</td>\n",
       "      <td>19943.950670</td>\n",
       "      <td>10738.956499</td>\n",
       "      <td>1960.101440</td>\n",
       "      <td>323.043852</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>40.661682</td>\n",
       "      <td>0.620684</td>\n",
       "      <td>20326.275299</td>\n",
       "      <td>11283.968118</td>\n",
       "      <td>1962.383138</td>\n",
       "      <td>325.146269</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>40.750060</td>\n",
       "      <td>0.693425</td>\n",
       "      <td>21098.413062</td>\n",
       "      <td>12039.431608</td>\n",
       "      <td>1962.384766</td>\n",
       "      <td>329.422304</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>33.513987</td>\n",
       "      <td>11.098186</td>\n",
       "      <td>21524.973895</td>\n",
       "      <td>14329.986473</td>\n",
       "      <td>1218.739583</td>\n",
       "      <td>660.728350</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>33.142934</td>\n",
       "      <td>11.781266</td>\n",
       "      <td>22104.488041</td>\n",
       "      <td>15047.489793</td>\n",
       "      <td>1181.934189</td>\n",
       "      <td>706.700874</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>34.484850</td>\n",
       "      <td>9.890458</td>\n",
       "      <td>22607.543507</td>\n",
       "      <td>14414.156057</td>\n",
       "      <td>1253.468648</td>\n",
       "      <td>605.497568</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>53.069798</td>\n",
       "      <td>7.371742</td>\n",
       "      <td>26429.063591</td>\n",
       "      <td>19831.582393</td>\n",
       "      <td>4643.286377</td>\n",
       "      <td>1288.822489</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>53.453540</td>\n",
       "      <td>7.416601</td>\n",
       "      <td>27577.043838</td>\n",
       "      <td>20690.695208</td>\n",
       "      <td>4706.879069</td>\n",
       "      <td>1299.484420</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>53.446441</td>\n",
       "      <td>7.463602</td>\n",
       "      <td>27629.142181</td>\n",
       "      <td>20753.441290</td>\n",
       "      <td>4706.237549</td>\n",
       "      <td>1307.643728</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>53.683812</td>\n",
       "      <td>7.482429</td>\n",
       "      <td>28363.862194</td>\n",
       "      <td>21360.395188</td>\n",
       "      <td>4745.404622</td>\n",
       "      <td>1312.355821</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>53.763687</td>\n",
       "      <td>7.490874</td>\n",
       "      <td>28612.253669</td>\n",
       "      <td>21541.210122</td>\n",
       "      <td>4758.646484</td>\n",
       "      <td>1314.497405</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>53.755704</td>\n",
       "      <td>7.501893</td>\n",
       "      <td>28615.746997</td>\n",
       "      <td>21591.311168</td>\n",
       "      <td>4757.386068</td>\n",
       "      <td>1316.073125</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>41.243127</td>\n",
       "      <td>0.317249</td>\n",
       "      <td>30863.627770</td>\n",
       "      <td>3380.109708</td>\n",
       "      <td>1694.784993</td>\n",
       "      <td>14.829966</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>41.277846</td>\n",
       "      <td>0.448929</td>\n",
       "      <td>30928.800468</td>\n",
       "      <td>4291.384959</td>\n",
       "      <td>1705.318278</td>\n",
       "      <td>17.761925</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>41.377453</td>\n",
       "      <td>0.165226</td>\n",
       "      <td>32598.019048</td>\n",
       "      <td>1105.464323</td>\n",
       "      <td>1672.794922</td>\n",
       "      <td>13.619075</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>41.623752</td>\n",
       "      <td>0.217343</td>\n",
       "      <td>34219.281134</td>\n",
       "      <td>2165.876547</td>\n",
       "      <td>1692.242920</td>\n",
       "      <td>16.395613</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         RMSE   std_RMSE       S_score   std_S_score          MSE  \\\n",
       "9   17.497979   3.562215    825.774870    401.682215   400.173126   \n",
       "41  16.745512   0.604739    965.182981    367.017037   193.470510   \n",
       "14  16.503820   2.025543   1006.692206    529.569084   190.939529   \n",
       "49  17.605326   0.619697   1112.880867     39.172785   326.713832   \n",
       "19  18.371744   1.790792   1142.275679    218.889242   364.033732   \n",
       "35  18.700393   2.609535   1161.005507    636.561197   370.646434   \n",
       "16  17.011934   2.814851   1263.985011    916.279482   215.209880   \n",
       "45  17.625022   1.263987   1487.148969     48.503773   228.362208   \n",
       "13  19.126311   2.110756   1665.596473    449.880854   232.322617   \n",
       "31  29.325745   9.705228   2807.163994   2002.520789  1359.117264   \n",
       "47  23.095542   9.421141   4379.919678   5407.203321   622.112859   \n",
       "5   26.392097   9.130650   7546.435694   7709.062278   720.886286   \n",
       "18  24.769750  11.028122   8120.346761  10031.896243   740.888097   \n",
       "27  24.398759  10.564490   8286.917443   9814.929095   614.265757   \n",
       "39  24.441491  11.277916   8361.951758  10133.429452   702.232931   \n",
       "32  24.292283  10.925982   8533.229048  11136.027635   623.655436   \n",
       "1   33.496920  10.015116   9815.157938   8726.546643  1543.873779   \n",
       "28  24.219685  12.090146  10898.724662  14318.843285   670.551412   \n",
       "40  43.701816   5.195296  11330.866388   4283.397028  2937.274577   \n",
       "21  37.354193   4.081427  11360.469851   8188.081295  1723.544189   \n",
       "15  24.227292  12.241211  11688.374155  15281.609160   690.193242   \n",
       "30  25.646023  11.132737  11704.781574  14895.488752   733.367716   \n",
       "3   24.594619  11.942162  11795.221873  15182.471734   679.291280   \n",
       "24  24.864661  11.826517  11807.551516  15377.039205   693.166280   \n",
       "37  24.937354  11.748832  11807.782297  15334.189330   694.724798   \n",
       "46  26.464920  10.795586  11860.579275  15207.862708   808.737396   \n",
       "7   25.471285  11.416411  12005.732445  15456.828492   668.215520   \n",
       "42  25.293153  11.516187  12101.026358  15248.684488   718.063202   \n",
       "33  25.099941  11.695238  12108.065325  15580.112495   713.100601   \n",
       "2   44.148993   5.650175  12549.591689   5181.409746  2994.281250   \n",
       "10  44.226517   5.726926  12700.091277   5322.964460  3006.863118   \n",
       "4   44.650993   6.144368  12940.459855   6082.571791  3099.312703   \n",
       "0   33.413085  10.533023  18486.133544  12226.266101  1247.956136   \n",
       "34  40.667191   0.495222  19747.666920  10710.124251  1977.917562   \n",
       "43  40.616117   0.548840  19943.950670  10738.956499  1960.101440   \n",
       "25  40.661682   0.620684  20326.275299  11283.968118  1962.383138   \n",
       "20  40.750060   0.693425  21098.413062  12039.431608  1962.384766   \n",
       "23  33.513987  11.098186  21524.973895  14329.986473  1218.739583   \n",
       "22  33.142934  11.781266  22104.488041  15047.489793  1181.934189   \n",
       "17  34.484850   9.890458  22607.543507  14414.156057  1253.468648   \n",
       "6   53.069798   7.371742  26429.063591  19831.582393  4643.286377   \n",
       "12  53.453540   7.416601  27577.043838  20690.695208  4706.879069   \n",
       "26  53.446441   7.463602  27629.142181  20753.441290  4706.237549   \n",
       "38  53.683812   7.482429  28363.862194  21360.395188  4745.404622   \n",
       "8   53.763687   7.490874  28612.253669  21541.210122  4758.646484   \n",
       "36  53.755704   7.501893  28615.746997  21591.311168  4757.386068   \n",
       "44  41.243127   0.317249  30863.627770   3380.109708  1694.784993   \n",
       "48  41.277846   0.448929  30928.800468   4291.384959  1705.318278   \n",
       "11  41.377453   0.165226  32598.019048   1105.464323  1672.794922   \n",
       "29  41.623752   0.217343  34219.281134   2165.876547  1692.242920   \n",
       "\n",
       "        std_MSE  nodes  dropout activation batch_size  \n",
       "9    298.613398   [32]      0.4       tanh         64  \n",
       "41    66.504243  [256]      0.3       tanh         64  \n",
       "14    69.731883  [128]      0.3       tanh         64  \n",
       "49   226.434678  [128]      0.1       tanh         64  \n",
       "19   287.240205   [32]      0.1       tanh         64  \n",
       "35   205.968875  [256]      0.2       tanh        128  \n",
       "16   101.440674   [64]      0.2       tanh         64  \n",
       "45   120.898029   [64]      0.2       tanh         64  \n",
       "13    91.951862   [64]      0.1       tanh         64  \n",
       "31  1006.017335   [32]      0.3       tanh        128  \n",
       "47   552.379638  [256]      0.4       tanh        128  \n",
       "5    673.245429   [64]      0.1       tanh        128  \n",
       "18   710.912814  [128]      0.3       tanh        256  \n",
       "27   628.087601  [256]      0.3       tanh        128  \n",
       "39   735.094165   [64]      0.1       tanh        128  \n",
       "32   651.057081  [128]      0.2       tanh         64  \n",
       "1    940.089034   [64]      0.1       tanh        256  \n",
       "28   722.170198  [128]      0.4       tanh         64  \n",
       "40  1018.155713   [64]      0.4       tanh        512  \n",
       "21   556.301155  [128]      0.3       tanh        512  \n",
       "15   704.961146  [256]      0.1       tanh        128  \n",
       "30   656.771684  [256]      0.1       tanh        256  \n",
       "3    699.559097  [256]      0.1       tanh        256  \n",
       "24   700.378040  [256]      0.3       tanh        128  \n",
       "37   701.520493  [256]      0.4       tanh        128  \n",
       "46   635.561485  [256]      0.2       tanh        256  \n",
       "7    719.926001  [256]      0.3       tanh         64  \n",
       "42   681.933139  [256]      0.1       tanh        256  \n",
       "33   687.626129  [256]      0.1       tanh        128  \n",
       "2   1104.139331   [32]      0.2       tanh        256  \n",
       "10  1116.986012   [32]      0.1       tanh        256  \n",
       "4   1169.493138   [32]      0.3       tanh        256  \n",
       "0    652.101355  [128]      0.2       tanh        256  \n",
       "34   345.778755  [128]      0.4       tanh        512  \n",
       "43   323.043852  [128]      0.4       tanh        512  \n",
       "25   325.146269  [128]      0.4       tanh        512  \n",
       "20   329.422304  [128]      0.2       tanh        512  \n",
       "23   660.728350  [128]      0.4       tanh        128  \n",
       "22   706.700874  [256]      0.2       tanh        128  \n",
       "17   605.497568  [256]      0.1       tanh        256  \n",
       "6   1288.822489   [32]      0.1       tanh        512  \n",
       "12  1299.484420   [32]      0.3       tanh        512  \n",
       "26  1307.643728   [32]      0.4       tanh        512  \n",
       "38  1312.355821   [32]      0.1       tanh        512  \n",
       "8   1314.497405   [32]      0.1       tanh        512  \n",
       "36  1316.073125   [32]      0.3       tanh        512  \n",
       "44    14.829966  [256]      0.3       tanh        512  \n",
       "48    17.761925  [256]      0.3       tanh        512  \n",
       "11    13.619075  [256]      0.2       tanh        256  \n",
       "29    16.395613  [128]      0.1       tanh        128  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.to_csv(\"results/results_lstm_fd003\")\n",
    "results.sort_values(by=['S_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7104533f",
   "metadata": {},
   "source": [
    "## Sommaire: <a class=\"anchor\" id=\"sommaire\"></a>\n",
    "* [Sommaire](#sommaire)\n",
    "* [Preambule](#prem)\n",
    "     * [Package Loading](#package)\n",
    "     * [Functions](#function)\n",
    "* [LSTM](#lstm)\n",
    "    * [1.FD001](#fd001)\n",
    "        * [1.1 Data loading](#fd001dataload)\n",
    "        * [1.2 Model selection](#fd001modelselect)\n",
    "    * [2.FD002](#fd002)\n",
    "         * [2.1 Data loading](#fd002dataload)\n",
    "         * [2.2 Model selection](#fd002modelselect)\n",
    "    * [3.FD003](#fd003)\n",
    "         * [3.1 Data loading](#fd003dataload)\n",
    "         * [3.2 Model selection](#fd003modelselect)\n",
    "    * [4.FD004](#fd004)\n",
    "         * [4.1 Data loading](#fd004dataload)\n",
    "         * [4.2 Model selection](#fd004modelselect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bdb17e",
   "metadata": {},
   "source": [
    "### FD004  <a class=\"anchor\" id=\"fd004\">  </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afe0ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [64]\t0.2\ttanh\t128\n",
    "# 0.2 125 39\n",
    "\n",
    "def create_model2C(input_shape, nodes_per_layer, dropout, activation):\n",
    "\n",
    "    # weights_file = 'weights_file.h5'\n",
    "    \n",
    "    model = Sequential([LSTM(64, activation='tanh', return_sequences=True),\n",
    "                        # Dropout(dropout),\n",
    "                        LSTM(64, activation='tanh'),\n",
    "                        Dense(256, activation = 'relu'),\n",
    "                        Dropout(0.2),\n",
    "                        Dense(1)\n",
    "                        ])\n",
    "    # model.save_weights(weights_file)\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model3C(input_shape, nodes_per_layer, dropout, activation):\n",
    "    \n",
    "    # weights_file = 'weights_file.h5'\n",
    "    # bs = 64\n",
    "    \n",
    "    model = Sequential([LSTM(64, activation='tanh', return_sequences=True),\n",
    "                        # Dropout(dropout),\n",
    "                        LSTM(64, activation='tanh', return_sequences=True),\n",
    "                        LSTM(32, activation='tanh'),\n",
    "                        Dense(256, activation = 'relu'),\n",
    "                        Dropout(0.2),\n",
    "                        Dense(1)\n",
    "                        ])\n",
    "    # model.save_weights(weights_file)\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model4C(input_shape, nodes_per_layer, dropout, activation):\n",
    "        \n",
    "    model = Sequential([LSTM(64, activation='tanh', return_sequences=True),\n",
    "                        # Dropout(dropout),\n",
    "                        LSTM(64, activation='tanh', return_sequences=True),\n",
    "                        LSTM(32, activation='tanh', return_sequences=True),\n",
    "                        LSTM(nodes_per_layer[1], activation='tanh'),\n",
    "                        Dense(32, activation = 'relu'),\n",
    "                        Dense(256, activation = 'relu'),\n",
    "                        Dropout(dropout),\n",
    "                        Dense(1)\n",
    "                        ])\n",
    "        \n",
    "    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function for the rectified RUL\n",
    "def rul_piecewise_fct(X_train, rul):\n",
    "    \n",
    "    X_train['RUL'].clip(upper=rul, inplace=True)\n",
    "    \n",
    "    return X_train\n",
    "\n",
    "# Function for data preprocessing\n",
    "def prep_data(train, test, drop_sensors, remaining_sensors, alpha):\n",
    "    \n",
    "    X_train_interim = add_operating_condition(train.drop(drop_sensors, axis=1))\n",
    "    X_test_interim = add_operating_condition(test.drop(drop_sensors, axis=1))\n",
    "\n",
    "    X_train_interim, X_test_interim = condition_scaler(X_train_interim, X_test_interim, remaining_sensors)\n",
    "\n",
    "    X_train_interim = exponential_smoothing(X_train_interim, remaining_sensors, 0, alpha)\n",
    "    X_test_interim = exponential_smoothing(X_test_interim, remaining_sensors, 0, alpha)\n",
    "    \n",
    "    return X_train_interim, X_test_interim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25327739",
   "metadata": {},
   "source": [
    "#### Data loading <a id = \"fd004dataload\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ac671d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61249, 27) (41214, 26) (248, 1)\n"
     ]
    }
   ],
   "source": [
    "train, test, y_test = prepare_data('FD004.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50','P30','Nf','Nc','Ps30','phi',\n",
    "                'NRf','NRc','BPR','htBleed','W31','W32'] # selection based on main_notebook\n",
    "\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "train = rul_piecewise_fct(train, 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b411443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [256]\t0.3\ttanh\t64\n",
    "# 0.2 125 39\n",
    "\n",
    "# Lower alpha's perform better, so we can ditch a few high ones to reduce the search space\n",
    "alpha_list = [0.01, 0.05] + list(np.arange(10,60+1,10)/100)\n",
    "\n",
    "sequence_list = list(np.arange(10,40+1,5))\n",
    "epoch_list = list(np.arange(5,20+1,5))\n",
    "nodes_list = [ [256, 64], [256, 32]]\n",
    "\n",
    "# lowest dropout=0.1, because I know zero dropout will yield better training results but worse generalization\n",
    "dropouts = list(np.arange(1,3)/10)  \n",
    "\n",
    "# again, earlier testing revealed relu performed significantly worse, so I removed it from the options\n",
    "activation_functions = ['tanh', 'tanh']\n",
    "batch_size_list = [64, 128]\n",
    "sensor_list = [sensor_names]\n",
    "\n",
    "tuning_options = np.prod([len(alpha_list),\n",
    "                          len(sequence_list),\n",
    "                          len(epoch_list),\n",
    "                          len(nodes_list),\n",
    "                          len(dropouts),\n",
    "                          len(activation_functions),\n",
    "                          len(batch_size_list),\n",
    "                          len(sensor_list)])\n",
    "tuning_options\n",
    "ITERATIONS = 1\n",
    "SEED = 0\n",
    "rul_piecewise = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "398f3354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1\n",
      "(51787, 39, 14) (51787, 1) (248, 39, 14)\n",
      "Epoch 1/20\n",
      "405/405 [==============================] - 20s 40ms/step - loss: 2400.6450 - val_loss: 1934.7534\n",
      "Epoch 2/20\n",
      "405/405 [==============================] - 14s 35ms/step - loss: 1778.7178 - val_loss: 1922.0099\n",
      "Epoch 3/20\n",
      "405/405 [==============================] - 15s 37ms/step - loss: 1516.0817 - val_loss: 430.3576\n",
      "Epoch 4/20\n",
      "405/405 [==============================] - 16s 38ms/step - loss: 343.0865 - val_loss: 265.8938\n",
      "Epoch 5/20\n",
      "405/405 [==============================] - 16s 39ms/step - loss: 286.6220 - val_loss: 284.0049\n",
      "Epoch 6/20\n",
      "405/405 [==============================] - 16s 39ms/step - loss: 267.3003 - val_loss: 240.4367\n",
      "Epoch 7/20\n",
      "405/405 [==============================] - 16s 39ms/step - loss: 255.1975 - val_loss: 246.1687\n",
      "Epoch 8/20\n",
      "405/405 [==============================] - 16s 39ms/step - loss: 239.7551 - val_loss: 228.8415\n",
      "Epoch 9/20\n",
      "405/405 [==============================] - 16s 39ms/step - loss: 229.3929 - val_loss: 220.5020\n",
      "Epoch 10/20\n",
      "405/405 [==============================] - 16s 39ms/step - loss: 226.8700 - val_loss: 237.1463\n",
      "Epoch 11/20\n",
      "405/405 [==============================] - 16s 39ms/step - loss: 221.2084 - val_loss: 229.5872\n",
      "Epoch 12/20\n",
      "405/405 [==============================] - 16s 39ms/step - loss: 218.6274 - val_loss: 225.5062\n",
      "Epoch 13/20\n",
      "405/405 [==============================] - 16s 39ms/step - loss: 212.2516 - val_loss: 221.2392\n",
      "Epoch 14/20\n",
      "405/405 [==============================] - 16s 39ms/step - loss: 211.0873 - val_loss: 218.4307\n",
      "Epoch 15/20\n",
      "405/405 [==============================] - 16s 39ms/step - loss: 211.4523 - val_loss: 211.7596\n",
      "Epoch 16/20\n",
      "405/405 [==============================] - 16s 39ms/step - loss: 207.1328 - val_loss: 225.7687\n",
      "Epoch 17/20\n",
      "405/405 [==============================] - 16s 39ms/step - loss: 205.5059 - val_loss: 218.3322\n",
      "Epoch 18/20\n",
      "405/405 [==============================] - 16s 38ms/step - loss: 204.0088 - val_loss: 266.6801\n",
      "Epoch 19/20\n",
      "405/405 [==============================] - 16s 39ms/step - loss: 203.0890 - val_loss: 220.9307\n",
      "Epoch 20/20\n",
      "405/405 [==============================] - 16s 39ms/step - loss: 201.4223 - val_loss: 225.9486\n",
      "8/8 [==============================] - 1s 8ms/step\n",
      "CPU times: total: 1min 19s\n",
      "Wall time: 5min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = pd.DataFrame(columns=['RMSE', 'std_RMSE', \n",
    "                                'S_score','std_S_score',\n",
    "                                'MSE', 'std_MSE',\n",
    "                                'nodes', 'dropout',\n",
    "                                'activation', 'batch_size'])\n",
    "\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    if ITERATIONS < 10:\n",
    "        print('iteration ', i+1)\n",
    "    elif ((i+1) % 10 == 0):\n",
    "        print('iteration ', i+1)    \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    \n",
    "# parameter's sample\n",
    "    alpha = 0.2\n",
    "    sequence_length = 39\n",
    "    epochs = 20\n",
    "    nodes_per_layer = random.sample(nodes_list, 1)[0]\n",
    "    dropout = 0.3\n",
    "    activation = 'tanh'\n",
    "    batch_size = random.sample(batch_size_list, 1)[0]\n",
    "    remaining_sensors = remaining_sensors\n",
    "    drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = create_model4C(input_shape, nodes_per_layer, dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #       append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11e70b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.031585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1682.405336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>225.948593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE  \\\n",
       "0  15.031585       0.0  1682.405336          0.0  225.948593      0.0   \n",
       "\n",
       "       nodes  dropout activation batch_size  \n",
       "0  [256, 64]      0.3       tanh        128  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "561f6209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.108670</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1187.232245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.054581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.506189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>918.364225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210.429535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.539137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999.440788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211.386520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.012004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>266389.055452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1156.816528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34.154820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287564.466358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1166.551636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.262780</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285836.886247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1173.938110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35.243336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>419212.856331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1242.092773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35.290780</td>\n",
       "      <td>0.0</td>\n",
       "      <td>423244.192450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1245.439209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.313713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>544881.207506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1318.685791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE        S_score  std_S_score          MSE  std_MSE  \\\n",
       "7  14.108670       0.0    1187.232245          0.0   199.054581      0.0   \n",
       "3  14.506189       0.0     918.364225          0.0   210.429535      0.0   \n",
       "2  14.539137       0.0     999.440788          0.0   211.386520      0.0   \n",
       "0  34.012004       0.0  266389.055452          0.0  1156.816528      0.0   \n",
       "6  34.154820       0.0  287564.466358          0.0  1166.551636      0.0   \n",
       "4  34.262780       0.0  285836.886247          0.0  1173.938110      0.0   \n",
       "5  35.243336       0.0  419212.856331          0.0  1242.092773      0.0   \n",
       "8  35.290780       0.0  423244.192450          0.0  1245.439209      0.0   \n",
       "1  36.313713       0.0  544881.207506          0.0  1318.685791      0.0   \n",
       "\n",
       "        nodes  dropout activation batch_size  \n",
       "7   [256, 64]      0.3       tanh        128  \n",
       "3  [256, 128]      0.3       tanh         64  \n",
       "2   [256, 32]      0.3       tanh         64  \n",
       "0  [256, 128]      0.3       tanh        128  \n",
       "6   [256, 64]      0.3       tanh         64  \n",
       "4  [256, 128]      0.3       tanh         64  \n",
       "5   [256, 64]      0.3       tanh         64  \n",
       "8   [256, 32]      0.3       tanh         64  \n",
       "1   [256, 64]      0.3       tanh        128  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "603fae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51787, 39, 14) (51787, 1) (248, 39, 14)\n",
      "Epoch 1/20\n",
      "810/810 [==============================] - 37s 43ms/step - loss: 2131.9399 - val_loss: 2329.2988\n",
      "Epoch 2/20\n",
      "810/810 [==============================] - 38s 47ms/step - loss: 1761.0864 - val_loss: 2318.7090\n",
      "Epoch 3/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 509.3386 - val_loss: 340.8644\n",
      "Epoch 4/20\n",
      "810/810 [==============================] - 36s 44ms/step - loss: 303.0032 - val_loss: 298.2368\n",
      "Epoch 5/20\n",
      "810/810 [==============================] - 38s 47ms/step - loss: 265.1307 - val_loss: 249.6993\n",
      "Epoch 6/20\n",
      "810/810 [==============================] - 38s 47ms/step - loss: 250.5625 - val_loss: 244.8042\n",
      "Epoch 7/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 236.3432 - val_loss: 241.0859\n",
      "Epoch 8/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 223.8640 - val_loss: 202.0907\n",
      "Epoch 9/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 214.2158 - val_loss: 198.5914\n",
      "Epoch 10/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 208.0134 - val_loss: 192.0031\n",
      "Epoch 11/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 207.2384 - val_loss: 197.8322\n",
      "Epoch 12/20\n",
      "810/810 [==============================] - 36s 44ms/step - loss: 202.7988 - val_loss: 196.8735\n",
      "Epoch 13/20\n",
      "810/810 [==============================] - 37s 45ms/step - loss: 200.9858 - val_loss: 186.8552\n",
      "Epoch 14/20\n",
      "810/810 [==============================] - 36s 44ms/step - loss: 198.6272 - val_loss: 177.6061\n",
      "Epoch 15/20\n",
      "810/810 [==============================] - 35s 44ms/step - loss: 197.7032 - val_loss: 189.0234\n",
      "Epoch 16/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 195.4715 - val_loss: 216.8854\n",
      "Epoch 17/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 194.6010 - val_loss: 183.1304\n",
      "Epoch 18/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 192.4353 - val_loss: 188.4613\n",
      "Epoch 19/20\n",
      "810/810 [==============================] - 36s 44ms/step - loss: 192.0045 - val_loss: 183.2902\n",
      "Epoch 20/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 188.4408 - val_loss: 183.0692\n",
      "8/8 [==============================] - 1s 11ms/step\n",
      "(51787, 39, 14) (51787, 1) (248, 39, 14)\n",
      "Epoch 1/20\n",
      "810/810 [==============================] - 37s 43ms/step - loss: 2286.5676 - val_loss: 1984.1534\n",
      "Epoch 2/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 1761.0026 - val_loss: 1980.0454\n",
      "Epoch 3/20\n",
      "810/810 [==============================] - 33s 41ms/step - loss: 1764.1902 - val_loss: 2037.8783\n",
      "Epoch 4/20\n",
      "810/810 [==============================] - 34s 43ms/step - loss: 814.1929 - val_loss: 1279.1575\n",
      "Epoch 5/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 277.6631 - val_loss: 719.1301\n",
      "Epoch 6/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 254.6708 - val_loss: 679.5693\n",
      "Epoch 7/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 237.5952 - val_loss: 330.9023\n",
      "Epoch 8/20\n",
      "810/810 [==============================] - 34s 41ms/step - loss: 223.1392 - val_loss: 573.5182\n",
      "Epoch 9/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 211.9773 - val_loss: 468.1816\n",
      "Epoch 10/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 204.5201 - val_loss: 506.2433\n",
      "Epoch 11/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 198.7697 - val_loss: 245.3658\n",
      "Epoch 12/20\n",
      "810/810 [==============================] - 35s 44ms/step - loss: 196.3263 - val_loss: 256.8624\n",
      "Epoch 13/20\n",
      "810/810 [==============================] - 33s 41ms/step - loss: 194.0393 - val_loss: 194.4579\n",
      "Epoch 14/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 192.3464 - val_loss: 538.5207\n",
      "Epoch 15/20\n",
      "810/810 [==============================] - 33s 41ms/step - loss: 191.8277 - val_loss: 526.7364\n",
      "Epoch 16/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 189.3159 - val_loss: 614.3893\n",
      "Epoch 17/20\n",
      "810/810 [==============================] - 34s 41ms/step - loss: 187.6566 - val_loss: 611.4695\n",
      "Epoch 18/20\n",
      "810/810 [==============================] - 33s 41ms/step - loss: 185.0435 - val_loss: 551.2229\n",
      "Epoch 19/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 184.9746 - val_loss: 172.5423\n",
      "Epoch 20/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 182.1390 - val_loss: 168.8006\n",
      "8/8 [==============================] - 1s 11ms/step\n",
      "(51787, 39, 14) (51787, 1) (248, 39, 14)\n",
      "Epoch 1/20\n",
      "810/810 [==============================] - 39s 45ms/step - loss: 2234.5115 - val_loss: 1952.4222\n",
      "Epoch 2/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 1755.7345 - val_loss: 1958.2631\n",
      "Epoch 3/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 1763.1514 - val_loss: 1951.1277\n",
      "Epoch 4/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 1756.2021 - val_loss: 1954.8615\n",
      "Epoch 5/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 550.5239 - val_loss: 410.9790\n",
      "Epoch 6/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 265.2531 - val_loss: 306.9115\n",
      "Epoch 7/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 246.2295 - val_loss: 308.8468\n",
      "Epoch 8/20\n",
      "810/810 [==============================] - 35s 44ms/step - loss: 234.7088 - val_loss: 1172.3440\n",
      "Epoch 9/20\n",
      "810/810 [==============================] - 37s 45ms/step - loss: 221.2698 - val_loss: 1176.9360\n",
      "Epoch 10/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 209.2567 - val_loss: 1143.4385\n",
      "Epoch 11/20\n",
      "810/810 [==============================] - 36s 44ms/step - loss: 202.8125 - val_loss: 1132.8849\n",
      "Epoch 12/20\n",
      "810/810 [==============================] - 35s 44ms/step - loss: 195.6193 - val_loss: 1137.8611\n",
      "Epoch 13/20\n",
      "810/810 [==============================] - 35s 44ms/step - loss: 193.6857 - val_loss: 1113.8071\n",
      "Epoch 14/20\n",
      "810/810 [==============================] - 35s 44ms/step - loss: 189.1105 - val_loss: 1115.1737\n",
      "Epoch 15/20\n",
      "810/810 [==============================] - 36s 44ms/step - loss: 188.2937 - val_loss: 1127.5859\n",
      "Epoch 16/20\n",
      "810/810 [==============================] - 35s 44ms/step - loss: 187.6070 - val_loss: 1134.5232\n",
      "Epoch 17/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 185.2422 - val_loss: 1115.2400\n",
      "Epoch 18/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 182.6366 - val_loss: 1119.3890\n",
      "Epoch 19/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 182.6899 - val_loss: 861.2437\n",
      "Epoch 20/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 180.2217 - val_loss: 740.8498\n",
      "8/8 [==============================] - 1s 13ms/step\n",
      "(51787, 39, 14) (51787, 1) (248, 39, 14)\n",
      "Epoch 1/20\n",
      "810/810 [==============================] - 41s 47ms/step - loss: 2097.6479 - val_loss: 2042.1495\n",
      "Epoch 2/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 1760.9141 - val_loss: 2411.4519\n",
      "Epoch 3/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 509.5515 - val_loss: 1343.6880\n",
      "Epoch 4/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 338.1449 - val_loss: 1234.7805\n",
      "Epoch 5/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 277.6407 - val_loss: 1205.2092\n",
      "Epoch 6/20\n",
      "810/810 [==============================] - 37s 45ms/step - loss: 258.6941 - val_loss: 1184.2167\n",
      "Epoch 7/20\n",
      "810/810 [==============================] - 37s 45ms/step - loss: 241.5495 - val_loss: 1052.6390\n",
      "Epoch 8/20\n",
      "810/810 [==============================] - 37s 46ms/step - loss: 230.5683 - val_loss: 1032.8835\n",
      "Epoch 9/20\n",
      "810/810 [==============================] - 37s 45ms/step - loss: 217.5860 - val_loss: 1087.5343\n",
      "Epoch 10/20\n",
      "810/810 [==============================] - 37s 46ms/step - loss: 208.4728 - val_loss: 1078.7770\n",
      "Epoch 11/20\n",
      "810/810 [==============================] - 37s 46ms/step - loss: 203.5031 - val_loss: 1117.1158\n",
      "Epoch 12/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 198.0528 - val_loss: 982.0470\n",
      "Epoch 13/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 197.2485 - val_loss: 971.3873\n",
      "Epoch 14/20\n",
      "810/810 [==============================] - 37s 46ms/step - loss: 193.7420 - val_loss: 1120.4952\n",
      "Epoch 15/20\n",
      "810/810 [==============================] - 37s 45ms/step - loss: 191.8267 - val_loss: 1134.1418\n",
      "Epoch 16/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 190.7994 - val_loss: 1173.0363\n",
      "Epoch 17/20\n",
      "810/810 [==============================] - 36s 44ms/step - loss: 187.8748 - val_loss: 1116.7917\n",
      "Epoch 18/20\n",
      "810/810 [==============================] - 37s 46ms/step - loss: 184.7063 - val_loss: 1111.5409\n",
      "Epoch 19/20\n",
      "810/810 [==============================] - 36s 44ms/step - loss: 185.4782 - val_loss: 807.4093\n",
      "Epoch 20/20\n",
      "810/810 [==============================] - 37s 45ms/step - loss: 183.0070 - val_loss: 867.9826\n",
      "8/8 [==============================] - 1s 15ms/step\n",
      "(51787, 39, 14) (51787, 1) (248, 39, 14)\n",
      "Epoch 1/20\n",
      "810/810 [==============================] - 38s 43ms/step - loss: 2273.6965 - val_loss: 2178.1182\n",
      "Epoch 2/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 1758.9572 - val_loss: 2174.6616\n",
      "Epoch 3/20\n",
      "810/810 [==============================] - 34s 41ms/step - loss: 1719.0085 - val_loss: 1852.4165\n",
      "Epoch 4/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 384.8386 - val_loss: 1141.2229\n",
      "Epoch 5/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 269.4147 - val_loss: 1125.7577\n",
      "Epoch 6/20\n",
      "810/810 [==============================] - 42s 52ms/step - loss: 256.5494 - val_loss: 1132.7964\n",
      "Epoch 7/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 245.1381 - val_loss: 630.0219\n",
      "Epoch 8/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 243.2665 - val_loss: 747.7223\n",
      "Epoch 9/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 229.7140 - val_loss: 225.8096\n",
      "Epoch 10/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 219.9343 - val_loss: 513.7104\n",
      "Epoch 11/20\n",
      "810/810 [==============================] - 34s 41ms/step - loss: 210.4712 - val_loss: 196.9790\n",
      "Epoch 12/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 203.7495 - val_loss: 196.6578\n",
      "Epoch 13/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 199.7808 - val_loss: 181.0972\n",
      "Epoch 14/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 197.0439 - val_loss: 183.1219\n",
      "Epoch 15/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 198.2993 - val_loss: 197.1415\n",
      "Epoch 16/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 195.0717 - val_loss: 202.8510\n",
      "Epoch 17/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 193.3999 - val_loss: 184.6181\n",
      "Epoch 18/20\n",
      "810/810 [==============================] - 35s 43ms/step - loss: 191.2621 - val_loss: 178.6528\n",
      "Epoch 19/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 191.8936 - val_loss: 181.1167\n",
      "Epoch 20/20\n",
      "810/810 [==============================] - 34s 42ms/step - loss: 190.5063 - val_loss: 179.8456\n",
      "8/8 [==============================] - 1s 11ms/step\n",
      "(51787, 39, 14) (51787, 1) (248, 39, 14)\n",
      "Epoch 1/20\n",
      "810/810 [==============================] - 40s 46ms/step - loss: 2106.4622 - val_loss: 2318.0754\n",
      "Epoch 2/20\n",
      "810/810 [==============================] - 37s 45ms/step - loss: 1760.9392 - val_loss: 2318.9348\n",
      "Epoch 3/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 1766.5779 - val_loss: 2296.7649\n",
      "Epoch 4/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 1703.7363 - val_loss: 619.8405\n",
      "Epoch 5/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 466.7821 - val_loss: 354.3364\n",
      "Epoch 6/20\n",
      "810/810 [==============================] - 37s 46ms/step - loss: 309.9031 - val_loss: 1193.4297\n",
      "Epoch 7/20\n",
      "810/810 [==============================] - 38s 47ms/step - loss: 263.8958 - val_loss: 1199.4060\n",
      "Epoch 8/20\n",
      "810/810 [==============================] - 37s 45ms/step - loss: 260.8610 - val_loss: 1204.3889\n",
      "Epoch 9/20\n",
      "810/810 [==============================] - 36s 44ms/step - loss: 252.6787 - val_loss: 1196.7498\n",
      "Epoch 10/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 246.4612 - val_loss: 1206.9033\n",
      "Epoch 11/20\n",
      "810/810 [==============================] - 37s 46ms/step - loss: 238.3231 - val_loss: 1202.1349\n",
      "Epoch 12/20\n",
      "810/810 [==============================] - 36s 44ms/step - loss: 228.6011 - val_loss: 1194.3969\n",
      "Epoch 13/20\n",
      "810/810 [==============================] - 31596s 39s/step - loss: 220.4486 - val_loss: 1193.4095\n",
      "Epoch 14/20\n",
      "810/810 [==============================] - 49s 60ms/step - loss: 211.1567 - val_loss: 1163.5912\n",
      "Epoch 15/20\n",
      "810/810 [==============================] - 55s 68ms/step - loss: 208.2270 - val_loss: 1152.7345\n",
      "Epoch 16/20\n",
      "810/810 [==============================] - 51s 64ms/step - loss: 204.7444 - val_loss: 774.5895\n",
      "Epoch 17/20\n",
      "810/810 [==============================] - 52s 64ms/step - loss: 203.2471 - val_loss: 292.7488\n",
      "Epoch 18/20\n",
      "810/810 [==============================] - 60s 74ms/step - loss: 197.1554 - val_loss: 195.2100\n",
      "Epoch 19/20\n",
      "810/810 [==============================] - 56s 69ms/step - loss: 198.8743 - val_loss: 187.6892\n",
      "Epoch 20/20\n",
      "810/810 [==============================] - 48s 60ms/step - loss: 197.2415 - val_loss: 175.1340\n",
      "8/8 [==============================] - 1s 17ms/step\n",
      "(51787, 39, 14) (51787, 1) (248, 39, 14)\n",
      "Epoch 1/20\n",
      "810/810 [==============================] - 55s 64ms/step - loss: 2084.3391 - val_loss: 2220.5635\n",
      "Epoch 2/20\n",
      "810/810 [==============================] - 50s 62ms/step - loss: 1759.7372 - val_loss: 2127.1167\n",
      "Epoch 3/20\n",
      "810/810 [==============================] - 56s 69ms/step - loss: 1765.7476 - val_loss: 2065.4695\n",
      "Epoch 4/20\n",
      "810/810 [==============================] - 64s 79ms/step - loss: 506.5261 - val_loss: 266.9435\n",
      "Epoch 5/20\n",
      "810/810 [==============================] - 57s 71ms/step - loss: 267.8543 - val_loss: 242.6143\n",
      "Epoch 6/20\n",
      "810/810 [==============================] - 63s 77ms/step - loss: 254.7150 - val_loss: 248.1006\n",
      "Epoch 7/20\n",
      "810/810 [==============================] - 64s 79ms/step - loss: 242.4577 - val_loss: 252.8011\n",
      "Epoch 8/20\n",
      "810/810 [==============================] - 57s 70ms/step - loss: 233.3936 - val_loss: 226.7354\n",
      "Epoch 9/20\n",
      "810/810 [==============================] - 39s 48ms/step - loss: 220.2576 - val_loss: 230.8727\n",
      "Epoch 10/20\n",
      "810/810 [==============================] - 43s 53ms/step - loss: 212.0151 - val_loss: 203.5840\n",
      "Epoch 11/20\n",
      "810/810 [==============================] - 56s 69ms/step - loss: 207.5253 - val_loss: 211.4155\n",
      "Epoch 12/20\n",
      "810/810 [==============================] - 53s 65ms/step - loss: 201.4531 - val_loss: 200.3009\n",
      "Epoch 13/20\n",
      "810/810 [==============================] - 55s 68ms/step - loss: 199.3440 - val_loss: 191.8308\n",
      "Epoch 14/20\n",
      "810/810 [==============================] - 57s 70ms/step - loss: 195.7982 - val_loss: 175.5579\n",
      "Epoch 15/20\n",
      "810/810 [==============================] - 50s 62ms/step - loss: 195.2761 - val_loss: 216.1840\n",
      "Epoch 16/20\n",
      "810/810 [==============================] - 57s 70ms/step - loss: 194.2202 - val_loss: 193.8653\n",
      "Epoch 17/20\n",
      "810/810 [==============================] - 54s 67ms/step - loss: 191.7997 - val_loss: 257.4872\n",
      "Epoch 18/20\n",
      "810/810 [==============================] - 62s 77ms/step - loss: 188.8544 - val_loss: 256.8372\n",
      "Epoch 19/20\n",
      "810/810 [==============================] - 56s 69ms/step - loss: 190.5293 - val_loss: 248.9084\n",
      "Epoch 20/20\n",
      "810/810 [==============================] - 56s 70ms/step - loss: 187.7032 - val_loss: 212.2838\n",
      "8/8 [==============================] - 1s 20ms/step\n",
      "(51787, 39, 14) (51787, 1) (248, 39, 14)\n",
      "Epoch 1/20\n",
      "405/405 [==============================] - 50s 116ms/step - loss: 2410.8865 - val_loss: 2186.3250\n",
      "Epoch 2/20\n",
      "405/405 [==============================] - 48s 119ms/step - loss: 1761.7725 - val_loss: 2185.3933\n",
      "Epoch 3/20\n",
      "405/405 [==============================] - 51s 126ms/step - loss: 1760.1893 - val_loss: 2193.5461\n",
      "Epoch 4/20\n",
      "405/405 [==============================] - 44s 110ms/step - loss: 1762.2365 - val_loss: 2256.5359\n",
      "Epoch 5/20\n",
      "405/405 [==============================] - 49s 120ms/step - loss: 1761.8451 - val_loss: 2254.3403\n",
      "Epoch 6/20\n",
      "405/405 [==============================] - 54s 133ms/step - loss: 1763.1042 - val_loss: 2289.4844\n",
      "Epoch 7/20\n",
      "405/405 [==============================] - 53s 130ms/step - loss: 1762.6947 - val_loss: 2325.1174\n",
      "Epoch 8/20\n",
      "405/405 [==============================] - 45s 112ms/step - loss: 831.7431 - val_loss: 1009.8879\n",
      "Epoch 9/20\n",
      "405/405 [==============================] - 50s 124ms/step - loss: 290.1013 - val_loss: 984.7821\n",
      "Epoch 10/20\n",
      "405/405 [==============================] - 38s 94ms/step - loss: 264.8120 - val_loss: 1165.4866\n",
      "Epoch 11/20\n",
      "405/405 [==============================] - 35s 86ms/step - loss: 247.3848 - val_loss: 1106.6119\n",
      "Epoch 12/20\n",
      "405/405 [==============================] - 34s 84ms/step - loss: 239.7742 - val_loss: 939.0408\n",
      "Epoch 13/20\n",
      "405/405 [==============================] - 33s 81ms/step - loss: 224.9688 - val_loss: 912.1167\n",
      "Epoch 14/20\n",
      "405/405 [==============================] - 33s 82ms/step - loss: 212.2519 - val_loss: 645.7739\n",
      "Epoch 15/20\n",
      "405/405 [==============================] - 33s 81ms/step - loss: 208.3086 - val_loss: 639.8648\n",
      "Epoch 16/20\n",
      "405/405 [==============================] - 32s 79ms/step - loss: 201.5249 - val_loss: 786.2023\n",
      "Epoch 17/20\n",
      "405/405 [==============================] - 31s 76ms/step - loss: 198.3433 - val_loss: 1087.6721\n",
      "Epoch 18/20\n",
      "405/405 [==============================] - 36s 89ms/step - loss: 196.1184 - val_loss: 1089.9832\n",
      "Epoch 19/20\n",
      "405/405 [==============================] - 34s 85ms/step - loss: 194.9801 - val_loss: 1077.4972\n",
      "Epoch 20/20\n",
      "405/405 [==============================] - 36s 90ms/step - loss: 191.4023 - val_loss: 705.7845\n",
      "8/8 [==============================] - 1s 11ms/step\n",
      "(51787, 39, 14) (51787, 1) (248, 39, 14)\n",
      "Epoch 1/20\n",
      "810/810 [==============================] - 48s 56ms/step - loss: 2111.7502 - val_loss: 1937.9872\n",
      "Epoch 2/20\n",
      "810/810 [==============================] - 42s 52ms/step - loss: 1760.9620 - val_loss: 1979.5499\n",
      "Epoch 3/20\n",
      "810/810 [==============================] - 40s 50ms/step - loss: 1536.2025 - val_loss: 1386.9835\n",
      "Epoch 4/20\n",
      "810/810 [==============================] - 42s 52ms/step - loss: 332.9008 - val_loss: 378.6740\n",
      "Epoch 5/20\n",
      "810/810 [==============================] - 43s 53ms/step - loss: 263.4027 - val_loss: 459.4742\n",
      "Epoch 6/20\n",
      "810/810 [==============================] - 41s 51ms/step - loss: 250.3891 - val_loss: 488.9928\n",
      "Epoch 7/20\n",
      "810/810 [==============================] - 42s 51ms/step - loss: 239.3584 - val_loss: 409.0996\n",
      "Epoch 8/20\n",
      "810/810 [==============================] - 40s 49ms/step - loss: 231.7531 - val_loss: 355.1019\n",
      "Epoch 9/20\n",
      "810/810 [==============================] - 39s 49ms/step - loss: 219.7216 - val_loss: 418.5966\n",
      "Epoch 10/20\n",
      "810/810 [==============================] - 40s 49ms/step - loss: 209.9359 - val_loss: 392.4971\n",
      "Epoch 11/20\n",
      "810/810 [==============================] - 41s 51ms/step - loss: 204.9439 - val_loss: 534.2042\n",
      "Epoch 12/20\n",
      "810/810 [==============================] - 41s 51ms/step - loss: 198.7956 - val_loss: 406.8377\n",
      "Epoch 13/20\n",
      "810/810 [==============================] - 40s 49ms/step - loss: 196.5752 - val_loss: 540.7997\n",
      "Epoch 14/20\n",
      "810/810 [==============================] - 40s 49ms/step - loss: 194.4666 - val_loss: 571.5852\n",
      "Epoch 15/20\n",
      "810/810 [==============================] - 41s 51ms/step - loss: 194.2987 - val_loss: 608.1120\n",
      "Epoch 16/20\n",
      "810/810 [==============================] - 37s 46ms/step - loss: 191.6727 - val_loss: 667.2486\n",
      "Epoch 17/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 190.9631 - val_loss: 697.6126\n",
      "Epoch 18/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 188.0111 - val_loss: 768.0215\n",
      "Epoch 19/20\n",
      "810/810 [==============================] - 37s 45ms/step - loss: 187.1994 - val_loss: 785.3710\n",
      "Epoch 20/20\n",
      "810/810 [==============================] - 36s 45ms/step - loss: 185.4306 - val_loss: 756.3953\n",
      "8/8 [==============================] - 1s 22ms/step\n",
      "iteration  10\n",
      "(51787, 39, 14) (51787, 1) (248, 39, 14)\n",
      "Epoch 1/20\n",
      "405/405 [==============================] - 32s 73ms/step - loss: 2704.4084 - val_loss: 2122.5454\n",
      "Epoch 2/20\n",
      "405/405 [==============================] - 29s 71ms/step - loss: 1757.8685 - val_loss: 2133.1621\n",
      "Epoch 3/20\n",
      "405/405 [==============================] - 29s 72ms/step - loss: 1759.5490 - val_loss: 2152.8547\n",
      "Epoch 4/20\n",
      "405/405 [==============================] - 29s 72ms/step - loss: 1761.1415 - val_loss: 2183.5095\n",
      "Epoch 5/20\n",
      "405/405 [==============================] - 29s 72ms/step - loss: 1761.8837 - val_loss: 2187.3057\n",
      "Epoch 6/20\n",
      "405/405 [==============================] - 29s 72ms/step - loss: 1757.2542 - val_loss: 2218.8877\n",
      "Epoch 7/20\n",
      "405/405 [==============================] - 31s 76ms/step - loss: 508.2842 - val_loss: 1295.9832\n",
      "Epoch 8/20\n",
      "405/405 [==============================] - 29s 73ms/step - loss: 278.1107 - val_loss: 1199.6295\n",
      "Epoch 9/20\n",
      "405/405 [==============================] - 30s 73ms/step - loss: 255.0382 - val_loss: 1194.9877\n",
      "Epoch 10/20\n",
      "405/405 [==============================] - 29s 72ms/step - loss: 249.4274 - val_loss: 1204.5043\n",
      "Epoch 11/20\n",
      "405/405 [==============================] - 29s 73ms/step - loss: 237.9948 - val_loss: 977.2583\n",
      "Epoch 12/20\n",
      "405/405 [==============================] - 29s 72ms/step - loss: 234.9073 - val_loss: 1176.8092\n",
      "Epoch 13/20\n",
      "405/405 [==============================] - 30s 74ms/step - loss: 218.8176 - val_loss: 907.3329\n",
      "Epoch 14/20\n",
      "405/405 [==============================] - 29s 72ms/step - loss: 208.9131 - val_loss: 571.3983\n",
      "Epoch 15/20\n",
      "405/405 [==============================] - 29s 72ms/step - loss: 206.7218 - val_loss: 445.9372\n",
      "Epoch 16/20\n",
      "405/405 [==============================] - 30s 73ms/step - loss: 200.9199 - val_loss: 357.1163\n",
      "Epoch 17/20\n",
      "405/405 [==============================] - 29s 72ms/step - loss: 200.1719 - val_loss: 330.5582\n",
      "Epoch 18/20\n",
      "405/405 [==============================] - 29s 72ms/step - loss: 194.4672 - val_loss: 244.8255\n",
      "Epoch 19/20\n",
      "405/405 [==============================] - 30s 74ms/step - loss: 194.5652 - val_loss: 188.6620\n",
      "Epoch 20/20\n",
      "405/405 [==============================] - 29s 72ms/step - loss: 191.9574 - val_loss: 191.5121\n",
      "8/8 [==============================] - 1s 19ms/step\n",
      "CPU times: total: 1h 7min 37s\n",
      "Wall time: 10h 54min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = pd.DataFrame(columns=['RMSE', 'std_RMSE', \n",
    "                                'S_score','std_S_score',\n",
    "                                'MSE', 'std_MSE',\n",
    "                                'nodes', 'dropout',\n",
    "                                'activation', 'batch_size'])\n",
    "\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    if ITERATIONS < 10:\n",
    "        print('iteration ', i+1)\n",
    "    elif ((i+1) % 10 == 0):\n",
    "        print('iteration ', i+1)    \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    \n",
    "# parameter's sample\n",
    "    alpha = 0.2\n",
    "    sequence_length = 39\n",
    "    epochs = 20\n",
    "    nodes_per_layer = random.sample(nodes_list, 1)[0]\n",
    "    dropout = 0.3\n",
    "    activation = 'tanh'\n",
    "    batch_size = random.sample(batch_size_list, 1)[0]\n",
    "    remaining_sensors = remaining_sensors\n",
    "    drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = create_model3C(input_shape, nodes_per_layer, dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #       append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71ffd003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.992329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>751.370703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>168.800629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13.233821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>780.216294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.134003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.410653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>874.103563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179.845642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.530306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>945.027037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>183.069214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13.838789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>985.140372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>191.512085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.569964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1051.923955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>212.283844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26.566606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130894.519913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>705.784546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.218556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>126892.621834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>740.849792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27.502641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41285.524330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>756.395264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.461546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85932.843265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>867.982605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE        S_score  std_S_score         MSE  std_MSE  \\\n",
       "1  12.992329       0.0     751.370703          0.0  168.800629      0.0   \n",
       "5  13.233821       0.0     780.216294          0.0  175.134003      0.0   \n",
       "4  13.410653       0.0     874.103563          0.0  179.845642      0.0   \n",
       "0  13.530306       0.0     945.027037          0.0  183.069214      0.0   \n",
       "9  13.838789       0.0     985.140372          0.0  191.512085      0.0   \n",
       "6  14.569964       0.0    1051.923955          0.0  212.283844      0.0   \n",
       "7  26.566606       0.0  130894.519913          0.0  705.784546      0.0   \n",
       "2  27.218556       0.0  126892.621834          0.0  740.849792      0.0   \n",
       "8  27.502641       0.0   41285.524330          0.0  756.395264      0.0   \n",
       "3  29.461546       0.0   85932.843265          0.0  867.982605      0.0   \n",
       "\n",
       "       nodes  dropout activation batch_size  \n",
       "1  [256, 32]      0.3       tanh         64  \n",
       "5  [256, 64]      0.3       tanh         64  \n",
       "4  [256, 32]      0.3       tanh         64  \n",
       "0  [256, 64]      0.3       tanh         64  \n",
       "9  [256, 32]      0.3       tanh        128  \n",
       "6  [256, 64]      0.3       tanh         64  \n",
       "7  [256, 64]      0.3       tanh        128  \n",
       "2  [256, 32]      0.3       tanh         64  \n",
       "8  [256, 64]      0.3       tanh         64  \n",
       "3  [256, 64]      0.3       tanh         64  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values('RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15dd21ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1\n",
      "(51787, 39, 14) (51787, 1) (248, 39, 14)\n",
      "Epoch 1/20\n",
      "405/405 [==============================] - 17s 35ms/step - loss: 2927.1733 - val_loss: 1911.2460\n",
      "Epoch 2/20\n",
      "405/405 [==============================] - 13s 32ms/step - loss: 1870.3485 - val_loss: 1901.4124\n",
      "Epoch 3/20\n",
      "405/405 [==============================] - 18s 44ms/step - loss: 1855.0073 - val_loss: 1250.2932\n",
      "Epoch 4/20\n",
      "405/405 [==============================] - 19s 48ms/step - loss: 570.4372 - val_loss: 331.8809\n",
      "Epoch 5/20\n",
      "405/405 [==============================] - 19s 46ms/step - loss: 417.6230 - val_loss: 341.8280\n",
      "Epoch 6/20\n",
      "405/405 [==============================] - 19s 47ms/step - loss: 399.6472 - val_loss: 318.0155\n",
      "Epoch 7/20\n",
      "405/405 [==============================] - 18s 44ms/step - loss: 387.8885 - val_loss: 326.5475\n",
      "Epoch 8/20\n",
      "405/405 [==============================] - 16s 40ms/step - loss: 373.5564 - val_loss: 304.5685\n",
      "Epoch 9/20\n",
      "405/405 [==============================] - 16s 41ms/step - loss: 355.7402 - val_loss: 358.7908\n",
      "Epoch 10/20\n",
      "405/405 [==============================] - 16s 41ms/step - loss: 351.9379 - val_loss: 348.9950\n",
      "Epoch 11/20\n",
      "405/405 [==============================] - 22s 53ms/step - loss: 340.7658 - val_loss: 336.6570\n",
      "Epoch 12/20\n",
      "405/405 [==============================] - 17s 43ms/step - loss: 343.5844 - val_loss: 274.9688\n",
      "Epoch 13/20\n",
      "405/405 [==============================] - 17s 43ms/step - loss: 331.1896 - val_loss: 276.5197\n",
      "Epoch 14/20\n",
      "405/405 [==============================] - 19s 48ms/step - loss: 328.7827 - val_loss: 269.5925\n",
      "Epoch 15/20\n",
      "405/405 [==============================] - 18s 44ms/step - loss: 322.1645 - val_loss: 271.0577\n",
      "Epoch 16/20\n",
      "405/405 [==============================] - 20s 50ms/step - loss: 319.0447 - val_loss: 266.6818\n",
      "Epoch 17/20\n",
      "405/405 [==============================] - 19s 47ms/step - loss: 321.1292 - val_loss: 257.9043\n",
      "Epoch 18/20\n",
      "405/405 [==============================] - 18s 43ms/step - loss: 318.5475 - val_loss: 259.0136\n",
      "Epoch 19/20\n",
      "405/405 [==============================] - 17s 42ms/step - loss: 314.4558 - val_loss: 249.7442\n",
      "Epoch 20/20\n",
      "405/405 [==============================] - 20s 49ms/step - loss: 313.0416 - val_loss: 251.8902\n",
      "8/8 [==============================] - 1s 19ms/step\n",
      "iteration  2\n",
      "(51787, 39, 14) (51787, 1) (248, 39, 14)\n",
      "Epoch 1/20\n",
      "810/810 [==============================] - 34s 37ms/step - loss: 2217.3875 - val_loss: 1948.3390\n",
      "Epoch 2/20\n",
      "810/810 [==============================] - 28s 35ms/step - loss: 1865.8616 - val_loss: 1928.4838\n",
      "Epoch 3/20\n",
      "810/810 [==============================] - 26s 33ms/step - loss: 1805.8175 - val_loss: 640.6243\n",
      "Epoch 4/20\n",
      "810/810 [==============================] - 28s 34ms/step - loss: 506.1474 - val_loss: 410.4250\n",
      "Epoch 5/20\n",
      "810/810 [==============================] - 26s 32ms/step - loss: 407.9942 - val_loss: 294.6911\n",
      "Epoch 6/20\n",
      "810/810 [==============================] - 26s 32ms/step - loss: 389.9653 - val_loss: 239.7589\n",
      "Epoch 7/20\n",
      "810/810 [==============================] - 26s 32ms/step - loss: 369.7043 - val_loss: 246.4403\n",
      "Epoch 8/20\n",
      "810/810 [==============================] - 26s 32ms/step - loss: 370.0117 - val_loss: 286.2843\n",
      "Epoch 9/20\n",
      "810/810 [==============================] - 27s 34ms/step - loss: 359.6160 - val_loss: 222.0381\n",
      "Epoch 10/20\n",
      "810/810 [==============================] - 27s 34ms/step - loss: 348.2173 - val_loss: 216.1051\n",
      "Epoch 11/20\n",
      "810/810 [==============================] - 27s 34ms/step - loss: 339.2668 - val_loss: 226.8071\n",
      "Epoch 12/20\n",
      "810/810 [==============================] - 25s 31ms/step - loss: 328.8935 - val_loss: 200.4218\n",
      "Epoch 13/20\n",
      "810/810 [==============================] - 25s 31ms/step - loss: 320.8559 - val_loss: 215.2001\n",
      "Epoch 14/20\n",
      "810/810 [==============================] - 27s 33ms/step - loss: 320.4111 - val_loss: 195.6741\n",
      "Epoch 15/20\n",
      "810/810 [==============================] - 28s 35ms/step - loss: 314.0083 - val_loss: 237.2024\n",
      "Epoch 16/20\n",
      "810/810 [==============================] - 26s 32ms/step - loss: 311.4209 - val_loss: 263.7466\n",
      "Epoch 17/20\n",
      "810/810 [==============================] - 25s 30ms/step - loss: 306.5037 - val_loss: 199.7173\n",
      "Epoch 18/20\n",
      "810/810 [==============================] - 28s 35ms/step - loss: 304.9613 - val_loss: 190.3012\n",
      "Epoch 19/20\n",
      "810/810 [==============================] - 28s 34ms/step - loss: 303.8538 - val_loss: 186.7152\n",
      "Epoch 20/20\n",
      "810/810 [==============================] - 26s 32ms/step - loss: 300.2770 - val_loss: 189.2582\n",
      "8/8 [==============================] - 1s 12ms/step\n",
      "iteration  3\n",
      "(51787, 39, 14) (51787, 1) (248, 39, 14)\n",
      "Epoch 1/20\n",
      "810/810 [==============================] - 31s 32ms/step - loss: 2420.4561 - val_loss: 1931.1252\n",
      "Epoch 2/20\n",
      "810/810 [==============================] - 26s 32ms/step - loss: 1886.1469 - val_loss: 1913.9149\n",
      "Epoch 3/20\n",
      "810/810 [==============================] - 25s 31ms/step - loss: 1883.5989 - val_loss: 1874.2450\n",
      "Epoch 4/20\n",
      "810/810 [==============================] - 25s 31ms/step - loss: 704.3004 - val_loss: 271.9845\n",
      "Epoch 5/20\n",
      "810/810 [==============================] - 24s 29ms/step - loss: 410.2738 - val_loss: 256.5729\n",
      "Epoch 6/20\n",
      "810/810 [==============================] - 23s 28ms/step - loss: 392.6045 - val_loss: 252.7058\n",
      "Epoch 7/20\n",
      "810/810 [==============================] - 26s 32ms/step - loss: 370.9432 - val_loss: 257.1093\n",
      "Epoch 8/20\n",
      "810/810 [==============================] - 27s 33ms/step - loss: 366.6407 - val_loss: 247.3273\n",
      "Epoch 9/20\n",
      "810/810 [==============================] - 26s 32ms/step - loss: 354.6284 - val_loss: 240.9127\n",
      "Epoch 10/20\n",
      "810/810 [==============================] - 28s 34ms/step - loss: 351.1451 - val_loss: 208.8838\n",
      "Epoch 11/20\n",
      "810/810 [==============================] - 27s 33ms/step - loss: 347.3217 - val_loss: 205.2943\n",
      "Epoch 12/20\n",
      "810/810 [==============================] - 29s 36ms/step - loss: 337.5354 - val_loss: 211.0803\n",
      "Epoch 13/20\n",
      "810/810 [==============================] - 27s 33ms/step - loss: 334.5547 - val_loss: 209.0785\n",
      "Epoch 14/20\n",
      "810/810 [==============================] - 24s 30ms/step - loss: 332.8920 - val_loss: 214.3061\n",
      "Epoch 15/20\n",
      "810/810 [==============================] - 23s 28ms/step - loss: 327.2089 - val_loss: 231.4574\n",
      "Epoch 16/20\n",
      "810/810 [==============================] - 26s 32ms/step - loss: 326.6280 - val_loss: 207.0222\n",
      "Epoch 17/20\n",
      "810/810 [==============================] - 28s 35ms/step - loss: 321.1921 - val_loss: 197.3240\n",
      "Epoch 18/20\n",
      "810/810 [==============================] - 25s 31ms/step - loss: 317.7684 - val_loss: 195.5952\n",
      "Epoch 19/20\n",
      "810/810 [==============================] - 25s 31ms/step - loss: 317.8707 - val_loss: 194.3204\n",
      "Epoch 20/20\n",
      "810/810 [==============================] - 26s 32ms/step - loss: 315.5496 - val_loss: 186.9102\n",
      "8/8 [==============================] - 1s 14ms/step\n",
      "iteration  4\n",
      "(51787, 39, 14) (51787, 1) (248, 39, 14)\n",
      "Epoch 1/20\n",
      "405/405 [==============================] - 26s 54ms/step - loss: 2998.3806 - val_loss: 1900.0248\n",
      "Epoch 2/20\n",
      "405/405 [==============================] - 23s 58ms/step - loss: 1895.8773 - val_loss: 1894.6149\n",
      "Epoch 3/20\n",
      "405/405 [==============================] - 20s 49ms/step - loss: 1903.9694 - val_loss: 1890.9576\n",
      "Epoch 4/20\n",
      "405/405 [==============================] - 17s 41ms/step - loss: 1894.5764 - val_loss: 1906.7622\n",
      "Epoch 5/20\n",
      "405/405 [==============================] - 19s 48ms/step - loss: 1406.1277 - val_loss: 475.8781\n",
      "Epoch 6/20\n",
      "405/405 [==============================] - 19s 48ms/step - loss: 518.3349 - val_loss: 454.9622\n",
      "Epoch 7/20\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 444.6027 - val_loss: 391.3817\n",
      "Epoch 8/20\n",
      "405/405 [==============================] - 21s 53ms/step - loss: 411.2355 - val_loss: 357.9264\n",
      "Epoch 9/20\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 375.4331 - val_loss: 335.1979\n",
      "Epoch 10/20\n",
      "405/405 [==============================] - 21s 52ms/step - loss: 355.4241 - val_loss: 291.6053\n",
      "Epoch 11/20\n",
      "405/405 [==============================] - 18s 44ms/step - loss: 344.7216 - val_loss: 325.0353\n",
      "Epoch 12/20\n",
      "405/405 [==============================] - 19s 47ms/step - loss: 342.8159 - val_loss: 264.0363\n",
      "Epoch 13/20\n",
      "405/405 [==============================] - 19s 47ms/step - loss: 328.5095 - val_loss: 255.4011\n",
      "Epoch 14/20\n",
      "405/405 [==============================] - 16s 38ms/step - loss: 331.7770 - val_loss: 261.4480\n",
      "Epoch 15/20\n",
      "405/405 [==============================] - 14s 34ms/step - loss: 328.1534 - val_loss: 238.2292\n",
      "Epoch 16/20\n",
      "405/405 [==============================] - 14s 35ms/step - loss: 321.9463 - val_loss: 268.0804\n",
      "Epoch 17/20\n",
      "405/405 [==============================] - 14s 35ms/step - loss: 321.6493 - val_loss: 249.1099\n",
      "Epoch 18/20\n",
      "405/405 [==============================] - 14s 34ms/step - loss: 319.5053 - val_loss: 249.9781\n",
      "Epoch 19/20\n",
      "405/405 [==============================] - 14s 35ms/step - loss: 315.6581 - val_loss: 216.0271\n",
      "Epoch 20/20\n",
      "405/405 [==============================] - 14s 35ms/step - loss: 313.7372 - val_loss: 202.0555\n",
      "8/8 [==============================] - 1s 9ms/step\n",
      "iteration  5\n",
      "(51787, 39, 14) (51787, 1) (248, 39, 14)\n",
      "Epoch 1/20\n",
      "810/810 [==============================] - 24s 26ms/step - loss: 2340.9014 - val_loss: 1914.5701\n",
      "Epoch 2/20\n",
      "810/810 [==============================] - 21s 25ms/step - loss: 1919.0259 - val_loss: 1911.3285\n",
      "Epoch 3/20\n",
      "810/810 [==============================] - 21s 25ms/step - loss: 1141.2084 - val_loss: 316.0157\n",
      "Epoch 4/20\n",
      "810/810 [==============================] - 21s 25ms/step - loss: 479.8956 - val_loss: 255.0673\n",
      "Epoch 5/20\n",
      "810/810 [==============================] - 21s 25ms/step - loss: 438.0903 - val_loss: 229.0798\n",
      "Epoch 6/20\n",
      "810/810 [==============================] - 21s 26ms/step - loss: 427.3733 - val_loss: 232.8661\n",
      "Epoch 7/20\n",
      "810/810 [==============================] - 21s 26ms/step - loss: 408.3945 - val_loss: 222.2787\n",
      "Epoch 8/20\n",
      "810/810 [==============================] - 21s 25ms/step - loss: 402.5952 - val_loss: 233.0338\n",
      "Epoch 9/20\n",
      "810/810 [==============================] - 21s 25ms/step - loss: 392.4008 - val_loss: 204.0088\n",
      "Epoch 10/20\n",
      "810/810 [==============================] - 21s 25ms/step - loss: 387.9305 - val_loss: 238.5103\n",
      "Epoch 11/20\n",
      "810/810 [==============================] - 20s 25ms/step - loss: 385.2984 - val_loss: 224.8531\n",
      "Epoch 12/20\n",
      "810/810 [==============================] - 21s 25ms/step - loss: 379.1364 - val_loss: 213.7279\n",
      "Epoch 13/20\n",
      "810/810 [==============================] - 21s 26ms/step - loss: 362.7568 - val_loss: 192.7137\n",
      "Epoch 14/20\n",
      "810/810 [==============================] - 21s 25ms/step - loss: 361.8918 - val_loss: 206.4110\n",
      "Epoch 15/20\n",
      "810/810 [==============================] - 21s 26ms/step - loss: 353.7099 - val_loss: 241.8925\n",
      "Epoch 16/20\n",
      "810/810 [==============================] - 20s 25ms/step - loss: 349.7869 - val_loss: 339.4402\n",
      "Epoch 17/20\n",
      "810/810 [==============================] - 21s 26ms/step - loss: 352.7488 - val_loss: 317.4565\n",
      "Epoch 18/20\n",
      "810/810 [==============================] - 21s 27ms/step - loss: 344.8050 - val_loss: 208.9064\n",
      "Epoch 19/20\n",
      "810/810 [==============================] - 26s 32ms/step - loss: 344.1115 - val_loss: 220.9707\n",
      "Epoch 20/20\n",
      "810/810 [==============================] - 24s 30ms/step - loss: 340.1964 - val_loss: 224.4867\n",
      "8/8 [==============================] - 1s 8ms/step\n",
      "CPU times: total: 26min\n",
      "Wall time: 37min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = pd.DataFrame(columns=['RMSE', 'std_RMSE', \n",
    "                                'S_score','std_S_score',\n",
    "                                'MSE', 'std_MSE',\n",
    "                                'nodes', 'dropout',\n",
    "                                'activation', 'batch_size'])\n",
    "\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    if ITERATIONS < 10:\n",
    "        print('iteration ', i+1)\n",
    "    elif ((i+1) % 10 == 0):\n",
    "        print('iteration ', i+1)    \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    \n",
    "# parameter's sample\n",
    "    alpha = 0.2\n",
    "    sequence_length = 39\n",
    "    epochs = 20\n",
    "    nodes_per_layer = random.sample(nodes_list, 1)[0]\n",
    "    dropout = 0.3\n",
    "    activation = 'tanh'\n",
    "    batch_size = random.sample(batch_size_list, 1)[0]\n",
    "    remaining_sensors = remaining_sensors\n",
    "    drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = create_model4C(input_shape, nodes_per_layer, dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #       append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4cad72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.671511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>905.976041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>186.910202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.757116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>893.839576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189.258224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.214624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1003.773496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>202.055542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.982881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1314.572553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>224.486740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.871048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13994.589940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>251.890152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[256, 32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE       S_score  std_S_score         MSE  std_MSE  \\\n",
       "2  13.671511       0.0    905.976041          0.0  186.910202      0.0   \n",
       "1  13.757116       0.0    893.839576          0.0  189.258224      0.0   \n",
       "3  14.214624       0.0   1003.773496          0.0  202.055542      0.0   \n",
       "4  14.982881       0.0   1314.572553          0.0  224.486740      0.0   \n",
       "0  15.871048       0.0  13994.589940          0.0  251.890152      0.0   \n",
       "\n",
       "       nodes  dropout activation batch_size  \n",
       "2  [256, 32]      0.3       tanh         64  \n",
       "1  [256, 64]      0.3       tanh         64  \n",
       "3  [256, 32]      0.3       tanh        128  \n",
       "4  [256, 64]      0.3       tanh         64  \n",
       "0  [256, 32]      0.3       tanh        128  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658b0547",
   "metadata": {},
   "source": [
    "#### Model selection <a id = \"fd004modelselect\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78f5caa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "8/8 [==============================] - 0s 15ms/step\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "8/8 [==============================] - 0s 15ms/step\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "iteration  10\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 15ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "8/8 [==============================] - 0s 15ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "8/8 [==============================] - 0s 15ms/step\n",
      "8/8 [==============================] - 0s 15ms/step\n",
      "8/8 [==============================] - 0s 15ms/step\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "iteration  20\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "8/8 [==============================] - 0s 15ms/step\n",
      "8/8 [==============================] - 0s 15ms/step\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "8/8 [==============================] - 0s 15ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "8/8 [==============================] - 0s 15ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "8/8 [==============================] - 0s 15ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "iteration  30\n",
      "8/8 [==============================] - 0s 15ms/step\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "8/8 [==============================] - 0s 15ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "iteration  40\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "iteration  50\n",
      "8/8 [==============================] - 0s 15ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "8/8 [==============================] - 0s 16ms/step\n",
      "CPU times: total: 1h 9min 55s\n",
      "Wall time: 14h 51min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = pd.DataFrame(columns=['RMSE', 'std_RMSE', \n",
    "                                'S_score','std_S_score',\n",
    "                                'MSE', 'std_MSE',\n",
    "                                'nodes', 'dropout',\n",
    "                                'activation', 'batch_size'])\n",
    "\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    if ITERATIONS < 10:\n",
    "        print('iteration ', i+1)\n",
    "    elif ((i+1) % 10 == 0):\n",
    "        print('iteration ', i+1)\n",
    "    \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "\n",
    "    # parameter's sample\n",
    "    alpha = 0.3\n",
    "    sequence_length = 30\n",
    "    epochs = 15\n",
    "    nodes_per_layer = random.sample(nodes_list, 1)[0]\n",
    "    dropout = random.sample(dropouts, 1)[0]\n",
    "    activation = random.sample(activation_functions, 1)[0]\n",
    "    batch_size = random.sample(batch_size_list, 1)[0]\n",
    "    remaining_sensors = remaining_sensors\n",
    "    drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "    \n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = create_model(input_shape, nodes_per_layer, dropout, activation)\n",
    "    \n",
    "    # create train-val split\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "           for unit_nr in X_test_interim['Unit'].unique())\n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "        \n",
    "    gss = GroupShuffleSplit(n_splits=3, train_size=0.80, random_state=0)\n",
    "    for train_unit, val_unit in gss.split(X_train_interim['Unit'].unique(), groups=X_train_interim['Unit'].unique()):\n",
    "        train_unit = X_train_interim['Unit'].unique()[train_unit]  # gss returns indexes and index starts at 1\n",
    "        train_split_array = gen_data_wrapper(X_train_interim, sequence_length, remaining_sensors, train_unit)\n",
    "        train_split_label = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'], train_unit)\n",
    "        \n",
    "        val_unit = X_train_interim['Unit'].unique()[val_unit]\n",
    "        val_split_array = gen_data_wrapper(X_train_interim, sequence_length, remaining_sensors, val_unit)\n",
    "        val_split_label = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'], val_unit)\n",
    "        \n",
    "        # train and evaluate model\n",
    "        history = model.fit(train_split_array, train_split_label,\n",
    "                            validation_data=(val_split_array, val_split_label),\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            callbacks=[cb],\n",
    "                            verbose=0)\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "        \n",
    "        test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "    \n",
    "    # append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8760a2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>16.014446</td>\n",
       "      <td>1.122225</td>\n",
       "      <td>1648.018057</td>\n",
       "      <td>430.508037</td>\n",
       "      <td>234.990128</td>\n",
       "      <td>49.712271</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>16.719837</td>\n",
       "      <td>0.857054</td>\n",
       "      <td>1658.167758</td>\n",
       "      <td>273.375191</td>\n",
       "      <td>239.616842</td>\n",
       "      <td>37.855458</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16.770742</td>\n",
       "      <td>0.787315</td>\n",
       "      <td>1942.513307</td>\n",
       "      <td>398.675394</td>\n",
       "      <td>262.900579</td>\n",
       "      <td>52.261578</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>17.736544</td>\n",
       "      <td>1.348821</td>\n",
       "      <td>2571.548906</td>\n",
       "      <td>380.349717</td>\n",
       "      <td>269.373947</td>\n",
       "      <td>73.964361</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>18.110949</td>\n",
       "      <td>1.982117</td>\n",
       "      <td>2752.945688</td>\n",
       "      <td>1603.314743</td>\n",
       "      <td>276.890533</td>\n",
       "      <td>58.884608</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>18.959587</td>\n",
       "      <td>0.888993</td>\n",
       "      <td>2957.138854</td>\n",
       "      <td>425.307524</td>\n",
       "      <td>253.749090</td>\n",
       "      <td>59.223473</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>17.774564</td>\n",
       "      <td>0.710162</td>\n",
       "      <td>2999.142898</td>\n",
       "      <td>298.172653</td>\n",
       "      <td>244.539729</td>\n",
       "      <td>35.950421</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>18.372242</td>\n",
       "      <td>1.679014</td>\n",
       "      <td>3110.906198</td>\n",
       "      <td>920.607156</td>\n",
       "      <td>293.228841</td>\n",
       "      <td>68.834558</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.303144</td>\n",
       "      <td>2.009490</td>\n",
       "      <td>3376.373134</td>\n",
       "      <td>2269.266565</td>\n",
       "      <td>242.369481</td>\n",
       "      <td>61.523606</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>19.513405</td>\n",
       "      <td>2.928476</td>\n",
       "      <td>3791.624621</td>\n",
       "      <td>2108.094207</td>\n",
       "      <td>326.621857</td>\n",
       "      <td>123.281974</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>20.674740</td>\n",
       "      <td>1.425597</td>\n",
       "      <td>5028.160895</td>\n",
       "      <td>1285.116833</td>\n",
       "      <td>298.353882</td>\n",
       "      <td>118.986201</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19.770342</td>\n",
       "      <td>2.833061</td>\n",
       "      <td>5224.297934</td>\n",
       "      <td>2615.959907</td>\n",
       "      <td>235.819901</td>\n",
       "      <td>41.676597</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>20.576422</td>\n",
       "      <td>1.101599</td>\n",
       "      <td>5285.151698</td>\n",
       "      <td>3017.749243</td>\n",
       "      <td>273.212514</td>\n",
       "      <td>62.634710</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>19.722002</td>\n",
       "      <td>1.970430</td>\n",
       "      <td>7570.191757</td>\n",
       "      <td>4517.336097</td>\n",
       "      <td>309.327220</td>\n",
       "      <td>138.345958</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>19.444382</td>\n",
       "      <td>3.098527</td>\n",
       "      <td>7651.456713</td>\n",
       "      <td>7720.740905</td>\n",
       "      <td>249.823451</td>\n",
       "      <td>50.072535</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.280780</td>\n",
       "      <td>2.904222</td>\n",
       "      <td>8221.432981</td>\n",
       "      <td>3635.971548</td>\n",
       "      <td>253.130849</td>\n",
       "      <td>45.921204</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>27.351141</td>\n",
       "      <td>11.694201</td>\n",
       "      <td>8890.046501</td>\n",
       "      <td>8690.719142</td>\n",
       "      <td>939.795787</td>\n",
       "      <td>883.974314</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.537574</td>\n",
       "      <td>0.778112</td>\n",
       "      <td>8913.002870</td>\n",
       "      <td>2369.589098</td>\n",
       "      <td>235.448680</td>\n",
       "      <td>44.867337</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>27.112451</td>\n",
       "      <td>11.749777</td>\n",
       "      <td>9389.004779</td>\n",
       "      <td>9106.741339</td>\n",
       "      <td>913.428904</td>\n",
       "      <td>871.396653</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.410235</td>\n",
       "      <td>10.482028</td>\n",
       "      <td>12165.813068</td>\n",
       "      <td>6837.682339</td>\n",
       "      <td>1089.169454</td>\n",
       "      <td>817.750203</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22.009332</td>\n",
       "      <td>0.520123</td>\n",
       "      <td>13245.349264</td>\n",
       "      <td>8483.975438</td>\n",
       "      <td>338.235250</td>\n",
       "      <td>190.641739</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>21.692668</td>\n",
       "      <td>2.143730</td>\n",
       "      <td>15843.705896</td>\n",
       "      <td>5892.558219</td>\n",
       "      <td>245.489365</td>\n",
       "      <td>60.063546</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>22.993930</td>\n",
       "      <td>2.706531</td>\n",
       "      <td>17440.622019</td>\n",
       "      <td>8004.863703</td>\n",
       "      <td>219.625376</td>\n",
       "      <td>39.723272</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25.635211</td>\n",
       "      <td>11.676830</td>\n",
       "      <td>21215.650734</td>\n",
       "      <td>26791.658059</td>\n",
       "      <td>689.791265</td>\n",
       "      <td>650.203043</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>23.015445</td>\n",
       "      <td>0.352502</td>\n",
       "      <td>21882.045251</td>\n",
       "      <td>11865.207646</td>\n",
       "      <td>260.553558</td>\n",
       "      <td>68.842242</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>40.067836</td>\n",
       "      <td>12.389764</td>\n",
       "      <td>22101.471815</td>\n",
       "      <td>14599.200799</td>\n",
       "      <td>2107.423543</td>\n",
       "      <td>1281.641915</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>41.939191</td>\n",
       "      <td>11.483438</td>\n",
       "      <td>24034.132165</td>\n",
       "      <td>17233.702628</td>\n",
       "      <td>2303.496094</td>\n",
       "      <td>1296.837346</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>22.100436</td>\n",
       "      <td>1.979827</td>\n",
       "      <td>25069.220574</td>\n",
       "      <td>11312.695168</td>\n",
       "      <td>221.077983</td>\n",
       "      <td>43.951116</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>23.304423</td>\n",
       "      <td>2.606788</td>\n",
       "      <td>25434.268260</td>\n",
       "      <td>10631.528076</td>\n",
       "      <td>409.484695</td>\n",
       "      <td>244.258665</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>25.235045</td>\n",
       "      <td>0.747162</td>\n",
       "      <td>25781.513083</td>\n",
       "      <td>9699.517275</td>\n",
       "      <td>301.628657</td>\n",
       "      <td>109.153342</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26.504065</td>\n",
       "      <td>12.224702</td>\n",
       "      <td>29013.761659</td>\n",
       "      <td>37995.837165</td>\n",
       "      <td>725.870249</td>\n",
       "      <td>697.364288</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>46.099770</td>\n",
       "      <td>6.186099</td>\n",
       "      <td>32202.674937</td>\n",
       "      <td>8397.422576</td>\n",
       "      <td>2492.975952</td>\n",
       "      <td>1044.437513</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>26.766810</td>\n",
       "      <td>12.290329</td>\n",
       "      <td>34781.358775</td>\n",
       "      <td>45976.917338</td>\n",
       "      <td>720.373062</td>\n",
       "      <td>688.062875</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>25.955791</td>\n",
       "      <td>12.890484</td>\n",
       "      <td>35214.388965</td>\n",
       "      <td>46914.985194</td>\n",
       "      <td>716.243276</td>\n",
       "      <td>690.055938</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35.660674</td>\n",
       "      <td>10.856720</td>\n",
       "      <td>36087.211203</td>\n",
       "      <td>32642.267761</td>\n",
       "      <td>1372.092407</td>\n",
       "      <td>777.572573</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33.107661</td>\n",
       "      <td>7.612121</td>\n",
       "      <td>38065.054459</td>\n",
       "      <td>16548.856235</td>\n",
       "      <td>963.233561</td>\n",
       "      <td>854.466604</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>30.923750</td>\n",
       "      <td>9.514746</td>\n",
       "      <td>41306.800541</td>\n",
       "      <td>41356.241592</td>\n",
       "      <td>844.829137</td>\n",
       "      <td>617.484585</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46.891224</td>\n",
       "      <td>4.852644</td>\n",
       "      <td>46864.265595</td>\n",
       "      <td>21333.287389</td>\n",
       "      <td>2498.433309</td>\n",
       "      <td>933.064508</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>29.952216</td>\n",
       "      <td>9.917747</td>\n",
       "      <td>46934.722596</td>\n",
       "      <td>33898.363581</td>\n",
       "      <td>710.601679</td>\n",
       "      <td>696.659626</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21.831781</td>\n",
       "      <td>3.794163</td>\n",
       "      <td>48927.570907</td>\n",
       "      <td>32382.399434</td>\n",
       "      <td>236.415003</td>\n",
       "      <td>63.493433</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>25.768956</td>\n",
       "      <td>0.460521</td>\n",
       "      <td>57627.984432</td>\n",
       "      <td>11844.285932</td>\n",
       "      <td>207.088348</td>\n",
       "      <td>36.108895</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>28.017931</td>\n",
       "      <td>0.459576</td>\n",
       "      <td>69914.859213</td>\n",
       "      <td>21949.213300</td>\n",
       "      <td>337.663406</td>\n",
       "      <td>91.346898</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32.877812</td>\n",
       "      <td>7.158704</td>\n",
       "      <td>81540.868318</td>\n",
       "      <td>8513.120547</td>\n",
       "      <td>708.902883</td>\n",
       "      <td>663.091633</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.166646</td>\n",
       "      <td>5.817571</td>\n",
       "      <td>82598.408316</td>\n",
       "      <td>33329.585941</td>\n",
       "      <td>1453.354329</td>\n",
       "      <td>374.934359</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>44.004027</td>\n",
       "      <td>0.262199</td>\n",
       "      <td>98166.424596</td>\n",
       "      <td>8542.635136</td>\n",
       "      <td>1705.471924</td>\n",
       "      <td>28.878221</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>44.154278</td>\n",
       "      <td>0.215430</td>\n",
       "      <td>100491.565821</td>\n",
       "      <td>9237.666263</td>\n",
       "      <td>1719.515096</td>\n",
       "      <td>15.890800</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>44.161259</td>\n",
       "      <td>0.232004</td>\n",
       "      <td>100798.074231</td>\n",
       "      <td>9956.353325</td>\n",
       "      <td>1719.922038</td>\n",
       "      <td>15.991949</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>44.241421</td>\n",
       "      <td>0.120131</td>\n",
       "      <td>104227.314972</td>\n",
       "      <td>5176.794605</td>\n",
       "      <td>1716.183187</td>\n",
       "      <td>19.780867</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25.925670</td>\n",
       "      <td>4.993290</td>\n",
       "      <td>151276.791096</td>\n",
       "      <td>178105.877797</td>\n",
       "      <td>225.610504</td>\n",
       "      <td>26.890160</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>31.614301</td>\n",
       "      <td>11.043018</td>\n",
       "      <td>201339.802380</td>\n",
       "      <td>217110.424390</td>\n",
       "      <td>725.335571</td>\n",
       "      <td>685.874430</td>\n",
       "      <td>[256]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         RMSE   std_RMSE        S_score    std_S_score          MSE  \\\n",
       "37  16.014446   1.122225    1648.018057     430.508037   234.990128   \n",
       "29  16.719837   0.857054    1658.167758     273.375191   239.616842   \n",
       "7   16.770742   0.787315    1942.513307     398.675394   262.900579   \n",
       "13  17.736544   1.348821    2571.548906     380.349717   269.373947   \n",
       "38  18.110949   1.982117    2752.945688    1603.314743   276.890533   \n",
       "44  18.959587   0.888993    2957.138854     425.307524   253.749090   \n",
       "22  17.774564   0.710162    2999.142898     298.172653   244.539729   \n",
       "28  18.372242   1.679014    3110.906198     920.607156   293.228841   \n",
       "0   18.303144   2.009490    3376.373134    2269.266565   242.369481   \n",
       "30  19.513405   2.928476    3791.624621    2108.094207   326.621857   \n",
       "48  20.674740   1.425597    5028.160895    1285.116833   298.353882   \n",
       "24  19.770342   2.833061    5224.297934    2615.959907   235.819901   \n",
       "31  20.576422   1.101599    5285.151698    3017.749243   273.212514   \n",
       "35  19.722002   1.970430    7570.191757    4517.336097   309.327220   \n",
       "34  19.444382   3.098527    7651.456713    7720.740905   249.823451   \n",
       "1   21.280780   2.904222    8221.432981    3635.971548   253.130849   \n",
       "15  27.351141  11.694201    8890.046501    8690.719142   939.795787   \n",
       "4   22.537574   0.778112    8913.002870    2369.589098   235.448680   \n",
       "40  27.112451  11.749777    9389.004779    9106.741339   913.428904   \n",
       "2   30.410235  10.482028   12165.813068    6837.682339  1089.169454   \n",
       "23  22.009332   0.520123   13245.349264    8483.975438   338.235250   \n",
       "42  21.692668   2.143730   15843.705896    5892.558219   245.489365   \n",
       "17  22.993930   2.706531   17440.622019    8004.863703   219.625376   \n",
       "8   25.635211  11.676830   21215.650734   26791.658059   689.791265   \n",
       "20  23.015445   0.352502   21882.045251   11865.207646   260.553558   \n",
       "25  40.067836  12.389764   22101.471815   14599.200799  2107.423543   \n",
       "47  41.939191  11.483438   24034.132165   17233.702628  2303.496094   \n",
       "26  22.100436   1.979827   25069.220574   11312.695168   221.077983   \n",
       "41  23.304423   2.606788   25434.268260   10631.528076   409.484695   \n",
       "16  25.235045   0.747162   25781.513083    9699.517275   301.628657   \n",
       "6   26.504065  12.224702   29013.761659   37995.837165   725.870249   \n",
       "14  46.099770   6.186099   32202.674937    8397.422576  2492.975952   \n",
       "49  26.766810  12.290329   34781.358775   45976.917338   720.373062   \n",
       "43  25.955791  12.890484   35214.388965   46914.985194   716.243276   \n",
       "9   35.660674  10.856720   36087.211203   32642.267761  1372.092407   \n",
       "32  33.107661   7.612121   38065.054459   16548.856235   963.233561   \n",
       "39  30.923750   9.514746   41306.800541   41356.241592   844.829137   \n",
       "45  46.891224   4.852644   46864.265595   21333.287389  2498.433309   \n",
       "33  29.952216   9.917747   46934.722596   33898.363581   710.601679   \n",
       "12  21.831781   3.794163   48927.570907   32382.399434   236.415003   \n",
       "19  25.768956   0.460521   57627.984432   11844.285932   207.088348   \n",
       "46  28.017931   0.459576   69914.859213   21949.213300   337.663406   \n",
       "10  32.877812   7.158704   81540.868318    8513.120547   708.902883   \n",
       "3   40.166646   5.817571   82598.408316   33329.585941  1453.354329   \n",
       "36  44.004027   0.262199   98166.424596    8542.635136  1705.471924   \n",
       "21  44.154278   0.215430  100491.565821    9237.666263  1719.515096   \n",
       "18  44.161259   0.232004  100798.074231    9956.353325  1719.922038   \n",
       "27  44.241421   0.120131  104227.314972    5176.794605  1716.183187   \n",
       "5   25.925670   4.993290  151276.791096  178105.877797   225.610504   \n",
       "11  31.614301  11.043018  201339.802380  217110.424390   725.335571   \n",
       "\n",
       "        std_MSE  nodes  dropout activation batch_size  \n",
       "37    49.712271  [256]      0.3       tanh         64  \n",
       "29    37.855458  [256]      0.3       tanh        128  \n",
       "7     52.261578  [256]      0.1       tanh         64  \n",
       "13    73.964361   [32]      0.1       tanh         64  \n",
       "38    58.884608  [256]      0.2       tanh        256  \n",
       "44    59.223473   [64]      0.2       tanh        128  \n",
       "22    35.950421  [256]      0.4       tanh        128  \n",
       "28    68.834558   [64]      0.2       tanh        128  \n",
       "0     61.523606  [128]      0.3       tanh        128  \n",
       "30   123.281974  [256]      0.1       tanh        256  \n",
       "48   118.986201   [32]      0.3       tanh        128  \n",
       "24    41.676597  [256]      0.2       tanh        256  \n",
       "31    62.634710  [256]      0.2       tanh        256  \n",
       "35   138.345958   [32]      0.3       tanh        128  \n",
       "34    50.072535   [32]      0.4       tanh         64  \n",
       "1     45.921204  [256]      0.2       tanh         64  \n",
       "15   883.974314   [32]      0.4       tanh        256  \n",
       "4     44.867337  [256]      0.1       tanh         64  \n",
       "40   871.396653   [32]      0.2       tanh        256  \n",
       "2    817.750203   [64]      0.2       tanh        512  \n",
       "23   190.641739   [64]      0.4       tanh        256  \n",
       "42    60.063546   [64]      0.4       tanh        128  \n",
       "17    39.723272   [32]      0.4       tanh         64  \n",
       "8    650.203043   [64]      0.1       tanh        256  \n",
       "20    68.842242  [128]      0.2       tanh        256  \n",
       "25  1281.641915   [32]      0.1       tanh        512  \n",
       "47  1296.837346   [32]      0.4       tanh        512  \n",
       "26    43.951116   [64]      0.2       tanh         64  \n",
       "41   244.258665   [64]      0.1       tanh        256  \n",
       "16   109.153342   [32]      0.1       tanh        128  \n",
       "6    697.364288   [32]      0.4       tanh        128  \n",
       "14  1044.437513   [32]      0.2       tanh        512  \n",
       "49   688.062875  [256]      0.1       tanh        256  \n",
       "43   690.055938  [128]      0.2       tanh        128  \n",
       "9    777.572573   [64]      0.1       tanh        512  \n",
       "32   854.466604   [32]      0.4       tanh        256  \n",
       "39   617.484585  [256]      0.1       tanh        512  \n",
       "45   933.064508   [32]      0.1       tanh        512  \n",
       "33   696.659626  [128]      0.4       tanh        256  \n",
       "12    63.493433  [256]      0.1       tanh        128  \n",
       "19    36.108895   [32]      0.4       tanh         64  \n",
       "46    91.346898   [64]      0.2       tanh        128  \n",
       "10   663.091633  [256]      0.2       tanh        512  \n",
       "3    374.934359  [256]      0.1       tanh        512  \n",
       "36    28.878221  [128]      0.2       tanh        512  \n",
       "21    15.890800  [128]      0.2       tanh        512  \n",
       "18    15.991949  [128]      0.2       tanh        512  \n",
       "27    19.780867  [256]      0.3       tanh        512  \n",
       "5     26.890160  [256]      0.3       tanh         64  \n",
       "11   685.874430  [256]      0.1       tanh        128  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.to_csv(\"results/results_lstm_fd004\")\n",
    "results.sort_values(by=['S_score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
